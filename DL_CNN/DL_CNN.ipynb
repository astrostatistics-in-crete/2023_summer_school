{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A small introduction\n",
    "\n",
    "## What are Convolutional Neural Networks (CNN)\n",
    "\n",
    "> Is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. <br>\n",
    ">\n",
    "> _[A Comprehensive Guide to Convolutional Neural Networks â€” the ELI5 way, by Sumit Saha](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)_\n",
    "\n",
    "## Example of CNN architecture\n",
    "\n",
    "![CNN_schematic](https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n",
    "\n",
    "The design of CNN allows to apply similar concepts to Neural Networks with special data processing techniques on data and between layers to learn from image data. \n",
    "\n",
    "## Components\n",
    "\n",
    "**Convolutional Layers**\n",
    "\n",
    "The convultion in CNNs is a technique inspired by the organization of the visual cortex, as neurons respond to stimulius in a given field of view. The convolution is a way to propogate information from nearby pixels in an image. \n",
    "\n",
    "The aim of CNN is to **reduce the dimensions** and **keep the importan features** that help in good predictions.  \n",
    "\n",
    "Essentially a convolution is a matrix multiplication between the image and a *kernel* (another matrix, smaller than the image). Note, the shape of your input data has changed after going through the convolution (_'valid padding'_, in contrast to _'same padding'_ where the original dimensions are kept)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/kernel_snapshot.png\"> </img>\n",
    "    </div>\n",
    "\n",
    "![convolution](https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "Convoluting a 5x5x1 image with a 3x3x1 kernel to get a 3x3x1 convolved feature. The kernel (shown in yellow) takes into account only the pixels in the two diagonals (marked as 'x1' in the lower right corner of the yellow matrix). Therefore, in the first (frozen) image there are 9 pixels with the kernel considering 5 of them with values : 1+1+0+1+1 = 4 (the value trasnfered to the convoled feature).\n",
    "</div>\n",
    "\n",
    "The kernel is not necessary to move one pixel at a time. By chaning the _stride_ we can select any kind of movement, which includes both the width and the height. A (1,1) stride will move one pixel right (stating always from the top left corner) and after completing the row it will move one pixel down (and left again). A (2,2 will do the similar thing but with two pixels moves. Hoever, in this case we also **downsampling** the extracted feature. \n",
    "\n",
    "**Activaltion function**\n",
    "\n",
    "The function used to impose a non-linear transformation to the input data. Perhaps the most typical one used is the ReLU (Rectified Linear Unit), which has the advantage of not activating all neurosn at the same time.  \n",
    "\n",
    "**Pooling**\n",
    "\n",
    "Sometimes data is big and we want to speed up the process. Can we *pool* some cells together to reduce our data size between convolutions? Yes! The technique is called (obviously...) _pooling_ and it can be performed by either taking the average of all the pixels that the pooling layer is over the feature layer (**average pooling**) or the maximum value found in any of the pixels (**max pooling**). \n",
    "\n",
    "![pooling_2](https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png)\n",
    "<div style=\"text-align: center;\">\n",
    "Examples of max and average pooling. \n",
    "</div>\n",
    "\n",
    "![pooling_1](https://miro.medium.com/max/792/1*uoWYsCV5vBU8SHFPAPao-w.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "A 3x3 max pooling acting over a 5x5 feature map. \n",
    "</div>\n",
    "\n",
    "The benefits of pooling layers are: i. the **decrease of dimensions** that help the decrease the computational power, ii. they extract the most **dominant features which are rotational and positional invariant**. \n",
    "\n",
    "There are two flavors of pooling layers, either local (with dimensions smaller that the feature dimensions) or _global_ that act on the whole feature layer (and they actually convert it to a single value), which is more aggressive. \n",
    "\n",
    "**Fully connected layers**\n",
    "\n",
    "Once input images have been convolved and reduced into an appropriate size, they can be pushed through a fully connected layer into the familiar NN architecture. This layer learns converts all high-level features as identified from the previous part of the network into the final output (classes) \n",
    "\n",
    "**Dropout**\n",
    "\n",
    "One way to prevent overfitting is the dropout method - remove individual nodes from the network (with some probability) at each training stage. This could be at the level of the input node or at hidden layers.\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "Each layer's weights (and therefore outputs) are updated every training iteration. More layers can mean larger changes down the network (nonlinear behavior), for small changes in weights, so small learning rates may be needed which makes training hard. Instead we may enforce each layer to prduce **predictable** output from layer to layer using batch normalization giving more stable behavior and reducing training time. Predictbale in this case means that the distribution of outputs from the previous layer has specific properties: unit variance, zero mean. In other words it is a technique to standardize the input to a layer. ([Ioffe & Szegedy 2015](https://arxiv.org/abs/1502.03167))\n",
    "\n",
    "(Source: images and material mostly from [this web article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53).\n",
    "\n",
    "\n",
    "\n",
    "**Example classification networks**\n",
    "\n",
    "AlexNet & LeNet: image classification networks - \"In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010, AlexNet was trained to classify 1.2 million high-resolution images into 1000 different classes. It achieved top-1 and top-5 error rates of 37.5% and 17%, which outperforms state-of-the-art methods at that time.\" [article](https://medium.com/mlearning-ai/alexnet-and-image-classification-8cd8511548b4)\n",
    "    \n",
    "![example network architectures](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)\n",
    "\n",
    "## Visualization of layers\n",
    "\n",
    "[Link 1](https://poloclub.github.io/cnn-explainer/)\n",
    "\n",
    "[Link 2](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Galaxy morphology estimation\n",
    "\n",
    "**TASK 1: Build a network to classify stars, spiral and elliptical galaxies from synthetic data with noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import keras\n",
    "from IPython.display import clear_output\n",
    "import keras.utils as ult\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization,Conv3D, MaxPooling3D, Dense, Add, Activation\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an auxiliary function to plot the accuracy and loss value during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.losses2 = []\n",
    "        self.val_losses2 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.losses2.append(logs.get('categorical_accuracy'))\n",
    "        self.val_losses2.append(logs.get('val_categorical_accuracy'))\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.x, self.losses2, label=\"Training accuracy\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses2, label=\"Validation accuracy\",linestyle='--')\n",
    "        plt.ylim(0,1)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.x, self.losses, label=\"Training loss\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses, label=\"Validation loss\",linestyle='--')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,galaxy_labels):\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[0]))\n",
    "    plt.imshow(images[0,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[0,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[0,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[1]))\n",
    "    plt.imshow(images[1,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[1,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[1,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[5]))\n",
    "    plt.imshow(images[5,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[5,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[5,:,:,2], vmax=255)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_trans(label_id):\n",
    "    if label_id==0: return \"star\"\n",
    "    if label_id==1: return \"spiral galaxy\"\n",
    "    if label_id==2: return \"elliptical galaxy\"\n",
    "    else: return \"unknown\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "**data**: are images at different wavelenghts i.e. 3D with 2 spatial (41x41 pixels) and 1 spectral (3 bands) dimension     \n",
    "**labels**: take values 0: star, 1: spiral galaxy, 2: elliptical galaxy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/galaxy_cubes.npz') as data:\n",
    "    images = data['images']\n",
    "    galaxy_labels = data['labels']\n",
    "\n",
    "print([images.shape, galaxy_labels.shape])\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add white noise to observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= images+np.random.randn(10000,41,41,3)*20\n",
    "images= np.clip(images, 0, 255)\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "## Create training and testing (validation) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT:_ to test various architectures fast keep the train/test sizes rather **small**, i.e. use a huge fraction for test to get a very small number for train+validation (a few hundrends). At the end you would like to retrain with the **full dataset** (that will take some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is for data and y for labels... \n",
    "\n",
    "# first split to get the test data, and X_train_full which is both train+validation\n",
    "# you can use random_state if you want to reproduce the same exact splits\n",
    "# and shuffle if you want to mix the data before splits\n",
    "\n",
    "X_train_full, X_test_img, y_train_full, y_test = train_test_split(\n",
    "        ... , ... , test_size = ... ) #, shuffle = True, random_state=42)\n",
    "\n",
    "# split into train and validation\n",
    "X_train_img, X_valid_img, y_train, y_valid = train_test_split(\n",
    "        ... , ... , test_size = ... ) #, shuffle = True, random_state=24 )\n",
    "\n",
    "print(f'From {len(images)} images, we use as:')\n",
    "print(f'test: \\t\\t {len(X_test)}')\n",
    "print(f'train: \\t\\t  {len(X_train_img)}')\n",
    "print(f'validation:\\t  {len(X_valid_img)}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is a data manipulation as keras needs the number of objects \n",
    "# with properties at each \"channel\", and their correspoding number. \n",
    "# As keras thinks of images as RGB it uses 3 as last number. \n",
    "# To avoid keras to assume anything add specifically ',1' at the end.\n",
    "\n",
    "X_train = X_train_img.reshape(len(X_train_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "X_valid = X_valid_img.reshape(len(X_valid_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "X_test  = X_test_img.reshape(len(X_test_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "\n",
    "\n",
    "# NOTE: converting labels to categorical representation  \n",
    "y_train_cat = ult.to_categorical(y_train,num_classes=3)\n",
    "y_valid_cat = ult.to_categorical(y_valid,num_classes=3)\n",
    "y_test_cat  = ult.to_categorical(y_test,num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network layers and characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((images.shape[1], images.shape[2], images.shape[3], 1),name='main_input')\n",
    "\n",
    "conv00  = Conv3D(16, (3, 3, 2), strides=(1, 1, 1), padding='same', name='conv00')(inputs)\n",
    "#bn00 = BatchNormalization()(conv00)\n",
    "act00 = Activation('relu')(conv00)\n",
    "pool00  = MaxPooling3D(pool_size=(3, 3, 1), strides=(2, 2, 1), padding='same')(act00)\n",
    "\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "(add more layers according to your taste!)\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "fl0 = Flatten(name='fl0')( NAME OF LAST LAYER HERE )\n",
    "#Do0 = Dropout(rate=0.5)(fl0)\n",
    "fc0 = Dense(32,activation='linear')(fl0)\n",
    "#Do1 = Dropout(rate=0.5)(fc0)\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "(add more Dense layers if you want!)\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "Dn0 = Dense( ... , activation= '...', name='Dn0' )( NAME OF LAST LAYER HERE )\n",
    "\n",
    "my_model = Model(inputs=[inputs], outputs=[Dn0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select optimizer and compile the model\n",
    "\n",
    "_HINT:_ check the documentation ([keras:accuracy_metrics](https://keras.io/api/metrics/accuracy_metrics/)) and remember that we are using the one-hot labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optzr =  Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer=optzr, metrics =[' ... '])\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT:_ to test training fast keep the **batch_size larger**, and the **epochs smaller**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "history=my_model.fit( ... , ... , \n",
    "                    batch_size = ..., \n",
    "                    epochs = ... ,\n",
    "                    validation_data=[ ... , ...],\n",
    "                    callbacks=[plot_losses],shuffle=True)\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT_: if you want to speed up the process a bit select the number of TO objects of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO = len(X_test) # or smaller...\n",
    "\n",
    "ls,acc=my_model.evaluate( X_test[0:TO], y_test_cat[0:TO])  \n",
    "print(\"Loss value: %.2f\" % (ls))  \n",
    "print(\"Accuracy: %.1f\" % (acc*100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict label for particular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select object\n",
    "obj = ...        # must be < len( X_test)\n",
    "\n",
    "preds = my_model.predict(Test_data[obj:obj+1,:,:,:,:])\n",
    "print(f\"Probability per class: {', '.join([str(i*100)[0:5]+'%' for i in preds[0]])}\")\n",
    "print(f'Highest for class: {label_trans( test_labels[obj])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use intemediate layers as outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT_: use model.layers to print all layers of the model, and select which ones you want as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1=my_model.layers[1].output  \n",
    "lr2=my_model.layers[4].output\n",
    "lr3=my_model.layers[7].output\n",
    "\n",
    "activation_model_lr1 = Model(inputs=[inputs], outputs=lr1)\n",
    "activation_model_lr2 = Model(inputs=[inputs], outputs=lr2)\n",
    "activation_model_lr3 = Model(inputs=[inputs], outputs=lr3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the activations for particular inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = np.random.randint(0,len(X_test)-1)\n",
    "\n",
    "plt.imshow(X_test[s,:,:,0,0])\n",
    "plt.title(f'Input image index: {s}')\n",
    "plt.show()\n",
    "\n",
    "# NOTE: in the following select convolution layers\n",
    "activations_lr1 = activation_model_lr1.predict( X_test[s:s+1,:,:,:,:]) \n",
    "# activations_lr2 = activation_model_lr2.predict( X_test[s:s+1,:,:,:,:]) \n",
    "# activations_lr3 = activation_model_lr2.predict( X_test[s:s+1,:,:,:,:]) \n",
    "\n",
    "# NOTE: check the number of nodes in the CNN\n",
    "for i in np.arange(16):\n",
    "    img=activations_lr1[0,:,:,0,i]\n",
    "    plt.imshow(img)\n",
    "    plt.title('Number ' + str(i))\n",
    "    plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transfer Learning\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/ResNet-50-architecture.png\"> \n",
    "Figure 4.1. The ResNet-50 architecture. <br>\n",
    "(Credit: <a href=\"https://www.researchgate.net/figure/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and_fig1_338603223\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">source</a>)\n",
    "    </img>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        # New FC layer, random init\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(205, 205, 3))\n",
    "\n",
    "FC_LAYERS = [256]\n",
    "dropout = 0.5\n",
    "\n",
    "finetune_model = build_finetune_model(base_model, dropout=dropout, fc_layers=FC_LAYERS, num_classes=3)\n",
    "\n",
    "finetune_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT_: start with a smaller number of images (a few hundreds) to create the new images, and then increase that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "n_new_imgs = ... \n",
    "images2=np.empty([n_new_imgs,205,205,3])\n",
    "for i in range(n_new_imgs):\n",
    "    for c in range(3):\n",
    "        tmp=images[i,:,:,c]\n",
    "        img = Image.fromarray(tmp)\n",
    "        img2 = img.resize((205,205),Image.BICUBIC)\n",
    "        images2[i,:,:,c]=img2    \n",
    "\n",
    "#show_images(images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for faster computation time we select a number of initial images that we reshape\n",
    "train_fraction = 0.7\n",
    "\n",
    "num_train = int(n_new_imgs*train_fraction)\n",
    "num_valid = int((n_new_imgs-num_train)/2)\n",
    "num_test = num_valid\n",
    "\n",
    "X_train_rs = images2[0:num_train,:,:,:]\n",
    "X_valid_rs = images2[num_train:num_train+num_valid,:,:,:]\n",
    "X_test_rs  = images2[num_train+num_valid:n_new_imgs,:,:,:]\n",
    "\n",
    "y_train_rs = galaxy_labels[0:num_train]\n",
    "y_valid_rs = galaxy_labels[num_train:num_train+num_valid]\n",
    "y_test_rs  = galaxy_labels[num_train+num_valid:n_new_imgs]\n",
    "\n",
    "y_train_cat_rs = ult.to_categorical(y_train_rs,num_classes=3)\n",
    "y_valid_cat_rs = ult.to_categorical(y_valid_rs,num_classes=3)\n",
    "y_test_cat_rs  = ult.to_categorical(y_test_rs,num_classes=3)\n",
    "\n",
    "print('A few checks of what we did:')\n",
    "print(f'- data to train: {X_train_rs.shape}')\n",
    "print(f'- validation labels: {y_valid_rs.shape}, e.g. objetc 2: {y_valid_rs[2]}')\n",
    "print(f'- test categorical labels: {y_valid_cat_rs.shape}, e.g. object 2: {y_valid_rs[2]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT:_ to test training fast keep the batch_size larger, and the epochs smaller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.compile(optzr, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "                                                                        \n",
    "history = finetune_model.fit( ... , ... , \n",
    "                             batch_size= ..., \n",
    "                             epochs= ...,\n",
    "                             validation_data=[ ... , ... ],\n",
    "                             callbacks=[plot_losses],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls,acc = finetune_model.evaluate( ... , ... )\n",
    "print(\"Loss value: %.2f\" % (ls))  \n",
    "print(\"Accuracy: %.1f\" % (acc*100))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to answer these questions\n",
    "\n",
    "**1. What do you notice?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "If the training accuracy is very close to 1 and the training loss always systematically under the validation loss then the model is overfitting!\n",
    "    \n",
    "Check <a href='https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/' target=\"_blank\" rel=\"noopener noreferrer\"> this site</a> for how to diagnosize the various cases.\n",
    "\n",
    "</details>\n",
    "\n",
    "**2. Can you guess why is happening**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "There is a huge number of parameters (~50M) in this pretrained model that easily overfitts the data.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
