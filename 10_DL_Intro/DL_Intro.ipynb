{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-release",
   "metadata": {},
   "source": [
    "<font size=6>**Deep Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-performance",
   "metadata": {},
   "source": [
    "**_Deep Learning_** is a type of Machine Learning which is characterized by being **deep**.\n",
    "\n",
    "Meaning, it uses **multiple layers** to process the input information (Figure 0).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Simple_vs_Deep.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 0.  A simple <i>feedforward</i> Neural Network compared with a Deep <i>feedforward</i> Neural Network.<br>\n",
    "            (From <a href=\"https://thedatascientist.com/what-deep-learning-is-and-isnt/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The actual way the depth is designed can be very different. It could be achieved e.g. by **stacking** sequential layers (_feedforward neural networks_), via **recurrent** layers (_recurrent neural networks_), via a \"**mix**\" of these two approaches (_U-nets_), and many other ways.\n",
    "\n",
    "Don't worry: we will explain how to _computationally_ create neurons/layers [later](#Generic_Architecture_and_Neurons)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-radius",
   "metadata": {},
   "source": [
    "# Why Deep Learning is cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-calculator",
   "metadata": {},
   "source": [
    "    It is not, we are geeks, and that's the truth.\n",
    "\n",
    "However ... we do live in the era of \"Big Data\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96765074",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Survey_Size_Evolution.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.1.  Current and projected survey size evolution.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-murder",
   "metadata": {},
   "source": [
    "We cannot expect to humanly inspect these data and derive the intuition for the rules which categorize them.\n",
    "\n",
    "$\\rightarrow$ We have to leverage on:\n",
    "\n",
    "- the **large number** of examples\n",
    "\n",
    "- algorithms that can abstract **arbitrarily complex** rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-assurance",
   "metadata": {},
   "source": [
    "## So how does Deep Learning address big data issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-worker",
   "metadata": {},
   "source": [
    "The basic idea is that layers construct **new features**.\n",
    "\n",
    "In practice, Deep Learning systems include implicit **feature engeneering** _on top_ of the learning task (e.g., _classificaton_ or _regression_).<br>\n",
    "\n",
    "In this way, they are a step forward with respect to \"classic\" ML approaches (Figure 1).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Feature_Engeneering.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.2.  A Deep Neural Network seen as a combination of feature extractor + learner (e.g. classifier or regressor).<br>\n",
    "            (From <a href=\"https://stats.stackexchange.com/questions/562466/neural-networks-automatically-do-feature-engineering-how/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "From this perspective, the connections between the network neurons represent **potential correlations** betweeen features.\n",
    "\n",
    "<u>What are the implications?</u>\n",
    "\n",
    "The scientist **does _not_ have to get detailed insight of the problem** to build the proper features or select the proper classifier<br>\n",
    "$\\rightarrow$ the DL system does it all for us!\n",
    "\n",
    "This comes particularly handy when we deal with databases with **millions of objects** and **hundreds of features**!\n",
    "\n",
    "<u>References</u>\n",
    "\n",
    "In case you are curious, it has been proven that Deep Neural Networks are indeed \"**_universal approximators_**\"\n",
    "(e.g. [Kurt Hornik (1991), Neural Networks, 4, 2](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub)), meaning that they can in principle explain any linear or non-linear relation beteen the features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-separation",
   "metadata": {},
   "source": [
    "## Some example applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-accessory",
   "metadata": {},
   "source": [
    "Indeed, Deep Learning (hereafter, **DL**) is being used to solve very _different_ problems, e.g.:\n",
    "\n",
    "- **Self-Driving cars**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_Self_Driving.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.A.  NVIDIA's driverless car simulator.<br>\n",
    "            (From <a href=\"https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf/\">\"End to End Learning for Self-Driving Cars\" (2016)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Protein Structure Prediction**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_AlphaFold.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.B.  Deep Mind's Alpha Fold network for the prediction of molecular structures of proteins.\n",
    "            Original paper: <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Jumper, J., Evans, R., Pritzel, A. et al. 2021,  Nature, 596, 583</a>.<br>\n",
    "            (From <a href\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Natural Language Processing, translation, and text generation**\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_NLP.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.C.  Google's unified text-to-text transformer.<br>\n",
    "            (From <a href\"https://arxiv.org/abs/1910.10683\">Raffel et al. 2021, arxiv/1910.10683</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Computer Vision (lots and lots of it!)**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "    <img src=\"images/Facebook_Detectron.gif\" alt=\"Detectron example\">\n",
    "        <center>\n",
    "        Figure 1.3.D.  Facebook's Detectron2 for multiple computer vision tasks.<br>\n",
    "        (From <a href\"https://ai.facebook.com/tools/detectron2/\">Meta AI blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "... and many, many other _scary_ applications like:\n",
    "\n",
    "- **Deep Fakes**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_DeepFake.jpg\">\n",
    "        <center>\n",
    "        Figure 1.3.E.  Deep Fakes can be used to bring back actors from when the cinema was actually good (i.e., before 1999!), but also to produce false evidence.  Luckily, there are already ML efforts to uncover Deep Fakes, e.g. <a href\"https://arxiv.org/abs/2101.01456/\">Zi et al. 2021, arXiv/2101.01456\n",
    "</a>.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **AI chats**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ChatGPT.png\">\n",
    "        <center>\n",
    "        Figure 1.3.F.  Large Language Models applied as chat tools necessarily bring along the creators' moral judgment.\n",
    "        (From <a href\"https://openai.com/blog/chatgpt\">OpenAI's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Video Games**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"https://assets-global.website-files.com/621e749a546b7592125f38ed/62271e2f604e640534eeca99_AlphaStar%2003.gif\">\n",
    "        <center>\n",
    "        Figure 1.3.G.  Deep Mind's Alpha Star absolutely demolishng a human player who later claimed ... erhm ... that the internet connection was bad that day because ... ehrm, mmmh ... someone in the house was watching Netflix.<br>\n",
    "        (From <a href\"https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "Catching up with all the new DL developments is becoming physically impossible, but you can follow great channels like [Two Minute Papers](https://www.youtube.com/c/K%C3%A1rolyZsolnai/featured) to try and stay updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-tractor",
   "metadata": {},
   "source": [
    "## Deep Learning in Astronomy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-sigma",
   "metadata": {},
   "source": [
    "The application of DL in Astronomy is still at an **amatour level**, with respect to what happens in the industry (_prejudice against the \"black box\"?_).<br>  However ... \n",
    "\n",
    "Astronomy is the perfect ML lab because it offers:\n",
    "- tough problems to solve\n",
    "- large data\n",
    "\n",
    "In fact, Deep Learning publications are **exploding** in Astronomy (Figure 2)!\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Learning_astro_papers.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.A. Number of refereed astronomy papers containing the text \"Deep Learning\" in their abstracts.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/search/filter_property_fq_property=AND&filter_property_fq_property=property%3A%22refereed%22&fq=%7B!type%3Daqp%20v%3D%24fq_property%7D&fq_property=(property%3A%22refereed%22)&q=abs%3A%22deep%20learning%22%20year%3A2015-2022&sort=date%20desc%2C%20bibcode%20desc&p_=0\">NASA ADS</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<font size=3><u>**Some notable examples**</u><font>\n",
    "\n",
    "**Galaxy Classification**\n",
    "    \n",
    "- [Dieleman et al. (2015), MNRAS, 450, 1441](https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract) $-$ calculate probabilities for the 37 Galaxy Zoo possible answers\n",
    "\n",
    "    - **training**: classification of 61,578 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: standard CNN\n",
    "\n",
    "<table><tr>\n",
    "    <td width=420>\n",
    "        <img src=\"images/Galaxy_Zoo_flowchart.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.B. Galaxy Zoo classification tree.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2013yCat..74352835W/abstract\">Willet et al. (2013)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dieleman_Fig11.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.C. Activation of the CNN layers.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract\">Dieleman et al. (2015)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "- [Ackerman et al. 2017, MNRAS, 479, 415](https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract) $-$ identify mergers\n",
    "\n",
    "    - **training**: classification of ~4000 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: CNN with transfer learning\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Ackerman_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.D. Some galaxy pairs confidently identified as mergers.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "**Galaxy Morphology**\n",
    "    \n",
    "- [Aragon-Calvo et al. 2020, MNRAS, 498, 3713](https://ui.adsabs.harvard.edu/abs/2020MNRAS.498.3713A/abstract) $-$ obtain structural parameters via self-supervised learning\n",
    "\n",
    "    - **training**: re-produce parameters used to generate artificial galaxies\n",
    "    - **architecture**: semantic autoencoder\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Aragon_Semantic_Autoencoder.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.E. Fitting of morpological structural parameters with an Autoencoder.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "    \n",
    "**Serendipitous Detection**  \n",
    "    \n",
    "- [Lanusse et al. 2018, MNRAS, 473, 3895](https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract) $-$ spot gravitational lenses\n",
    "\n",
    "    - **training**: 20,000 LSST-like observations\n",
    "    - **architecture**: CNN + ResNet\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/DeepLens_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.F. Some images correctly identified as hosting lenses.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract\">Lanusse et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "- [Dekany & Grebel al. 2020, ApJ, 898, 46](https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract) $-$ spot fundamental-mode RR Lyrae stars \n",
    "\n",
    "    - **training**: 10$^7$$-$10$^8$ near-IR photometric time-series\n",
    "    - **architecture**: RNN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dekani_Fig4.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.G. Spatial distribution of the objects used as training set.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract\">Dekany & Grebel al. (2020)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "    \n",
    "**Image Reconstruction**\n",
    "       \n",
    "- [Schawinski et al. 2017, MNRAS, 467, 110](https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract) $-$ image denoising\n",
    "\n",
    "    - **training**: 4550 nearby SDSS galaxies\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Schawinski_Fig2.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.H. Degraded image details reconstructed by a GAN.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract\">Schawinski et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "**Cosmological Simulations**\n",
    "    \n",
    "- [Rodríguez et al. 2018, ComAC, 5, 4](https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract) $-$ create computationally _cheap_ cosmological simulations\n",
    "\n",
    "    - **training**: 10 independent L-PICOLA simulation boxes\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Rodriguez_Fig1.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.I. Comparison between the results of a N-body simulation (<i>left</i>) and from a GAN (<i>right</i>).<br>\n",
    "            (Adapted from <a herf=\"https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract\">Rodríguez et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "**Source Density Predicition**\n",
    "\n",
    "- [Xu et al. 2023, ApJ preprint](https://ui.adsabs.harvard.edu/abs/2023arXiv230401670X/abstract) $-$ get a census of Giant Molecular Clouds (GMCs) from their $N_H$ column-density images\n",
    "    \n",
    "    - **training**: 7179 high-res MHD simulations of clouds\n",
    "    - **architecture**: Diffusion Models\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/Xu_Fig2.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.4.J. The true GMC number density (<i>bottom-left</i>) is iteratively reconstructed, conditioned on the line-of-sight $N_H$ column density (<i>top-left</i>) .<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract\">Rodríguez et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-thread",
   "metadata": {},
   "source": [
    "# Neural Networks (NN) Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-compromise",
   "metadata": {},
   "source": [
    "## Generic Architecture and Neurons\n",
    "<a id='Generic_Architecture_and_Neurons'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-methodology",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Generic_Architecture.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.1.  A simple, generic <i>feedforward</i> deep neural architecture.  Neurons of a layers might be connected to al the neurons of the neighboring layers, like in this example (<i>fully-connected</i> layers), or not.<br>\n",
    "            (Adapted from <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-arbor",
   "metadata": {},
   "source": [
    "<font size=3><u>**Nomenclature**</u><font>\n",
    "    \n",
    "**Neuron**: A simple element in a network, carrying 1 value.\n",
    "    \n",
    "**Layers**: A collection of neurons activated simulataneusly.<br>\n",
    "    \n",
    "    Layers are represented diffrently depending on the architecture.\n",
    "    E.g., fully-connected layers (as in the Figure above), appear as vertical stripes of neurons.\n",
    "  \n",
    "- **input layer**: the data\n",
    "- **hidden layers**: the internal layers (\"_hidden_\" from the point of view of the NN user)\n",
    "- **output layer**: the variable(s) of interest (e.g., class(es) or $y$)\n",
    "    \n",
    "    \n",
    "    E.g., if we provide an image as input, each pixel is 1 neuron of the input layer.\n",
    "\n",
    "\n",
    "Contemporary NNs contain hundreds to thousands of layers, with million to billion of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-munich",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-relations",
   "metadata": {},
   "source": [
    "The core of the functioning of any NN is how the **information flows** through a neuron.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Weights_and_Biases.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.2.  How the information is propagated through a neuron. Don't get confused with $\\hat{y}$: in this image, it only represents the neuron's output, not the target variabe (e.g. the <i>class</i>)<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "1. **The first stage is <u>linear</u>:**<br>\n",
    "    A neuron takes all the inputs (values) x$_i$ directed into it, multiplies each of them by a different _weight_ ($w_i$), and takes the sum.<br>\n",
    "    Then, it adds a _bias_ ($b$).\n",
    "<br>\n",
    "\n",
    "2. **The second stage is (usually) <u>non-linear</u>:**<br>\n",
    "    The summation is passed to an **activation function**.<br>\n",
    "    The activation function acts as a filter, basically deciding when and how the information shall flow. \n",
    "\n",
    "The neuron output is therefore:\n",
    "\n",
    "   $$ \\hat{y} = f(\\sum{\\textbf{w}\\cdot{}\\textbf{x} + b}) $$\n",
    "\n",
    "<u>**Important**</u>\n",
    "\n",
    "The _weights_ and _biases_ are <u>the</u> elements that are fit during the training of the model! \n",
    "\n",
    "Fitting a model == optimizing **all** the _weights_ and _biases_ within the NN, in order to **approximate** the desired output $y$ given a corresponding example $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-poison",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-might",
   "metadata": {},
   "source": [
    "Activation functions are what make NNs so **efficient** as universal tools.\n",
    "\n",
    "The introduce <u>non-linearities</u> $\\rightarrow$ a NN can create an arbitrarily complex model.\n",
    "\n",
    "They can be basically **any** _filter_-like function, but they better posses some features:\n",
    "\n",
    "- **computationally inexpensive** $\\leftarrow$ hence simple, since they get executed at each neuron\n",
    "\n",
    "- **zero-centered** $\\leftarrow$ not to shift values towards a preferential direction\n",
    "\n",
    "- **differentiable** $\\leftarrow$ because NNs work with [Backpropagation](#Backpropagation)\n",
    "\n",
    "- **avoid vanishing when chained** $\\leftarrow$ more correctly, we need to avoid vanishing gradients<br>\n",
    "  (see [Gradient Descent and Loss](#Gradient-Descent-and-Loss))\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Activation_Functions.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.3.  A collection of commonly used activation functions<br>\n",
    "            (Adapted from <a href=\"https://wandb.ai/lavanyashukla/vega-plots/reports/Natural-Language-Processing--Vmlldzo2Nzk2Ng\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-evidence",
   "metadata": {},
   "source": [
    "## NN Architecture Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-vacation",
   "metadata": {},
   "source": [
    "NN come in **countless architectures**, and even trying to classify them is a tough task ...\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/NN_Zoo.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.4.  A <i>not so</i> comprehensive scheme of NN architectures, afterall.<br>\n",
    "            (Image credit: <a href=\"https://www.asimovinstitute.org/author/fjodorvanveen/\">Fjodor van Venn</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Morpheus.jpg\">\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-scholar",
   "metadata": {},
   "source": [
    "# Training NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-shannon",
   "metadata": {},
   "source": [
    "As mentioned above:\n",
    "\n",
    "    training a NN = optimize its weights and biases\n",
    "    \n",
    "in order to produce the desired output (class or values) given a corresponding input.\n",
    "\n",
    "- - -\n",
    "\n",
    "The details of the training method depends on the DL learning problem:\n",
    "\n",
    "- **supervised:** we have labelled examples \n",
    "- **unsupervised:** no labeles are available \n",
    "- **reinforced:** the examples are associated to a \"reward\" \n",
    "\n",
    "In this notbook, we will focus on the **supervised** case to illustrate the NN mechanics:<br>\n",
    "$\\rightarrow$ Let's assume we have some predicting variables $X$, and labels $y$.\n",
    "\n",
    "- - -\n",
    "\n",
    "How can we _tell_ the network in which way it shall modify weights (and biases)?<br>\n",
    "Let's break down the NN rationale:\n",
    "\n",
    "1. We initialize the weights to some arbitrary value\n",
    "2. We take a sub-sample (batch) of the data $X$, $X_{batch}$ \n",
    "3. We propagate $X_{batch}$ through the network, obtaining a predicted $\\hat{y}_{batch}$\n",
    "4. We assess the **error** between $\\hat{y}_{batch}$ and the true $y_{batch}$\n",
    "5. We need to **backpropagate** back the information about the difference\n",
    "6. We need to **update** the weights in the right direction\n",
    "7. Repeat from step 2 untill all data are used\n",
    "\n",
    "The critical steps are #4, #5 and #6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-quebec",
   "metadata": {},
   "source": [
    "## Error Function, Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-municipality",
   "metadata": {},
   "source": [
    "Let's consider an **errorr** (a.k.a. **loss**, or **cost**) function:\n",
    "\n",
    "$$ E (\\hat{y}, y) $$\n",
    "\n",
    "which assesses the intensity of the error.  For _example_, we might adopt: $E (\\hat{y}, y) = {1\\over2}(\\hat{y} - y)^2 $.\n",
    "\n",
    "To be precise, $\\hat{y}$ is itself a function of the input **x**, and of the NN parameters $\\theta$ (all the $w$ and $b$ of each node):\n",
    "\n",
    "$$ \\hat{y} = f(\\textbf{x}, \\theta) $$\n",
    "\n",
    "therefore:\n",
    "\n",
    "$$ E = E (x, y, \\theta) $$\n",
    "\n",
    "- - -\n",
    "\n",
    "The **gradient** of $E$ with respect to $\\theta$, i.e. $\\nabla E (x, y, \\theta)$, is a vector _roughly_ pointing towards the minimum of $E$:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Gradient_Descent.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.1.  Iterative update of a weight via the Gradient Descent method.<br>\n",
    "            (From <a href=\"https://ekamperi.github.io/machine%20learning/2019/07/28/gradient-descent.html\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "We can therefore **update** the weights by adding a vector proportional to $\\nabla E$:\n",
    "\n",
    "$$ \\theta^\\prime = \\theta - \\eta \\cdot \\nabla E (x, y, \\theta) ~~~~~(1)$$\n",
    "\n",
    "where the proportionality constant $\\eta$ is called **learning rate** (**LR**) because it regulates how fast we shall proceed over the mininum (and possibly _overshooting_ it, if $\\alpha$ is too large).\n",
    "\n",
    "This is the **Gradient Descent** method.\n",
    "\n",
    "- - - \n",
    "\n",
    "So we need to calculate $\\nabla E~$:\n",
    "\n",
    "$$ \\nabla E (x, y, \\theta) =\n",
    "      \\left( \\frac{\\partial E}{\\partial \\theta_1},\n",
    "             \\frac{\\partial E}{\\partial \\theta_2},\n",
    "             . . .,\n",
    "             \\frac{\\partial E}{\\partial \\theta_n}\n",
    "\\right)$$\n",
    "\n",
    "But what are the elements $\\frac{\\partial E}{\\partial \\theta_i}$?\n",
    "\n",
    "Let's start from the output neuron, and consider only the parameter $i = 1$, i.e. $\\theta_i = w_1$.<br>\n",
    "We can _re_-write $\\frac{\\partial E}{\\partial \\theta_1}$ as:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\theta_1} = \n",
    "  \\frac{\\partial E}{\\partial w_1} = \n",
    "    \\frac{\\partial E}{\\partial \\hat{y}} \\cdot\n",
    "    \\frac{\\partial \\hat{y}}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "where we applied the **chain rule** for derivatives.\n",
    "\n",
    "We have the ingredients to calculate the two components because we saw above their functional form:\n",
    "\n",
    "- $ \\hat{y} = f(\\sum{\\textbf{w}\\cdot{}\\textbf{x} + b}) $\n",
    "\n",
    "- $E (\\hat{y}, y) = {1\\over2}(\\hat{y} - y)^2 $.\n",
    "\n",
    "You can imagine **propagating** the chain rule back to the original input: all the steps are either _linear_ or passing through a _differentiable_ activation function!\n",
    "\n",
    "\n",
    "A visual explanation of the **Backpropagation Algorithm** is given in this\n",
    "\n",
    "> [Google Developers webpage](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-discovery",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In its simplest form, the Gradient Descent is performed as \"**minibatch GD**\", i.e., using the average gradient over the **batch** of data, $X_{batch}$:\n",
    "\n",
    "$$ \\nabla E (x, y, \\theta) = {1 \\over N}\\sum_i^{N_{batch}} \\nabla E (x_i, y_i, \\theta) $$\n",
    "\n",
    "> <u>NOTE</u>: Confusingly enough, the \"**batch GD**\" is the one using <u>all</u> data at once.\n",
    "    \n",
    "However in some cases, especially with large datasets, it might be convenient to use an **estimate** of the Gradient, e.g. with a **Stochastic Gradient Descent (SGD)**.\n",
    "    \n",
    "In SGD, we:\n",
    "1. Propagate 1 example through the NN\n",
    "2. Calculate its gradient and update the weigths\n",
    "3. Repeat for all the training examples\n",
    "\n",
    "**PROs**:\n",
    "- starts converging earlier because of more frequent updates $\\rightarrow$ quick insights\n",
    "- stochasticity can help avoiding local minima\n",
    "    \n",
    "**CONs**:\n",
    "- the Error function oscillates more    \n",
    "- reaches a sub-optimal minimum compared to batch GD\n",
    "- it is slower because it is sequential (cannot be parallelized)\n",
    "\n",
    "    \n",
    "Read more on [this post](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/).\n",
    "    \n",
    "> <u>NOTE:</u> To add confusion, it is common to call the \"minibatch GD\" as \"SGD\", since in practice it applies the stochasticity by picking minibatches.  In the remainder we will subtend the same nomenclature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-meditation",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "Let's recall _Equation 1_:\n",
    "\n",
    "$$ \\theta^\\prime = \\theta - \\eta \\cdot \\nabla E (x, y, \\theta) ~~~~~(1)$$\n",
    "\n",
    "Ok $-$ we have SGD $-$ **BUT**:\n",
    "\n",
    "1. A NN model can easily contain millions of parameters $\\rightarrow$ Can we efficiently **explore** the parameter space, to find the global minimum?\n",
    "2. How we **pick** the correct learning rate $\\eta$?\n",
    "\n",
    "$\\rightarrow$ **Optimization Algorithms** address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-peace",
   "metadata": {},
   "source": [
    "### Momentum optimization\n",
    "\n",
    "This method takes in account the **previous values** of the gradient.\n",
    "\n",
    "It is particularly useful when exploring _monotonic_ parameter spaces, because it can skip faster through large areas of the landscape.\n",
    "    \n",
    "The parameter update is a variation to Equation 1:\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{m} &\\leftarrow \\beta \\textbf{m}  - \\eta \\cdot \\nabla_\\theta E (\\theta) ~~~~~ (2) \\\\\n",
    "\\theta     &\\leftarrow \\theta + \\textbf{m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "- $`beta$ represents the current \"velocity\" of the gradient.<br>\n",
    "- The larger $\\beta\\in$ [0, 1], the more previous steps are taken into account.\n",
    "\n",
    "\n",
    "We can interpret Equation 2 as:\n",
    "\n",
    "> _\"Globally descending following the direction m, and updating m based on local gradient.\"_\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1000>\n",
    "        <img src=\"images/Momentum.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.2.  Representation of the influence of Momentum on the GD.<br>\n",
    "            The descent is much faster than the standard Gradient Descent (i.e. $\\beta = 0$)<br>\n",
    "            However, for very large values of $\\beta$, the search can 'oscillate' around the optimum point.<br>\n",
    "            (From <a href=\"https://mlfromscratch.com/optimizers-explained/#/\">here</a>)  \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-wheel",
   "metadata": {},
   "source": [
    "### Adaptive learning rates\n",
    "\n",
    "They address the issue that the descent is **different for each parameter** (_i.e., for each 'direction' in the parameter space_):\n",
    "\n",
    "Their differ on how they **dynamically adjust** the learning rate (LR).\n",
    "\n",
    "<font size=3><u>**AdaGrad**</u><font>\n",
    "\n",
    "AdaGrad **separately** scales the LR for each parameter $\\rightarrow$ Each direction gets a \"personalized\" search.\n",
    "\n",
    "> LR scaling $\\leftarrow$ inversely to the **sum of the squares** of _all_ the historical gradients.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=2200>\n",
    "        <img src=\"images/AdaGrad.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.3.  AdaGrad optimization compared to that of Gradient Descent.<br> \n",
    "            (Adapted from <a href=\"https://github.com/ageron/handson-ml2\">Figure 11-7, A. Gerone 2022</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "**PROs:**\n",
    "- Directions with larger gradient see their LR decrease faster.\n",
    "    \n",
    "**MAJOR CON:**\n",
    "\n",
    "- May stop too early $\\rightarrow$ _better not use it!_\n",
    "\n",
    "<font size=3><u>**RMSProp**</u><font>\n",
    "\n",
    "Puts a patch to AdaGrad early stopping.\n",
    "\n",
    "> LR scaling $\\leftarrow$ inversely to the **exponentially weighted moving average** of previous gradients.\n",
    "\n",
    "\n",
    "<font size=3><u>**Adam**</u><font>\n",
    "\n",
    "Adam (_i.e., Adaptive Moments_) is sort of **RMSProp + Momentum Optimization**.\n",
    "\n",
    "> LR scaling $\\leftarrow$ uses both the **first** and the **second** moment of past gradients.\n",
    "\n",
    "- - -\n",
    "\n",
    "_NOTE: Don't confuse \"moment\" of a quantity with \"momentum\":_\n",
    "    \n",
    "$$ moment\\_n = E~[X^n]$$\n",
    "\n",
    "- _moment_1 = **average**_\n",
    "- _moment_2 = **uncentered variance** (variance without centering around the mean)_\n",
    "   \n",
    "_In other words, RMSProp  uses the first moment $-$ with $E[]$ being an exponentially-weighted average._\n",
    "    \n",
    "_**Adam also** makes use of the second moments of the historical gradients._\n",
    "\n",
    "- - -\n",
    "    \n",
    "**TL;DR:** Adam is arguably the **best-performing** optimizer on average:\n",
    "- quite **robust** with respect to the choice of hyperpars\n",
    "- usually works with **default** hypepars\n",
    "\n",
    "$\\rightarrow$ Safe choice!\n",
    "    \n",
    "    \n",
    "<font size=3><u>**Reading material**</u><font>\n",
    "    \n",
    "At this [page](https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html) you will find graphical explanations of these [and more] optimizers as well as their mathematical formulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-phone",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "In DL, it is common to cycle the training on the same data $N$ times $-$ Each pass is called \"**Epoch**\".\n",
    "\n",
    "Training rationale with **N_epochs** epochs, and **N_batches** batches:\n",
    "\n",
    "    0. set epoch = 0\n",
    "    1. Train on batch 1, update gradient\n",
    "       Continue train on batch 2, update gradient\n",
    "       Continue train on batch 3, update gradient\n",
    "       .. .\n",
    "       Continue train on batch N_batches, update gradient\n",
    "    3. epoch += 1\n",
    "       (epoch completed)\n",
    "    4. if epoch <= N_epochs: continue from 1 else stop\n",
    "    \n",
    "- - -\n",
    "\n",
    "<u>NOTE:</u>\n",
    "_This introduces bias towards the training data, but it usually more than compensated by a longer gradient descent._\n",
    "\n",
    "- - -\n",
    "\n",
    "We can display the values of the Loss as a function of  \"**time**\" $\\rightarrow$ **Learning Curves** (**Validation Curves**)\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.3.  Learning curves for train and validation set.<br>\n",
    "            (From <a href=\"https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "We can use them to spot **overfitting** (train goes much better than validation) or **underfitting** (training shows trend to to better):\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Good.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.4.A.  <b>A good fit</b>.<br>\n",
    "            Training and validation converge <u>and</u> they flatten.<br>\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Underfitting.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.4.B.  <b>Underfitting</b>.<br>\n",
    "            The training set shows a downward trend at the right edge: maybe a few more epochs could yield the best performance.<br>\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Overfitting.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.4.C.  <b>Overfitting</b>.<br>\n",
    "            The validation curve cannot keep up with the training curve.\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-purple",
   "metadata": {},
   "source": [
    "# What you will see in the Workshops\n",
    "\n",
    "You will explore 2 domains of DL networks:\n",
    "\n",
    "- **Supervised Learning - Regression** $~~~~\\rightarrow$ **Fully-connected Layers**\n",
    "\n",
    "- **Supervised Learning - Classification** $~\\rightarrow$ **Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "- **Unsupervised Learning - Generation**  $~\\rightarrow$ **Diffusion Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e9e4e",
   "metadata": {},
   "source": [
    "## Supervised Learning: A simple Deep NN\n",
    "\n",
    "We will enter the DL coding realm via a simple **Regression** problem.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/Regression_Models.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.1.  The struggle of Regression.<br>\n",
    "            (Adapted from <a href=\"https://xkcd.com/2048/\">xkcd</a>)<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/NN_Regression.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.2.  A simple Deep NN can be used as a regressor.<br>\n",
    "            (From <a href=\"https://medium.com/@rajatgupta310198/getting-started-with-neural-network-for-regression-and-tensorflow-58ad3bd75223\">here</a>)<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cce6d",
   "metadata": {},
   "source": [
    "## Supervised Learning: CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746c292",
   "metadata": {},
   "source": [
    "**CNNs** excel at classifying images thanks to their **Convolutional Layers**:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1000>\n",
    "        <img src=\"images/CNN_Architecture.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.3.  A prototypical CNN architecture.<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- <font color='darkred'>**Convolution**</font> $~\\rightarrow$ Filters that scan the image to detect different features\n",
    "- <font color='darkgreen'>**Pooling**</font> $~~~~~~~~\\rightarrow$ Reduce dimensionality to increase abstraction\n",
    "- <font color='darkblue'>**Flattening**</font> $~~~~\\rightarrow$ Encodes features into variables\n",
    "- <font color='purple'>**Dense Layers**</font> $\\rightarrow$ Feature classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-meter",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "CNNs require lots of data $\\rightarrow$  **Transfer Learning** helps to address this issue:\n",
    "\n",
    "    1. Pre-train the convolutional/pooling with any extended image dataset\n",
    "    2. Freeze those parameters\n",
    "    3. Fit the classifier part on your astro images\n",
    "\n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/CNN_Transfer.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.4.  Don't have enough data?  Just add cats and dogs, SMH ...<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "> The pre-training will teach the CNN to **recognize features** in images (shapes, edges, etc.).\n",
    "\n",
    "See [Ackerman et al. 2017, MNRAS, 479, 415](https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract) $-$ identify mergers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-wrong",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Generating Data\n",
    "\n",
    "We will show how **versatile** DL is by training a model to generate mock samples from some _empirical data distribution_.\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/Generated_Data_NN.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "<table><tr>\n",
    "    <td width=700>\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.5. A NN can be trained to learn some data distribution and draw samples from it.<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-weekly",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Dinosaurs.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "There are several libraries that implement **Deep Learning routines** in Python.\n",
    "\n",
    "For the Woskshops we will focus on [**Tensorflow**](https://www.tensorflow.org/) ($\\rightarrow$ via [**Keras**](https://keras.io/)), and [**Pytorch**](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-watch",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Tensorflow_Logo.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "**Tensorflow** does the _heavy lifting_ for us:\n",
    "- pre-defined **functions** and **layers**\n",
    "- automatically computes the **derivatives** (for Backprop!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-press",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "**Keras** is an **API (Application Programming Interface)** $-$ it supports multiple libraries, e.g.:\n",
    "- Tensorflow (Google)\n",
    "- CNTK (Microsoft)\n",
    "- MXNet (Apache)\n",
    "- Theano\n",
    "\n",
    "It provides _high-level_ functionalities built on e.g. Tensorflow.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Keras.png\">\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69502564",
   "metadata": {},
   "source": [
    "You can see it imported in different ways, because it is **mantained separately** by both Tensorflow and Keras (functionalities _might_ differ slightly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39c566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 14:32:01.651740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 14:32:01.789676: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:01.789696: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-09 14:32:02.560098: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:02.560198: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:02.560207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Install: Anaconda -> Tensorflow -> Keras\n",
    "#\n",
    "import keras.models\n",
    "# and\n",
    "from keras import backend as K\n",
    "# use Keras repository code\n",
    "\n",
    "# ... OR\n",
    "\n",
    "import tensorflow.keras\n",
    "# use TensorFlow repository code itself (recommended: better maintaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ae999",
   "metadata": {},
   "source": [
    "It can be coded in 3 \"_styles_\":\n",
    "\n",
    "<u>Sequential API</u>\n",
    "\n",
    "  >**PROs:** Simpler (stack of layers)<br>\n",
    "  >**CONs:** Single-input, single-output\n",
    "\n",
    "<u>Functional API</u>\n",
    "\n",
    "  > **PROs:** More flexible, multi-input, multi-output<br>\n",
    "  > **CONs:** Steep learning curve, need to understand tensor programming\n",
    "  \n",
    "<u>Subclassing API</u>\n",
    "\n",
    "> _For dynamic behaviors (advanced School?)_  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c7cc2",
   "metadata": {},
   "source": [
    "### Keras Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50af5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 20)                15700     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,910\n",
      "Trainable params: 15,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 14:32:03.713659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 14:32:03.714059: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714215: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714593: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714960: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-06-09 14:32:03.714981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-09 14:32:03.715932: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "\n",
    "# Either:\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# ... OR:\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(20, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa11e16",
   "metadata": {},
   "source": [
    "### Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03682252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                220       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 430\n",
      "Trainable params: 430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(10,))\n",
    "x = layers.Dense(20, activation='relu')(inputs)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "#model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3e647",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Pytorch_Logo.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Pretty much the same concept:\n",
    "- **Sequential API**\n",
    "- **Functional API**\n",
    "\n",
    "We will use the Functional API, which in Pytorch is istantiated a _class_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0fc6f",
   "metadata": {},
   "source": [
    "### Pytorch Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cd7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=784, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model:\n",
    "model = MyModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfdc17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrostat23",
   "language": "python",
   "name": "astrostat23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "558.987px",
    "width": "693.987px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "399.902px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
