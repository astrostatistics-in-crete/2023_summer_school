{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**Classical Statistics: Advanced Topics**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import corner\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a simple example: Fitting a spectral line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.load(\"data/lithium_line_example.npy\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(spectrum['wavelength'], spectrum['flux'])\n",
    "ax[1].errorbar(spectrum['wavelength'], spectrum['flux'], yerr=spectrum['flux_error'], fmt='o', marker='.')\n",
    "\n",
    "ax[1].set_xlim(6705, 6709)\n",
    "ax[1].set_ylim(1200, 2700)\n",
    "\n",
    "for a in ax:\n",
    "    a.grid()\n",
    "\n",
    "ax[0].set_xlabel(r'Wavelength ($\\AA$)')\n",
    "ax[1].set_xlabel(r'Wavelength ($\\AA$)')\n",
    "ax[0].set_ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by subtracting the continuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithium_line = spectrum[spectrum['wavelength'] > 6705]\n",
    "lithium_line = lithium_line[lithium_line['wavelength'] < 6709]\n",
    "\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by describing our model: a linear continuum plus a Gaussian absorption feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_flux(p, wavelength):\n",
    "    \"\"\"Return the flux for our model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    wavelength : float\n",
    "        The input wavelength(s) at which we wish to calculate our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    flux : float\n",
    "        The model flux(es) calculated by our model\n",
    "    \"\"\"\n",
    "    \n",
    "    m, b, c, sigma, loc = p\n",
    "    \n",
    "    linear_part = m*(wavelength-6707) + b\n",
    "    gaussian_part = -c * norm.pdf(wavelength, loc=loc, scale=sigma)\n",
    "    \n",
    "    return linear_part + gaussian_part\n",
    "\n",
    "def ln_likelihood(p, data):\n",
    "    \"\"\"Return the likelihood of our model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    data : ndarray\n",
    "        The observed data from which we wish to calculate the likelihood of our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ln_likelihood : float\n",
    "        The log likelihood of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    m, b, c, sigma, loc = p\n",
    "    if sigma <= 0: return -np.inf\n",
    "    if loc < 6706 or loc > 6708: return -np.inf\n",
    "    \n",
    "    # First, calculate the model fluxes at the observed wavelengths\n",
    "    model_fluxes = model_flux(p, data['wavelength'])\n",
    "    \n",
    "    # Now capture our observed fluxes\n",
    "    observed_fluxes = data['flux']\n",
    "    observed_flux_errors = data['flux_error']\n",
    "    \n",
    "    # Compare the two\n",
    "    ln_likelihood = -np.log(observed_flux_errors * np.sqrt(2*np.pi)) - (model_fluxes - observed_fluxes)**2 / (2 * observed_flux_errors**2)\n",
    "        \n",
    "    return sum(ln_likelihood)\n",
    "\n",
    "def neg_ln_likelihood(p, data):\n",
    "    \"\"\"Wrapper for ln_likelihood to return the negative of the log likelihood.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    data : ndarray\n",
    "        The observed data from which we wish to calculate the likelihood of our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_ln_likelihood : float\n",
    "        The negative of the log likelihood of our model\n",
    "    \"\"\"\n",
    "\n",
    "    return -ln_likelihood(p, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a trial solution and use scipy minimize to find a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = 40\n",
    "# b = -265800\n",
    "m = 40\n",
    "b = 2400\n",
    "c = 400\n",
    "sigma = 0.2\n",
    "loc = 6707.1\n",
    "\n",
    "p0 = m, b, c, sigma, loc\n",
    "\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(p0, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C1')\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(neg_ln_likelihood, p0, method='Nelder-Mead', args=lithium_line)\n",
    "\n",
    "best_p = res.x\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(best_p, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C1')\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the likelihood vary with model parameters?\n",
    "\n",
    "Let's see how the likelihood will vary with the model parameter $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(12, 2.5))\n",
    "\n",
    "m_best, b_best, c_best, sigma_best, loc_best = best_p\n",
    "\n",
    "labels = ['m', 'b', 'c', r'$\\sigma$', r'$\\mu$']\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    test_set = best_p[j] * np.linspace(0.8, 1.2, 100)\n",
    "    ll = np.zeros(len(test_set))\n",
    "\n",
    "    trial_p = deepcopy(best_p)\n",
    "    \n",
    "    for i, test_val in enumerate(test_set):\n",
    "\n",
    "        trial_p[j] = test_val\n",
    "        \n",
    "        ll[i] = ln_likelihood(trial_p, lithium_line)\n",
    "\n",
    "\n",
    "    ax[j].plot(test_set, ll)\n",
    "\n",
    "    ax[j].axvline(best_p[j], color='k', linestyle='dashed')\n",
    "    \n",
    "    ax[j].set_title(labels[j])\n",
    "    ax[j].grid()\n",
    "\n",
    "    ax[j].set_xlabel(r'$\\lambda (\\AA)$')\n",
    "ax[0].set_ylabel('ln $\\mathcal{L}$')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_best, b_best, c_best, sigma_best, loc_best = best_p\n",
    "\n",
    "\n",
    "loc_set = loc_best * np.linspace(0.9999, 1.0001, 100)\n",
    "ll = np.zeros(len(loc_set))\n",
    "\n",
    "trial_p = deepcopy(best_p)\n",
    "    \n",
    "for i, test_val in enumerate(loc_set):\n",
    "\n",
    "    trial_p[4] = test_val\n",
    "\n",
    "    ll[i] = ln_likelihood(trial_p, lithium_line)\n",
    "\n",
    "\n",
    "plt.plot(test_set, ll)\n",
    "\n",
    "plt.axvline(best_p[j], color='k', linestyle='dashed')\n",
    "\n",
    "plt.title('Line Center')\n",
    "plt.ylabel('ln $\\mathcal{L}$')\n",
    "plt.xlabel(r'$\\lambda (\\AA$')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Matrix\n",
    "\n",
    "Now that we have a maximum likelihood estimate, how do we calculate the errors on our solution? After all, our solution is only as good as the error bars allow possible solutions. This is where a Fisher matrix can come in handy.\n",
    "\n",
    "If the maximum likelihood is the point in parameter space where the derivative of the log likelihood function is equal to zero: \n",
    "$$\\frac{\\partial \\log L(y_i | \\theta)}{\\partial \\theta} = 0,$$\n",
    "\n",
    "then the Fisher matrix is essentially the multi-variate form of the expectation value $E()$ of all the second derivatives of the log of the likelihood:\n",
    "\n",
    "$$ F_{j,k} = \n",
    "    \\begin{bmatrix} \n",
    "        -E\\left[ \\frac{\\partial^2}{\\partial \\theta_1^2} \\log L(y_i | \\theta) \\right] & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_1 \\partial \\theta_2} \\log L(y_i | \\theta) \\right] & \\dots & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_1 \\partial \\theta_N} \\log L(y_i | \\theta) \\right] \\\\\n",
    "        -E\\left[ \\frac{\\partial^2}{\\partial \\theta_2 \\partial \\theta_1} \\log L(y_i | \\theta) \\right] & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_2^2} \\log L(y_i | \\theta) \\right] & \\dots & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_2 \\partial \\theta_N} \\log L(y_i | \\theta) \\right] \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        -E\\left[ \\frac{\\partial^2}{\\partial \\theta_N \\partial \\theta_1} \\log L(y_i | \\theta) \\right] & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_N \\partial \\theta_2} \\log L(y_i | \\theta) \\right] & \\dots & -E\\left[ \\frac{\\partial^2}{\\partial \\theta_N^2} \\log L(y_i | \\theta) \\right]\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, assuming all the observations are Gaussian (we have already assumed they are in our likelihood function), these derivatives are reduced to a much nicer expression. First, we take the log of the likelihood.\n",
    "\n",
    "$$ \\log L(y_i | \\theta) = -\\frac{1}{2} \\log 2 \\pi - \\frac{1}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} [y_i - y(\\lambda)]^2 $$\n",
    "\n",
    "Now, we take the partial derivative with respect to our model parameter $\\theta_i$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_i} \\log L(y_i | \\theta) = - \\frac{1}{\\sigma^2} [y_i - y(\\lambda)] \\frac{\\partial y(\\lambda)}{\\partial \\theta_i} $$\n",
    "\n",
    "Then, for any one element of the Fisher matrix, we have:\n",
    "\n",
    "$$ -E\\left[ \\frac{\\partial^2}{\\partial \\theta_j \\partial \\theta_k} \\log L(y_i | \\theta) \\right] = E\\left[ \\frac{1}{\\sigma^4} [y_i - y(\\lambda)]^2 \\frac{\\partial y(\\lambda)}{\\partial \\theta_j} \\frac{\\partial y(\\lambda)}{\\partial \\theta_k} \\right] $$\n",
    "\n",
    "Since $E\\left[[y_i - y(\\lambda)]^2\\right] = \\sigma^2$, the individual matrix elements of the Fisher matrix can be represented as:\n",
    "\n",
    "$$ F_{j,k} = \\sum_{i=1}^N \\frac{1}{\\sigma^2} \\frac{\\partial y(\\lambda)}{\\partial \\theta_j} \\frac{\\partial y(\\lambda)}{\\partial \\theta_k} $$\n",
    "\n",
    "### One more thing - calculating numerical derivatives\n",
    "To do that, we need to calculate numerical derivatives. A simple way to do that is to calculate the value of $y(\\lambda | \\theta_i)$ and $y(\\lambda | \\theta_i+\\delta)$. The difference in the two values divided by the difference in the model parameter gives a numerical derivative:\n",
    "\n",
    "$$ \\frac{\\partial y(\\lambda)}{\\partial \\theta_j} = \\frac{y(\\lambda | \\theta + \\delta) - y(\\lambda | \\theta)}{(\\theta + \\delta) - \\theta} = \\frac{y(\\lambda | \\theta + \\delta) - y(\\lambda | \\theta)}{\\delta} $$\n",
    "\n",
    "### Now, we can code it up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def calc_fisher_matrix(data, best_p, delta_p=1e-5):\n",
    "\n",
    "    N = len(best_p)\n",
    "    \n",
    "    # Create the Fisher matrix\n",
    "    F = np.zeros((N, N))\n",
    "\n",
    "    \n",
    "    # Now we calculate the numerical derivatives with our five model parameters    \n",
    "    for i in range(N):\n",
    "        \n",
    "        # Add the delta to the theta_i model parameter\n",
    "        test_p_i = deepcopy(best_p)\n",
    "        test_p_i[i] += delta_p*best_p[i]\n",
    "        \n",
    "        # Calculate the difference in the resulting model fluxes\n",
    "        test_flux = model_flux(test_p_i, data['wavelength'])\n",
    "        delta_flux = test_flux - data['flux']    \n",
    "        \n",
    "        # Now, we can calculate our derivatives\n",
    "        derivs_i = delta_flux / (delta_p*best_p[i])\n",
    "        \n",
    "        # Repeat the same calculation as above for each of the j model parameters\n",
    "        for j in range(N):\n",
    "            test_p_j = deepcopy(best_p)\n",
    "            test_p_j[j] += delta_p*best_p[j]\n",
    "\n",
    "            test_flux = model_flux(test_p_j, data['wavelength'])\n",
    "            delta_flux = test_flux - data['flux']    \n",
    "            derivs_j = delta_flux / (delta_p*best_p[j])\n",
    "            \n",
    "            # Now we can put these together to calculate the Fisher matrix\n",
    "            F[i,j] = sum(1/data['flux_error']**2 * derivs_i * derivs_j)\n",
    "    \n",
    "            \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = calc_fisher_matrix(lithium_line, best_p)\n",
    "\n",
    "for i in range(len(best_p)):\n",
    "    print(F[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we proceed, a side note on numerical derivatives\n",
    "\n",
    "There are fancier methods, but we are just using a simple finite difference scheme. However, its accuracy depends on the $\\delta$ we use: too large and the model starts to change (i.e., adding a $\\delta$ moves us significantly away from the maximum likelihood point) and too small and we run into numerical difficulties in calculating small numbers. \n",
    "\n",
    "Our default value in the function we wrote above is $10^-5$. Let's see if that's a good value. We can do this by testing a wide range of $\\delta$ values, across a large dynamical range. Since we don't have a sense a priori of what the correct scale is, we will recalculate the diagonal elements of the Fisher matrix over a logarithmic range of $\\delta$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_p_set = np.logspace(-8, -3, 100)\n",
    "\n",
    "sigma_m = np.zeros(len(delta_p_set))\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(8, 8))\n",
    "\n",
    "\n",
    "labels = ['m', 'b', 'c', 'sigma', 'loc']\n",
    "\n",
    "for j in range(len(best_p)):\n",
    "    for i, delta_p in enumerate(delta_p_set):\n",
    "\n",
    "        F = calc_fisher_matrix(lithium_line, best_p, delta_p=delta_p)\n",
    "        test_new_matrix = np.mat(F).I\n",
    "        sigma_m[i] = np.sqrt(test_new_matrix[j,j])\n",
    "\n",
    "    ax[j].plot(delta_p_set, sigma_m)\n",
    "    \n",
    "    ax[j].set_xscale('log')\n",
    "\n",
    "    ax[j].set_xlim(1e-8, 1e-3)\n",
    "    ax[j].set_ylabel(labels[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly things start to become strange at very small $\\delta$ of $10^{-8}$ and again at a very large $\\delta$ of $10^{-4}$. Somewhere in the middle, the parameters seem converged. $10^{-5}$ is reasonable, but we could probably go lower to $10^{-6}$ if we wanted to really focus on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix\n",
    "\n",
    "So, why is the Fisher matrix so valuable? Let's return to our original question: how do we calculate the error bars on our maximum likelihood estimate? It turns out that the inverse of the Fisher matrix is the covariance matrix:\n",
    "\n",
    "$$ F_{j,k}^{-1} = {\\rm cov} =\n",
    "    \\begin{bmatrix} \n",
    "        \\sigma_{\\theta_1}^2 & \\sigma_{\\theta_1} \\sigma_{\\theta_2} & \\dots & \\sigma_{\\theta_1} \\sigma_{\\theta_N} \\\\\n",
    "        \\sigma_{\\theta_2} \\sigma_{\\theta_1} & \\sigma_{\\theta_2}^2 & \\dots & \\sigma_{\\theta_2} \\sigma_{\\theta_N} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\sigma_{\\theta_N} \\sigma_{\\theta_1} & \\sigma_{\\theta_N} \\sigma_{\\theta_2} & \\dots & \\sigma_{\\theta_N}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Diagonal matrices of the standard deviations can be factored out:\n",
    "$$ {\\rm cov} = \n",
    "    \\begin{bmatrix} \n",
    "        \\sigma_{\\theta_1} &  &  & 0 \\\\\n",
    "         & \\sigma_{\\theta_2} &  &  \\\\\n",
    "         &  & \\ddots &  \\\\\n",
    "        0 &  &  & \\sigma_{\\theta_N}\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix} \n",
    "        1 & \\rho_{\\theta_1, \\theta_2} & \\dots & \\rho_{\\theta_1, \\theta_N} \\\\\n",
    "        \\rho_{\\theta_2, \\theta_1} & 1 & \\dots & \\rho_{\\theta_2, \\theta_N} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\rho_{\\theta_N, \\theta_1} & \\rho_{\\theta_N, \\theta_2}  & \\dots & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "        \\sigma_{\\theta_1} &  &  & 0 \\\\\n",
    "         & \\sigma_{\\theta_2} &  &  \\\\\n",
    "         &  & \\ddots &  \\\\\n",
    "        0 &  &  & \\sigma_{\\theta_N}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The off-diagonal terms of the middle matrix (the correlation matrix) are the same Pearson correlation coefficients you have already seen. \n",
    "\n",
    "### So, how can we visualize the correlation matrix?\n",
    "\n",
    "There are a few ways, but we can take a simple approach: Let's start by generating lots of samples based on the covariance matrix, then use Dan Foreman-Mackey's `corner` package to plot it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by re-generating the covariance matrix\n",
    "F = calc_fisher_matrix(lithium_line, best_p)\n",
    "cov = np.mat(F).I\n",
    "\n",
    "# We also need a set of \"mean\" values, in this case, the maximum likelihood\n",
    "mean = best_p\n",
    "\n",
    "# Generate the samples using the rvs method (rvs = random variates)\n",
    "samples = multivariate_normal.rvs(mean, cov, size=10000)\n",
    "\n",
    "# Use DFM's corner package\n",
    "corner.corner(samples)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret our results?\n",
    "\n",
    "For one thing, we find that a few parameters are showing significant correlations. For instance the slope of the continuum ($m$) is correlated with the depth ($c$) and width ($\\sigma$) of the Gaussian absorption feature.\n",
    "\n",
    "\n",
    "Next, we can check, do these results seem reasonable? One way to check this is to plot a bunch of results against the data. Note, this is also an excellent way of demonstrating the model fit along with its uncertainty in a publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "\n",
    "# Let's take the first 20 samples and plot them\n",
    "for sample in samples[:20]:\n",
    "\n",
    "    model_fluxes = model_flux(sample, model_wavelengths)\n",
    "\n",
    "    plt.plot(model_wavelengths, model_fluxes, color='k', alpha=0.05)\n",
    "\n",
    "# Now, compare against the data\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Error Propagation\n",
    "\n",
    "What we've unwittingly done is Monte Carlo error propagation: we've visually expressed the set of reasonable model solutions in a statistically meaningful way. If we want to quantitatively propagate errors, we can follow the same procedure. \n",
    "\n",
    "\n",
    "### Equivalent width\n",
    "\n",
    "Let's take a simple example: calculating the equivalent width of the lithium absorption feature. When one goes to compare the spectral feature against a stellar atmospheric model to calculate a lithium abundance, one uses the \"equivalent width\" of the feature rather than the raw data. Mathematically, an equivalent width $W$ can be calculated by integrating the fractional difference between the continuum flux $F_c$ and the line flux $F_s$:\n",
    "\n",
    "$$ W_{\\lambda} = \\int \\frac{F_c - F_s}{F_c}\\ {\\rm d}\\lambda = \\int 1 - \\frac{F_s}{F_c}\\ {\\rm d}\\lambda $$\n",
    "\n",
    "However, things get a little tricky since our continuum isn't normalized. So let's take care of that first. If we normalize our spectrum so the continuum is at unity, and the Gaussian feature along with it, then the equivalent width calculation is simply:\n",
    "\n",
    "$$ W_{\\lambda} = \\int F_s^*\\ {\\rm d}\\lambda $$\n",
    "\n",
    "Since it's a Gaussian, the integral could be computed analytically. However, just to demonstrate how this is done for an arbitrarily complex function, we'll use a numerical integration.\n",
    "\n",
    "Let's code it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_equivalent_width_integrand(wavelength, p):\n",
    "    \"\"\"Return the flux for our model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    wavelength : float\n",
    "        The input wavelength(s) at which we wish to calculate our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    flux : float\n",
    "        The model flux(es) calculated by our model\n",
    "    \"\"\"\n",
    "    \n",
    "    m, b, c, sigma, loc = p\n",
    "    \n",
    "    linear_part = m*(wavelength-6707) + b\n",
    "    gaussian_part = -c * norm.pdf(wavelength, loc=loc, scale=sigma)\n",
    "\n",
    "    # Normalize Gaussian depth to line center\n",
    "    c_normed = c / (m*(loc-6707) + b)\n",
    "    \n",
    "    # Normalized Gaussian\n",
    "    normed_gaussian = c_normed * norm.pdf(wavelength, loc=loc, scale=sigma)\n",
    "    \n",
    "    return normed_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can follow the same process as before: We generate a number of model parameter samples based on our maximum likelihood estimate and covariance matrix (in this case we use the samples we already generated) and separately calculate the equivalent width for each of those samples. Since the numerical integration is a somewhat expensive operation, we only need calculate the equivalent widht of a thousand or so samples to derive our estimate of the equivalent width.\n",
    "\n",
    "We'll use `Scipy`'s `quad` routine to calculate the integral. This function requires bounds on the integral, which we want to make large enough to capture the entire line. In this case, we'll use 6706 Ang to 6708 Ang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = []\n",
    "for p in samples[0:1000]:\n",
    "    res.append(quad(calc_equivalent_width_integrand, 6706, 6708, p)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.135, 0.170, 30)\n",
    "plt.hist(res, bins=bins)\n",
    "\n",
    "plt.axvline(np.mean(res), color='k')\n",
    "plt.axvline(np.mean(res) - np.std(res), color='k', linestyle='dashed')\n",
    "plt.axvline(np.mean(res) + np.std(res), color='k', linestyle='dashed')\n",
    "\n",
    "plt.xlabel(r'Equivalent Width ($\\AA$)')\n",
    "plt.ylabel('P(W)')\n",
    "\n",
    "plt.text(0.7, 0.9, r'$W_{\\rm Li}$ = %.3f$\\pm$%.3f $\\AA$'%(np.mean(res), np.std(res)),\n",
    "         transform=plt.gca().transAxes)\n",
    "\n",
    "plt.gca().set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learned? \n",
    "\n",
    "We have derived an equivalent width of 0.153 +/- 0.006 angstroms. The key here is that not only do we have a measurement for the equivalent width, but we have derived error bars for it as well! These error bars were propagated from the model we derived for the lithium line, including the covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With your partner, discuss the Fisher Information\n",
    "\n",
    "Returning to the Fisher matrix, the diagonal elements are called the \"Fisher Information\", $F(\\theta)$. Note the dependence on $\\theta$, as each parameter in a model will have a different Fisher Information. Spend a few minutes with your partner discussing the these diagonal elements. Use its relation with the covariance matrix to try and get a sense of what a large Fisher Information means about a parameter and what a small Fisher Information means about a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1: calculate the equivalent widths of lithium in other stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_1 = np.load(\"data/lithium_line_test_1.npy\")\n",
    "spectrum_2 = np.load(\"data/lithium_line_test_2.npy\")\n",
    "spectrum_3 = np.load(\"data/lithium_line_test_3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n",
    "\n",
    "\n",
    "for i, spectrum in enumerate([spectrum_1, spectrum_2, spectrum_3]):\n",
    "\n",
    "    lithium_line = spectrum[spectrum['wavelength'] > 6705]\n",
    "    lithium_line = lithium_line[lithium_line['wavelength'] < 6709]\n",
    "\n",
    "\n",
    "    ax[i].errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "                   yerr=lithium_line['flux_error'], \n",
    "                   fmt='o', marker='.')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(r'Wavelength ($\\AA$)')\n",
    "    a.set_ylabel('Counts')\n",
    "\n",
    "    a.grid()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2: Blended Lines\n",
    "\n",
    "The lithium line at 6707.78 Ang is actually blended with a small Fe I line at 6707.441 Ang. Can you combine both into a single model to obtain a more accurate equivalent width?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.load(\"data/lithium_line_example.npy\")\n",
    "\n",
    "lithium_line = spectrum[spectrum['wavelength'] > 6705]\n",
    "lithium_line = lithium_line[lithium_line['wavelength'] < 6709]\n",
    "\n",
    "\n",
    "\n",
    "m = 40\n",
    "b = 2400\n",
    "c = 400\n",
    "sigma = 0.2\n",
    "loc = 6707.1\n",
    "\n",
    "p0 = m, b, c, sigma, loc\n",
    "\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(p0, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C1', label='Li')\n",
    "\n",
    "\n",
    "m = 40\n",
    "b = 2400\n",
    "c = 50\n",
    "sigma = 0.2\n",
    "loc = 6706.8\n",
    "\n",
    "p0 = m, b, c, sigma, loc\n",
    "\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(p0, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C2', label='Fe I')\n",
    "\n",
    "\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.legend(loc = 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "cc5457b2014448c181ed536a3544a68d",
   "lastKernelId": "9fe88c74-12f4-4672-8a01-d4b0cee2ba5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
