{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Advanced Topics\n",
    "\n",
    "Here are are going to deal with more complex data problems: outliers and errors in both the x- and y-axes. First, let's load up the libraries we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review the basics: fitting a line to data\n",
    "\n",
    "We'll start with a demonstrative example using a dataset taken from the pedagogical manuscript [Data analysis recipes: Fitting a model to data](https://arxiv.org/pdf/1008.4686.pdf). This data has all the characteristics to make the problem interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data/Hogg_2010_example.npy\")\n",
    "\n",
    "print(\"There are %i\"%len(data) + \" rows in the dataset\")\n",
    "\n",
    "print(data.dtype.names)\n",
    "for line in data:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(data['x'], data['y'], xerr=data['x_err'], yerr=data['y_err'], marker='.', fmt='o', zorder=10)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you'll notice that we have data with x- and y-axis values that have error bars in both dimensions as well as covariance between the error bars. The goal is to fit a line to this data. \n",
    "\n",
    "### 1. As first (incorrect) approximation, let's ignore the x-error bars\n",
    "\n",
    "Let's start by defining our likelihood function:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{y_i}}} \\exp\\left[ - \\frac{(y_i - y(x))^2}{2 \\sigma^2_{y_i}} \\right], $$\n",
    "where $y(x)$ is our linear model: \n",
    "\n",
    "$$ y(x) = mx + b $$\n",
    "\n",
    "Code up a function that returns the sum of the log of the likelihood in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood(p, data):\n",
    "    \n",
    "    m, b = p\n",
    "    \n",
    "    \n",
    "    return sum(np.log(likelihood))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can feed this function into `emcee` to find the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nwalkers = 32\n",
    "\n",
    "# Let's give the code a reasonable starting point\n",
    "starting_point = [2.2, 50]\n",
    "\n",
    "# Generate the initial point for the walkers\n",
    "p0 = np.zeros((nwalkers, ndim))\n",
    "p0 = np.random.normal(loc=1, scale=1.0e-2, size=(nwalkers, ndim)) * starting_point\n",
    "\n",
    "# Create the sampler object\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_likelihood, args=[data])\n",
    "\n",
    "# Run the sampler\n",
    "state = sampler.run_mcmc(p0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ndim, 1, figsize=(12, 6))\n",
    "\n",
    "for i in range(ndim):\n",
    "    for j in range(nwalkers):\n",
    "        ax[i].plot(sampler.chain[j,:,i], alpha=0.2, color='k')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the distribution of and covariance between the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove burn-in\n",
    "chains = sampler.chain[:,200:]\n",
    "\n",
    "# Flatten the chains\n",
    "n_chains, n_steps, n_var = chains.shape\n",
    "flatchain = chains.reshape((n_chains*n_steps, n_var))\n",
    "\n",
    "corner.corner(flatchain)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(flatchain), size=30)\n",
    "samples = flatchain[idx]\n",
    "\n",
    "plt.errorbar(data['x'], data['y'], xerr=data['x_err'], yerr=data['y_err'], marker='.', fmt='o', zorder=10)\n",
    "\n",
    "tmp_x = np.linspace(40, 300, 100)\n",
    "for sample in samples:\n",
    "    m = sample[0]\n",
    "    b = sample[1]\n",
    "    plt.plot(tmp_x, m*tmp_x+b, color='k', alpha=0.1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dealing with outliers: a mixture model\n",
    "\n",
    "Now, things get interesting. To deal with outliers, we are going to add a bunch of variables to our model. First, we will add one variable $q_i$ for every data point, which is either a 0 or a 1, corresponding to whether that point is an outlier or not. \n",
    "\n",
    "We will additionally add a model for our outliers. In this case we'll use a simple model described by a Gaussian in the $y$-axis only, which includes two extra dimensions: a mean, $Y_b$, and a variance, $V_b$. \n",
    "\n",
    "Finally, there is a parameter $P_b$, constrained to be between 0 and 1 that indicates the fraction of our data points that are part of the background. \n",
    "\n",
    "Combined, our model now has $N+5$ model parameters ($m$, $b$, $\\{q_i\\}_{i=1}^{N}$, $P_b$, $Y_b$, $V_b$), where $N$ is the number of data points. Note that it is not a problem for our model to have more parameters than data points.\n",
    "\n",
    "Now, our likelihood is more complex and is comprised of two parts: one for the source and one for the background:\n",
    "\n",
    "$$ \\mathcal{L} = \\prod_{i=1}^N P_{\\rm background}(y_i|P_b, Y_b, V_b)^{1-q_i}\\ P_{\\rm source}(y_i|m, b)^{q_i} $$\n",
    "\n",
    "Our background model is a Gaussian in $y$ only, so\n",
    "\n",
    "$$ P_{\\rm background}(y_i|P_b, Y_b, V_b) = \\frac{1}{\\sqrt{2 \\pi (\\sigma_{y_i}^2 + V_b)}} \\exp \\left[ - \\frac{(y_i - Y_b)^2}{2 (\\sigma_{y_i}^2 + V_b)} \\right] $$\n",
    "\n",
    "and our likelihood for the model is unchanged from before:\n",
    "\n",
    "$$ P_{\\rm source}(y_i|m, b) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{y_i}^2}} \\exp \\left[ - \\frac{(y_i - f(y))^2}{2 \\sigma_{y_i}^2} \\right] $$\n",
    "\n",
    "How do we deal with a model with so many parameters? Well, luckily, most of these can be marginalized out. Let's see how. From Bayesian calculus, we can factor our likelihood:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    " P(m, b, \\{q_i\\}_{i=1}^{N}, P_b, Y_b, V_b) =& P(\\{q_i\\}_{i=1}^{N} | m, b, P_b, Y_b, V_b)\\  P(m, b, P_b, Y_b, V_b) \\\\\n",
    "    =& P(\\{q_i\\}_{i=1}^{N} | P_b)\\ P(m, b, P_b, Y_b, V_b) \\\\\n",
    " \\end{aligned}\n",
    "$$\n",
    "\n",
    "Where we have made the realization that each of the $q_i$'s depends only on $P_b$. We can remove all the $q_i$'s by analytically integrating over them (really a sum over each q_i being equal to 0 or 1). Through some math, which I will not entirely reproduce here, we can arrive at a reduced form of the likelihood:\n",
    "\n",
    "\n",
    "$$ \\mathcal{L} = \\prod_{i=1}^N P_b\\ P_{\\rm background}(y_i|P_b, Y_b, V_b) + (1-P_b)\\ P_{\\rm source}(y_i|m, b) $$\n",
    "\n",
    "Code this new likelihood into a function below. I will provide the priors and posterior function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood_outliers(p, data):\n",
    "    \n",
    "    m, b, P_b, Y_b, V_b = p\n",
    "    \n",
    "    \n",
    "    return sum(np.log(likelihood))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ln_prior_outliers(p):\n",
    "    \n",
    "    m, b, P_b, Y_b, V_b = p\n",
    "    \n",
    "    if P_b < 0 or 1 < P_b: return -np.inf\n",
    "    if V_b < 0 or V_b > 1000: return -np.inf\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def ln_posterior_outliers(p, data):\n",
    "\n",
    "    lp = ln_prior_outliers(p)\n",
    "    if np.isinf(lp): return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood_outliers(p, data)\n",
    "    if np.isnan(ll): return -np.inf\n",
    "\n",
    "    return lp + ll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 5\n",
    "nwalkers = 32\n",
    "\n",
    "starting_point = [2.2, 50, 0.05, 300, 200]\n",
    "\n",
    "p0 = np.zeros((nwalkers, ndim))\n",
    "p0 = np.random.normal(loc=1, scale=1.0e-2, size=(nwalkers, ndim)) * starting_point\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior_outliers, args=[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(p0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ndim, 1, figsize=(12, 6))\n",
    "\n",
    "for i in range(ndim):\n",
    "    for j in range(nwalkers):\n",
    "        ax[i].plot(sampler.chain[j,:,i], alpha=0.2, color='k')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove burn-in\n",
    "chains = sampler.chain[:,200:]\n",
    "\n",
    "# Flatten the chains\n",
    "n_chains, n_steps, n_var = chains.shape\n",
    "flatchain = chains.reshape((n_chains*n_steps, n_var))\n",
    "\n",
    "labels = ['m', 'b', r'$P_b$', r'$Y_b$', r'$V_b$']\n",
    "corner.corner(flatchain, labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.hist2d(sampler.flatchain[:,1], sampler.flatchain[:,0], \n",
    "              range=((-50, 100), (1.8, 2.7)), bins=50, plot_datapoints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(flatchain), size=30)\n",
    "samples = flatchain[idx]\n",
    "\n",
    "plt.errorbar(data['x'], data['y'], yerr=data['y_err'], marker='.', fmt='o', zorder=10)\n",
    "\n",
    "tmp_x = np.linspace(40, 300, 100)\n",
    "for sample in samples:\n",
    "    m = sample[0]\n",
    "    b = sample[1]\n",
    "    plt.plot(tmp_x, m*tmp_x+b, color='k', alpha=0.1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And what about the fraction of points with outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampler.flatchain[:,2], bins=np.linspace(0, 1, 50), density=True)\n",
    "\n",
    "plt.xlabel(r'$P_b$')\n",
    "plt.ylabel('Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What about x-axis errors?\n",
    "\n",
    "Here, we'll go back to our non-mixture model to come up with a treatment of errors on the x-axis. We'll do two things in this example. First, we'll switch from parameterizing our line as ($m$, $b$) into ($\\theta$, $b_{\\perp}$), where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m =& \\tan(\\theta) \\\\\n",
    "b_{\\perp} =& b \\cos(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Second, we will rotate the covariance matrix of each observation by an angle $\\theta$ so that the distance between our line and our model is in only one axis. To do this, we first define the unit vector orthogonal to the line:\n",
    "\n",
    "$$ \\hat{v} = \\left[ \\begin{array} -- \\sin \\theta \\\\ \\cos \\theta \\end{array} \\right] $$\n",
    "\n",
    "Now, we'll define a vector with our data point:\n",
    "\n",
    "$$ Z_i = \\left[ \\begin{array} -- x_i \\\\ y_i \\end{array} \\right] $$\n",
    "\n",
    "Then, the distance from the line to any individual data point is:\n",
    "\n",
    "$$ \\Delta = \\hat{v}^T Z_i - b \\cos \\theta $$\n",
    "\n",
    "We're almost there. The last step is to project the data point's covariance matrix $S_i$,\n",
    "\n",
    "$$ S_i = \\left[ \\begin{array}- \\sigma_{x_i}^2 & \\rho_{xy} \\sigma_{x_i} \\sigma_{y_i} \\\\ \\rho_{xy} \\sigma_{x_i} \\sigma_{y_i} & \\sigma_{y_i}^2 \\end{array} \\right] $$\n",
    "\n",
    "into an orthogonal variance $\\Sigma_i$, where:\n",
    "\n",
    "$$ \\Sigma_i^2 = \\hat{v}^T S_i \\hat{v} $$\n",
    "\n",
    "Finally, the log of the likelihood can be written as:\n",
    "$$ \\ln \\mathcal{L} = K - \\sum_{i=1}^N \\frac{\\Delta_i^2}{2 \\Sigma_i^2} $$\n",
    "\n",
    "Even if you didn't entirely follow the linear algebra steps to get here, compare it to the likelihood equation above, and it should start to look familiar.\n",
    "\n",
    "Since the matrix multiplication can be a bit tricky to get right in python, I am providing the likelihood code for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood_xyerr(p, data):\n",
    "\n",
    "    theta, b_perp = p\n",
    "\n",
    "    v_vec = np.mat(np.array([-np.sin(theta), np.cos(theta)])).T\n",
    "\n",
    "    Z = np.mat(np.array([data['x'], data['y']]))\n",
    "\n",
    "    delta = np.matmul(v_vec.T, Z) - b_perp\n",
    "    delta = np.array(delta)[0]\n",
    "    \n",
    "    S = np.array([[data['x_err']**2, data['rho_xy']*data['x_err']*data['y_err']], \n",
    "                  [data['rho_xy']*data['x_err']*data['y_err'], data['y_err']**2]])\n",
    "    S = np.swapaxes(S, 0, 2)\n",
    "    \n",
    "    sigma_2 = np.matmul(v_vec.T, np.matmul(S, v_vec).T)\n",
    "    sigma_2 = np.array(sigma_2)[0]\n",
    "\n",
    "    return -np.sum(delta**2 / (2*sigma_2))\n",
    "\n",
    "\n",
    "def ln_prior_xyerr(p):\n",
    "    \n",
    "    theta, b_perp = p\n",
    "\n",
    "    if theta > np.pi or theta < 0: return -np.inf\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def ln_posterior_xyerr(p, data):\n",
    "\n",
    "    lp = ln_prior_xyerr(p)\n",
    "    if np.isinf(lp): return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood_xyerr(p, data)\n",
    "    if np.isnan(ll): return -np.inf\n",
    "\n",
    "    return lp + ll\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will first run this model removing the first four points of our data set\n",
    "\n",
    "It turns out that the first four data points are the outliers, so we are cheating here to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nwalkers = 32\n",
    "\n",
    "\n",
    "starting_point = [1.09, 36]\n",
    "\n",
    "p0 = np.zeros((nwalkers, ndim))\n",
    "p0 = np.random.normal(loc=1, scale=1.0e-4, size=(nwalkers, ndim)) * starting_point\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior_xyerr, args=[data[4:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(p0, 500)\n",
    "# sampler.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ndim, 1, figsize=(12, 6))\n",
    "\n",
    "for i in range(ndim):\n",
    "    for j in range(nwalkers):\n",
    "        ax[i].plot(sampler.chain[j,:,i], alpha=0.2, color='k')\n",
    "        \n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel(r'$\\theta$ (rad.)')\n",
    "ax[1].set_ylabel(r'$b_{\\perp}$')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove burn-in\n",
    "chains = sampler.chain[:,200:]\n",
    "\n",
    "# Flatten the chains\n",
    "n_chains, n_steps, n_var = chains.shape\n",
    "flatchain = chains.reshape((n_chains*n_steps, n_var))\n",
    "\n",
    "labels = [r'$\\theta$', r'$b_{\\perp}$']\n",
    "corner.corner(flatchain, labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(flatchain), size=30)\n",
    "samples = flatchain[idx]\n",
    "\n",
    "plt.errorbar(data['x'], data['y'], \n",
    "             xerr=data['x_err'], yerr=data['y_err'], \n",
    "             marker='.', fmt='o', zorder=10)\n",
    "\n",
    "plt.errorbar(data['x'][4:], data['y'][4:], \n",
    "             xerr=data['x_err'][4:], yerr=data['y_err'][4:], \n",
    "             marker='.', fmt='o', zorder=10)\n",
    "\n",
    "\n",
    "tmp_x = np.linspace(30, 300, 100)\n",
    "for sample in samples:\n",
    "    \n",
    "    theta, b_perp = sample[0], sample[1]\n",
    "    m = np.tan(theta)\n",
    "    b = b_perp / np.cos(theta)\n",
    "        \n",
    "    plt.plot(tmp_x, m*tmp_x+b, color='k', alpha=0.1, zorder=1)\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.xlim(30, 300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fit using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nwalkers = 64\n",
    "\n",
    "\n",
    "starting_point = [1.09, 36]\n",
    "\n",
    "p0 = np.zeros((nwalkers, ndim))\n",
    "p0 = np.random.normal(loc=1, scale=1.0e-4, size=(nwalkers, ndim)) * starting_point\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior_xyerr, args=[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(p0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ndim, 1, figsize=(12, 6))\n",
    "\n",
    "for i in range(ndim):\n",
    "    for j in range(nwalkers):\n",
    "        ax[i].plot(sampler.chain[j,:,i], alpha=0.2, color='k')\n",
    "        \n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel(r'$\\theta$ (rad.)')\n",
    "ax[1].set_ylabel(r'$b_{\\perp}$')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove burn-in\n",
    "chains = sampler.chain[:,200:]\n",
    "\n",
    "# Flatten the chains\n",
    "n_chains, n_steps, n_var = chains.shape\n",
    "flatchain = chains.reshape((n_chains*n_steps, n_var))\n",
    "\n",
    "labels = [r'$\\theta$', r'$b_{\\perp}$']\n",
    "corner.corner(flatchain, labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(flatchain), size=30)\n",
    "samples = flatchain[idx]\n",
    "\n",
    "plt.errorbar(data['x'], data['y'], \n",
    "             xerr=data['x_err'], yerr=data['y_err'], \n",
    "             marker='.', fmt='o', zorder=10)\n",
    "\n",
    "\n",
    "tmp_x = np.linspace(30, 300, 100)\n",
    "for sample in samples:\n",
    "    \n",
    "    theta, b_perp = sample[0], sample[1]\n",
    "    m = np.tan(theta)\n",
    "    b = b_perp / np.cos(theta)\n",
    "        \n",
    "    plt.plot(tmp_x, m*tmp_x+b, color='k', alpha=0.1, zorder=1)\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.xlim(30, 300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done something statistically more rigorous, but now we are getting a worse result. To do this properly, we need to expand our model to handle the outliers. \n",
    "\n",
    "### 4. A mixture model that includes uncertainties in both the x- and y-axes\n",
    "\n",
    "Now write the likelihood function below that incorporates what you've learned from the previous exercises to include both the mixture model component to handle outliers and the transformation aspect to deal with both x- and y-uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood_xyerr_outlier(p, data):\n",
    "\n",
    "    theta, b_perp, P_b, Y_b, V_b = p\n",
    "\n",
    "        \n",
    "    return sum(np.log(likelihood_source + likelihood_outlier))\n",
    "    \n",
    "\n",
    "def ln_prior_xyerr_outlier(p):\n",
    "    \n",
    "    theta, b_perp, P_b, Y_b, V_b = p\n",
    "\n",
    "    if theta > np.pi or theta < 0: return -np.inf\n",
    "    if P_b < 0 or 0.5 < P_b: return -np.inf\n",
    "    if V_b < 0 or V_b > 1000: return -np.inf\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def ln_posterior_xyerr_outlier(p, data):\n",
    "\n",
    "    lp = ln_prior_xyerr_outlier(p)\n",
    "    if np.isinf(lp): return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood_xyerr_outlier(p, data)\n",
    "    if np.isnan(ll): return -np.inf\n",
    "\n",
    "    return lp + ll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 5\n",
    "nwalkers = 64\n",
    "\n",
    "\n",
    "theta = np.pi/2\n",
    "b = 40\n",
    "P_b = 0.5\n",
    "Y_b = 100\n",
    "V_b = 100\n",
    "\n",
    "starting_point = [theta, b, P_b, Y_b, V_b]\n",
    "\n",
    "p0 = np.zeros((nwalkers, ndim))\n",
    "p0 = np.random.normal(loc=1, scale=1.0e-6, size=(nwalkers, ndim)) * starting_point\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior_xyerr_outlier, args=[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(p0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ndim, 1, figsize=(12, 6))\n",
    "\n",
    "for i in range(ndim):\n",
    "    for j in range(nwalkers):\n",
    "        ax[i].plot(sampler.chain[j,:,i], alpha=0.2, color='k')\n",
    "        \n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel(r'$\\theta$ (rad.)')\n",
    "ax[1].set_ylabel(r'$b_{\\perp}$')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove burn-in\n",
    "chains = sampler.chain[:,200:]\n",
    "\n",
    "# Flatten the chains\n",
    "n_chains, n_steps, n_var = chains.shape\n",
    "flatchain = chains.reshape((n_chains*n_steps, n_var))\n",
    "\n",
    "labels = ['m', 'b', r'$P_b$', r'$Y_b$', r'$V_b$']\n",
    "corner.corner(flatchain, labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(flatchain), size=30)\n",
    "samples = flatchain[idx]\n",
    "\n",
    "plt.errorbar(data['x'], data['y'], \n",
    "             xerr=data['x_err'], yerr=data['y_err'], \n",
    "             marker='.', fmt='o', zorder=10)\n",
    "\n",
    "\n",
    "tmp_x = np.linspace(30, 300, 100)\n",
    "for sample in samples:\n",
    "    \n",
    "    theta, b_perp = sample[0], sample[1]\n",
    "    m = np.tan(theta)\n",
    "    b = b_perp / np.cos(theta)\n",
    "        \n",
    "    plt.plot(tmp_x, m*tmp_x+b, color='k', alpha=0.1, zorder=1)\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.xlim(30, 300)\n",
    "plt.ylim(0, 800)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss with your partner what you've found\n",
    "\n",
    "Try initializing your model at a few different points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "adb41e5e95f2492485f481f3a835e6aa",
   "lastKernelId": "cfdcea69-2403-486a-a779-0ee80c7be171"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
