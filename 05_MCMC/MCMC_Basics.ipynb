{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d73ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce8a18",
   "metadata": {},
   "source": [
    "<font size=6>**MARKOV CHAIN MONTE CARLO: Basics**</font>\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Challenges in Bayesian inference\n",
    "\n",
    "In the previous sessions we saw examples of parameter estimation for models of one parameter. We also estimated uncertainties, but this required a lot of steps. In this session we will see a general method of that can be applied to both simple and complex models, able to naturally provide uncertainties.\n",
    "\n",
    "### Challenge 1: multiple parameters $\\equiv$ multiple dimensions\n",
    "\n",
    "If our model has many parameters, $\\theta_1, \\theta_2, \\cdots, \\theta_k$, then the posterior is a $k$-dimensional function.\n",
    "\n",
    "$$ P(\\theta_1, \\theta_2, \\cdots, \\theta_k | \\mathrm{Data}) $$\n",
    "\n",
    "Finding the maximum of a $k$-D function is...way harder than in the 1-D or 2-D cases. Chances are that there are also multiple modes, which may not be found by many optimization algorithms.\n",
    "\n",
    "We could try to compute the posterior in a $k$-dimensional **grid** and select the point with the highest posterior. Though, a complex function might have narrow peaks - a high resolution grid might be necessary. The higher the resolution and the number of dimensions, the higher number of evaluations of the posterior - making the computation extremely time consuming.\n",
    "\n",
    "### Challenge 2: uncertainties\n",
    "\n",
    "Maximizing the posterior provides an estimate of the parameter but not its uncertainty. There are approximate solutions to get an estimate of the uncertainty on the parameter, but they usually make an assumption about the shape of the posterior (e.g. Gaussian peak).\n",
    "\n",
    "### Challenge 3: brute-force is non-optimal...\n",
    "\n",
    "Another complication with grids is that we may spend a lot of time at regions of the parameter space where the posterior is low. Instead, we would like to spend more time around the peaks, the regions of high posterior probability.\n",
    "\n",
    "![Mishra's Bird function](images/testfunc.png)\n",
    "\n",
    "\n",
    "## 1.2 A solution through sampling\n",
    "\n",
    "**If we could sample** from the posterior distribution $N$ points from the $k$-dimensional parameter space\n",
    "\n",
    "$$ \\Theta_1 \\equiv \\left(\\theta_{1,1}, \\theta_{2,1}, \\cdots, \\theta_{k,1}\\right) $$\n",
    "$$ \\Theta_2 \\equiv \\left(\\theta_{1,2}, \\theta_{2,2}, \\cdots, \\theta_{k,2}\\right) $$\n",
    "$$ \\vdots $$\n",
    "$$ \\Theta_N \\equiv \\left(\\theta_{1,N}, \\theta_{2,N}, \\cdots, \\theta_{k,N}\\right) $$\n",
    "\n",
    "then the **expected value** of each parameter $\\theta_i$ is simply the mean value:\n",
    "\n",
    "$$ \\bar{\\theta}_i = \\frac{1}{N} \\sum\\limits_{j=1}^{N} { \\theta_{i,j} } $$\n",
    "\n",
    "In a similar way, the **uncertainty** of each parameter is given by the formula for the sample standard deviation:\n",
    "\n",
    "$$ \\sigma_{\\theta_i} = \\sqrt { \\frac{1}{N-1}\\sum\\limits_{j=1}^{N}{ \\left(\\theta_{i,j} - \\bar{\\theta}_i  \\right)^2 } }$$\n",
    "\n",
    "and we can also find **confidence intervals or regions** (multiple probable regions), study the **covariances** (if the model parameters are correlated), etc.\n",
    "\n",
    "Almost all statistical quantities that we may need can be estimated from a large sample. The bigger the sample, the better the accuracy of our estimates.\n",
    "\n",
    "Most importantly, if someone wants to use our results (and propagate the errors), they can **directly** get the samples, **instead of relying on** means, intervals, etc. We can ship them the $N$ samples, and perform their analysis on them. E.g.:\n",
    "\n",
    "$$ \\hat{f}(\\theta_1, \\theta_2, \\cdots, \\theta_k) = \\frac{1}{N} \\sum\\limits_{i=1}^{N} {f(\\Theta_i)} $$\n",
    "\n",
    "###  Problem: how to sample from a posterior?\n",
    "\n",
    "# 2. The Markov Chain Monte Carlo technique\n",
    "\n",
    "The Marcov Chain Monte Carlo (MCMC) is a widely used technique to sample from probability distributions, and therefore likelihoods or posteriors.\n",
    "\n",
    "It is based on the idea of creating a chain of points in the parameter space, using a combination of (i) random walk and (ii) selection of points based on their relative probability.\n",
    "\n",
    "An algorithm is used to ensure that the chain will reach **equilibrium**: after a number of steps (or length of the Markov chain) the chain will contain points that follow the same distribution, the **target distribution**.\n",
    "\n",
    "> It is often characterized as one of the most influencial algorithms of the 20th century...\n",
    "\n",
    "## 2.1 Metropolis-Hastings algorithm: application on the distance example\n",
    "\n",
    "We will discuss the Metropolis-Hastings algorithm, but note that there are *many* others out there. Here are the steps:\n",
    "\n",
    "1. We start with one set of parameters $\\theta_1$. \n",
    "\n",
    "    - In our case, $\\theta_1$ is simply $\\alpha$ since we have only one parameter in our model.\n",
    "    \n",
    "    - In general, $\\theta_1$ can be a vector of 1, 5, or even a million separate parameters.\n",
    "    \n",
    "    This first value starts our Markov chain.\n",
    "      \n",
    "2. Using **some method** we obtain a new trial set of parameters $\\theta_2$. *A proposition for a new position...*\n",
    "    \n",
    "    - It is important that this set is chosen randomly, but based on the previous set.\n",
    "    - The dependence on only the previous set, is an essential property of a *Markov chain*.\n",
    "    - The randomness is where the *Monte Carlo* in MCMC comes from.\n",
    "    - The simplest method to obtain our new parameter values will be to add some random (Gaussian?) noise to our current value: $\\theta_2 = \\theta_1 + \\epsilon$. This is also called **step size**. You typically want to tune the step size, $\\epsilon$, to optimize the process for a given problem.\n",
    "    \n",
    "3. Now, we want to calculate and compare the posterior probabilities for both $\\theta_1$ and $\\theta_2$. If the new parameter is better than the current one,\n",
    "\n",
    "    $$ P(\\theta_2) > P(\\theta_1) $$\n",
    "    we always move the chain to $\\theta_2$. If not, we *might* move to $\\theta_2$ with probability equal to the ratio\n",
    "    $$ \\frac{P(\\theta_2)}{P(\\theta_1)}$$\n",
    "    In practice, we draw a random number from a uniform distribution between 0 and 1. If that random number is less than the ratio, we move the chain to $\\theta_2$. Else, we stay at $\\theta_1$ for another iteration.\n",
    "\n",
    "4. Now that we have our new value for $\\theta$, we return to step 2 and repeat for as many iterations as we want. Often this is in the thousands or more.\n",
    "\n",
    "![Metropolis-Hastings algorithm creating a Markov Chain. From [2]](images/mh.png)\n",
    "\n",
    "## 2.2 Let's implement the M-H algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce010e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metro_hastings(ln_posterior, theta_0, N_steps, step_size=0.2, args=[], verbose=True):\n",
    "    \"\"\"Metropolis-Hastings algorith for sampling a posterior distribution of one parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ln_posterior : function that returns the logarithm of the posterio\n",
    "    r\n",
    "    theta_0      : initial guess for the model parameter\n",
    "    N_steps      : the length of the Markov Chain that will be returned\n",
    "    step_size    : the standard deviation of the normally distributed step size\n",
    "    args         : additional arguments to be passed to the posterior function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array containing the Markov Chain.\n",
    "    \n",
    "    \"\"\"\n",
    "    chain = np.zeros(N_steps)                       # create the chain\n",
    "    chain[0] = theta_0                              # store the initial point...\n",
    "    if verbose:\n",
    "        print(f\"{chain[0]:.3g}\", end=\",\")   # ...and print it!\n",
    "    \n",
    "    # hold the current value of the posterior to avoid recomputing it if position is not changed\n",
    "    curr_P = ln_posterior(theta_0, *args)\n",
    "    \n",
    "    # populate the rest of the points in the chain\n",
    "    for i in range(N_steps - 1):\n",
    "        new_theta = chain[i] + np.random.normal(scale=step_size)\n",
    "        new_P = ln_posterior(new_theta, *args)\n",
    "        \n",
    "        # should we move to the new position?\n",
    "        if (new_P > curr_P) or (np.random.rand() < np.exp(new_P - curr_P)):\n",
    "            # if yes... store the new value, print it and update the 'current posterior'\n",
    "            chain[i + 1] = new_theta\n",
    "            if verbose:\n",
    "                print(f\"{new_theta:.3g}\", end=\", \")\n",
    "            curr_P = new_P\n",
    "        else:\n",
    "            # if not... store again the current position and print a '.'\n",
    "            chain[i + 1] = chain[i]\n",
    "            if verbose:\n",
    "                print(\".\", end=\" \")\n",
    "            \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e09f2",
   "metadata": {},
   "source": [
    "## 2.3 The data\n",
    "\n",
    "When massive stars ($> 8 M_\\odot$) they form neutron stars (SN) or black holes. Newly formed NSs receive \"kicks\" with velocities typically in the range 100-500 km/s. One distribution that matches that of the kick velocities is **Maxwellian** as it has been shown by studies of pulsars (e.g., Hobbs et al. 2006, MNRAS, 360, 974).\n",
    "\n",
    "Note that the Maxwellian distribution (i.e., Maxwellian-Boltzmann from Statistical Mechanics) has one parameter, $a$ that sets the magnitude of the velocities (note that $a$ is not the mean nor the mode):\n",
    "\n",
    "$$ \\Large\n",
    "            f(x; a) = \\sqrt{\\dfrac{2}{\\pi}} \\frac{x^2}{a^3} \\exp\\left({-\\frac{x^2}{2a^2}}\\right)\n",
    "$$\n",
    "\n",
    "In the following code we make an artificial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11390fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def make_velocity_sample(a1, a2, f, N):\n",
    "    \"\"\"Make a NS kick velocity distribution using two Maxwellian components and their relative contribution.\n",
    "    \n",
    "    a1 = the scale parameter of the ECSN kick velocity distribution\n",
    "    a2 = the scale parameter of the CCSN kick velocity distribution\n",
    "     f = the fraction of NSs formed by ECSN\n",
    "     N = the sample size   \n",
    "    \"\"\"\n",
    "    N_ECSN = st.binom(N, f).rvs()\n",
    "    N_CCSN = N - N_ECSN\n",
    "\n",
    "    # the two kick velocity distributions\n",
    "    distribution_ECSN = st.maxwell(scale=a1)\n",
    "    distribution_CCSN = st.maxwell(scale=a2)\n",
    "    # make samples from both distributions and join them\n",
    "    sample = np.concatenate([distribution_ECSN.rvs(size=N_ECSN), distribution_CCSN.rvs(size=N_CCSN)])\n",
    "    # shuffle the array to make sure we cannot distinguish the two samples anymore\n",
    "    np.random.shuffle(sample)\n",
    "    return sample\n",
    "    \n",
    "    \n",
    "a = 265.0\n",
    "N = 1000\n",
    "\n",
    "sample_1 = make_velocity_sample(a, a, 0.0, N=N)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sample_1, bins=\"fd\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4705b",
   "metadata": {},
   "source": [
    "## 2.4 Defining the posterior for our problem\n",
    "\n",
    "For **prior** we use a uniform one that allows only positive values.\n",
    "\n",
    "For the **log-likelihood** we use directly the `scipy.maxwell.logpdf` function. Note that `scipy` provides the `log` versions of distribution functions for better accuracy when dealing with small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183629c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior(a):\n",
    "    return -np.inf if a <= 0 else 0.0\n",
    "\n",
    "def datum_ln_likelihood(v, a):\n",
    "    return st.maxwell.logpdf(v, scale=a)\n",
    "\n",
    "def ln_likelihood(velocities, a):\n",
    "    return np.sum(datum_ln_likelihood(velocities, a))\n",
    "\n",
    "def ln_posterior(a, velocities):\n",
    "    result = ln_prior(a) + ln_likelihood(velocities, a)\n",
    "    return -np.inf if np.isnan(result) else result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bcf7c",
   "metadata": {},
   "source": [
    "## 2.5 Running the M-H algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_0 = 400.0   # our initial guess\n",
    "step_size = 10    # a step size that is \"appropriate\"\n",
    "\n",
    "chain = metro_hastings(ln_posterior, alpha_0, args=[sample_1], \n",
    "                       step_size=step_size, N_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c43ea4",
   "metadata": {},
   "source": [
    "## 2.6 The produced Markov Chain and the *burn-in* phase\n",
    "\n",
    "The first steps in the chain are not in the optimal regime. This is called the \"burn-in\" and is thrown away. In principle, there is no absolute way to determine if your chains have converged to the optimal region of parameter space. In practice, unless your problem is pathological, it is usually pretty obvious. Do remember this can be an issue and take care!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec592263",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 60   # change this value if needed\n",
    "\n",
    "plt.figure()\n",
    "plt.axvspan(0, n_burnin, color=\"r\", alpha=0.3)\n",
    "plt.plot(chain)\n",
    "_, _, y_min, y_max = plt.axis()\n",
    "plt.text(n_burnin + 3, (y_min + y_max) / 2.0, \"Burn-in\")\n",
    "chain_converged = chain[n_burnin:]\n",
    "plt.xlim(0,len(chain))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d8179",
   "metadata": {},
   "source": [
    "## 2.7 The acceptance fraction\n",
    "\n",
    "The acceptance fraction depends on the target distribution. It has been shown that for one-dimensional Gaussian distributions, the optimal acceptance fraction is 50%, while for high dimensional Gaussian distributions is 23.4%.\n",
    "\n",
    "Very small or large acceptance fraction indicates that the step size is not appropriate and the algorithm struggles to reach to the target distribution. By tuning the step size, $\\epsilon$, we can optimize the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_accepted = np.sum(chain_converged[:-1] != chain_converged[1:])\n",
    "print(\"Acceptance fraction: {:.3f}\".format(N_accepted/len(chain_converged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463ed1e",
   "metadata": {},
   "source": [
    "Our acceptance fraction might be low or high. We can adapt this by increasing/decreasing $\\epsilon$. This might affect the length of the burn-in phase.\n",
    "\n",
    "## 2.8 Reporting results in the $m \\pm s$ form or with confidence intervals\n",
    "\n",
    "Now, we can plot the posterior distribution of $\\alpha$ to obtain our final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = np.mean(chain_converged), np.std(chain_converged)\n",
    "lo68, median, hi68 = np.percentile(chain_converged, [16, 50, 84])\n",
    "\n",
    "print(\"  mean +/- std |  {:.3f} +/- {:.3f}\".format(mean, std))\n",
    "print(\"median +hi -lo |  {:.3f} +{:.3f} -{:.3f}\".format(\n",
    "    median, hi68 - median, median - lo68))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(chain_converged, bins=10, density=True)\n",
    "_, _, _, y_max = plt.axis()\n",
    "plt.ylim(ymax=y_max*1.4)\n",
    "\n",
    "plt.axvline(a, color=\"k\", label=\"True value\")\n",
    "plt.axvline(median, color=\"r\", linewidth=2, label=\"Median\")\n",
    "plt.axvspan(lo68, hi68, color=\"r\", alpha=0.3, label=\"68% CI\")\n",
    "plt.title(r\"${:.2f}^{{+{:.2f}}}_{{-{:.2f}}}$\".format(median, hi68 - median, median - lo68))\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Posterior density\")\n",
    "plt.legend(loc=\"upper center\", ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf799eb",
   "metadata": {},
   "source": [
    "# Exercise 1: effect of initial position\n",
    "\n",
    "Run the MCMC for three different initial positions.\n",
    "\n",
    "<font size=3><u>**Does the initial position affect the result?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "It doesn't! The chains converge to the same solution but it may take a while if we start away from it!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10.0                   # our step size\n",
    "\n",
    "alpha_0s = [..., ..., ...]         # pick the initial guesses\n",
    "step_sizes = [step_size] * 3       # the step sizes\n",
    "\n",
    "N_steps = 1000\n",
    "n_burnin = 500   # pick a value that is \"safe\" for all chains\n",
    "\n",
    "# make all chains\n",
    "chains = [metro_hastings(ln_posterior, alpha_0, args=[sample_1], step_size=step_size, N_steps=N_steps, verbose=False)\n",
    "          for alpha_0, step_size in zip(alpha_0s, step_sizes)]\n",
    "converged_chains = [chain[n_burnin:] for chain in chains]\n",
    "\n",
    "# plot the chains and samples\n",
    "plt.figure()\n",
    "for alpha_0, step_size, chain, converged_chain in zip(alpha_0s, step_sizes, chains, converged_chains):\n",
    "    acceptance_fraction = np.mean(converged_chain[:-1] != converged_chain[1:])\n",
    "    plt.plot(chain, alpha=0.7, label=f\"($a_0$={alpha_0:.6g}, step={step_size:.6g}): Acc. = {acceptance_fraction:.3f}\")\n",
    "plt.axvline(n_burnin, color=\"r\", ls=\":\", alpha=0.8, label=\"Burn-in\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for alpha_0, step_size, converged_chain in zip(alpha_0s, step_sizes, converged_chains):\n",
    "    plt.hist(converged_chain, bins=\"fd\", histtype=\"step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f0fa5",
   "metadata": {},
   "source": [
    "# 2.9. Exercise 2: effect of step size\n",
    "\n",
    "Now try with the same initial guess (away for the solution, e.g., 400), but with different step sizes.\n",
    "\n",
    "<font size=3><u>**How does the step size affect the convergence and result?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "    The larger the step, the quicker we explore the parameter space and find the solution. However, this makes it harder to go to newly proposed positions and we are not efficiently sampling the posterior (small acceptance fraction).\n",
    "    \n",
    "    With very small step sizes the convergence is so slow that the burn-in phase lasts for long, but we can't \"lose\" the solution. However, the acceptance fraction is too large: we are not efficiently exploring the space around the solution - it seems like a random walk around it. Thus, we may need very long chains to converge to the correct posterior distribution. \n",
    "    \n",
    "    Intermediate step sizes are optimal: quick convergence, good sampling of the posterior, acceptance fraction in the desired range.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8421b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_position = 400.0           # pick an initial position away from the solution\n",
    "alpha_0s = [initial_position] * 3  \n",
    "step_sizes = [..., ..., ...]    # pick the step sizes\n",
    "\n",
    "N_steps = 1000\n",
    "n_burnin = 500   # pick a value that is \"safe\" for all chains\n",
    "\n",
    "# make all chains\n",
    "chains = [metro_hastings(ln_posterior, alpha_0, args=[sample_1], step_size=step_size, N_steps=N_steps, verbose=False)\n",
    "          for alpha_0, step_size in zip(alpha_0s, step_sizes)]\n",
    "converged_chains = [chain[n_burnin:] for chain in chains]\n",
    "\n",
    "# plot the chains and samples\n",
    "plt.figure()\n",
    "for alpha_0, step_size, chain, converged_chain in zip(alpha_0s, step_sizes, chains, converged_chains):\n",
    "    acceptance_fraction = np.mean(converged_chain[:-1] != converged_chain[1:])\n",
    "    plt.plot(chain, alpha=0.7, label=f\"($a_0$={alpha_0:.6g}, step={step_size:.6g}): Acc. = {acceptance_fraction:.3f}\")\n",
    "plt.axvline(n_burnin, color=\"r\", ls=\":\", alpha=0.8, label=\"Burn-in\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for alpha_0, step_size, converged_chain in zip(alpha_0s, step_sizes, converged_chains):\n",
    "    plt.hist(converged_chain, bins=\"fd\", histtype=\"step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de300e",
   "metadata": {},
   "source": [
    "# 3. The MCMC Hammer... `emcee`:\n",
    "## ...a *seriously kick-ass MCMC*...\n",
    "\n",
    "![Mishra's Bird function](images/mchammer.gif)\n",
    "\n",
    "> official statement in an older version of the documentation: https://emcee.readthedocs.io/en/v2.2.1/\n",
    "\n",
    "There are various MCMC algorithms (or samplers) out there. One very good algorithm that does not require much tuning is Goodman & Weareâ€™s Affine Invariant Markov chain Monte Carlo Ensemble Sampler [3], with a Python implementation: the `emcee` module.\n",
    "\n",
    "## Multiple chains/walkers (ensemble)\n",
    "\n",
    "A walker might fail due to an \"unfrotunate\" initial position. By having many of them we might explore the parameter space more efficiently. Typical choices: 100 or 1000. `emcee` does this for us.\n",
    "\n",
    "## Implementation of the *stretch* move\n",
    "\n",
    "The different \"chains\" might \"talk\" to each other to help convergence! We will see that in an example later.\n",
    "\n",
    "## Parallelization\n",
    "\n",
    "`emcee` supports parallel computing, and therefore we can exploit the full potential of our machines or computer clusters. This is useful for large datasets, or complex likelihood functions (e.g., Kouroumpatzakis et al. 2020 [4]). \n",
    "\n",
    "# 4. Using `emcee`\n",
    "\n",
    "## 4.1 Setting the `emcee` sampler\n",
    "\n",
    "We need to set the number of \"walkers\", the number of steps that each walker will take, as well as the initial positions (typically randomly selected, but you can start from the same point as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a224e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "n_walkers = 50\n",
    "n_dim = 1\n",
    "n_steps = 500\n",
    "\n",
    "a_min = 10.0\n",
    "a_max = 800.0\n",
    "\n",
    "# set the parameter's initial positions\n",
    "a_initial = np.random.uniform(a_min, a_max, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "p0 = np.array([a_initial]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, \n",
    "                                log_prob_fn=ln_posterior, args=[sample_1],\n",
    "                                moves=emcee.moves.GaussianMove(30.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad7442",
   "metadata": {},
   "source": [
    "## 4.4 Sampling from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sampler.run_mcmc(p0, nsteps=n_steps, progress=True)\n",
    "print(sampler.chain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94141765",
   "metadata": {},
   "source": [
    "Let's inspect the output. There are three dimenions:\n",
    "\n",
    "$$\\large \n",
    "    \\left(\\text{# of walkers}\\right) \\times \n",
    "    \\left(\\text{# of steps}\\right) \\times\n",
    "    \\left(\\text{# of dimensions}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605ce9c",
   "metadata": {},
   "source": [
    "## 4.5 Inspecting the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0498a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(\"a\" if param_i == 0 else \"b\")\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa69f0",
   "metadata": {},
   "source": [
    "## 4.6 Removing the burn-in phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197173a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 300\n",
    "\n",
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(\"a\" if param_i == 0 else \"b\")\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f84bc",
   "metadata": {},
   "source": [
    "## 4.7 Corner plot of all the samples from the converged chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fa5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape      :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flat chain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a$\", r\"$b$\"],\n",
    "                    show_titles=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c42966",
   "metadata": {},
   "source": [
    "# 5. The stretch move of `emcee`\n",
    "\n",
    "When we have multiple walkers, we can exploit their distribution in the parameter space to explore it at an optimal scale. Essentially, a move from a walker at position $X_j$ can be at the direction of another walker $X_k$, with a step equal to the distance of the walkers times a factor that is randomly sampled in the range $(1/Z, Z)$ where $Z$ is the *stretch factor* which is by default $2.0$ in `emcee`. The theroy behind this move, including its compatibility with the Metropolis-Hastings algorithm can be found in  Goodman & Weare 2010 (https://msp.org/camcos/2010/5-1/p04.xhtml).\n",
    "\n",
    "![title](images/stretch_move.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf48d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "n_walkers = 50\n",
    "n_dim = 1\n",
    "n_steps = 500\n",
    "\n",
    "a_min = 0.0\n",
    "a_max = 800.0\n",
    "\n",
    "# set the parameter's initial positions\n",
    "a_initial = np.random.uniform(a_min, a_max, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "p0 = np.array([a_initial]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, \n",
    "                                log_prob_fn=ln_posterior, args=[sample_1],\n",
    "#                                 moves=emcee.moves.GaussianMove(30.0),\n",
    "                                )\n",
    "result = sampler.run_mcmc(p0, nsteps=n_steps, progress=True)\n",
    "\n",
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(\"a\" if param_i == 0 else \"b\")\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 50\n",
    "\n",
    "labels = [\"a\"]\n",
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(labels[param_i])\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape     :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flatchain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a$\"],\n",
    "                    show_titles=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6974f1",
   "metadata": {},
   "source": [
    "# Exercise 3: starting from the same position\n",
    "\n",
    "Run the MCMC analysis with a very narrow range of initial positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfe90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "n_walkers = 50\n",
    "n_dim = 1\n",
    "n_steps = 500\n",
    "\n",
    "a_min = 390.0\n",
    "a_max = 410.0\n",
    "\n",
    "# set the parameter's initial positions\n",
    "a_initial = np.random.uniform(a_min, a_max, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "p0 = np.array([a_initial]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, \n",
    "                                log_prob_fn=ln_posterior, args=[sample_1],\n",
    "#                                 moves=emcee.moves.GaussianMove(30.0),\n",
    "                                )\n",
    "result = sampler.run_mcmc(p0, nsteps=n_steps, progress=True)\n",
    "\n",
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(\"a\" if param_i == 0 else \"b\")\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89069c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 80\n",
    "\n",
    "labels = [\"a\"]\n",
    "plt.figure()\n",
    "for param_i in range(n_dim):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(labels[param_i])\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape     :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flatchain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a$\"],\n",
    "                    show_titles=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d637b",
   "metadata": {},
   "source": [
    "<font size=3><u>**Compare all the chains and the time needed by the Gaussian and Stretch steps. What are the advantages of using the *stretch* step?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "    \n",
    "    1. Using the stretch move takes a little bit longer, but the convergence is so fast that we can save time by reducing the number of steps.\n",
    "    \n",
    "    2. Having said that, with the stretch move we can reach to same number of samples in the same computation time with more walkers, ensuring better exploration of the parameter space.\n",
    "    \n",
    "    3. Even if we start from the same, effectively, position, the stretch move is able to move and expand the search.\n",
    "    \n",
    "    4. More importantly, with the stretch move we do not need to find the optimal step size!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa91f0",
   "metadata": {},
   "source": [
    "# 5.1. For advanced users...\n",
    "\n",
    "In the case of non-linear models, and with many dimensions, MCMC is harder to converge. The number of walkers, and step might need to be adapted.\n",
    "\n",
    "### Increase the **number of walkers**... \n",
    "\n",
    "...to explore the parameter space faster in the beginning!\n",
    "\n",
    "### Increase the **number of steps**...\n",
    "\n",
    "...to get a better chance for convergence. Fortunately, with `emcee` it is possible to run more steps at will! For example, for the fitting in Kouroumpatzakis et al. 2020 (https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.5967K/abstract), and the user was asked if they want to continue or stop after a given number of steps (or convergence being detected automatically).\n",
    "\n",
    "# 5.2. Let's try a more difficult model...\n",
    "\n",
    "The kick velocities NS receive are a reason to change the orbits in binary systems, either disrupting them completely, or inducing an eccentricity. Studies have shown that NS in many binaries have not received significant kick. This gave rise to the *Electron-Capture Supernova* (ECSN) mechanism which makes SN explosions symmetric and therefore, low in kick velocities (e.g, Dessart et al. 2006, ApJ, 644, 1063).\n",
    "\n",
    "The existence of a population of NSs that have experienced an ECSN would have the effect of an additional Maxwellian component with lower velocities. Let's see that...\n",
    "\n",
    "Therefore, we need **three parameters** to describe the overall distribution:\n",
    "\n",
    "* The scale of the less energetic ECSN kick velocity distribution\n",
    "* The scale of the normal, core-collapse supernova (CCSN) kick velocity distribution\n",
    "* The fraction of NSs that experienced ECSN (adjusting the relative contribution of the two components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_true = 20.0\n",
    "a2_true = 265.0\n",
    "f_true = 0.4\n",
    "N = 500\n",
    "sample_2 = make_velocity_sample(a1_true, a2_true, f_true, N)\n",
    "\n",
    "v_step = 20.0\n",
    "v_bins = np.arange(0.0, max(sample_2)+v_step, v_step)\n",
    "v_plot = np.linspace(0.0, max(sample_2), 1000)\n",
    "normalization = 1 # N * v_step\n",
    "plt.figure()\n",
    "plt.hist(sample_2, bins=v_bins, color=\"0.9\", ec=\"0.5\", label=\"Data\", density=True)\n",
    "plt.plot(v_plot, normalization * f_true * st.maxwell(scale=a1_true).pdf(v_plot), \"g-\", label=\"ECSN distribution\")\n",
    "plt.plot(v_plot, normalization * (1.0 - f_true) * st.maxwell(scale=a2_true).pdf(v_plot), \"r-\", label=\"CCSN distribution\")\n",
    "plt.xlabel(\"Velocity (km/s)\")\n",
    "plt.ylabel(\"Number of NSs\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior_a1(a1):\n",
    "    return -np.inf if a1 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_a2(a2):\n",
    "    return -np.inf if a2 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_f(f):\n",
    "    return -np.inf if f < 0 or f > 1 else 0.0\n",
    "\n",
    "def ln_prior(a1, a2, f):\n",
    "    return ln_prior_a1(a1) + ln_prior_a2(a2) + ln_prior_f(f)\n",
    "\n",
    "def datum_ln_likelihood(v, a1, a2, f):\n",
    "    return np.log(f * st.maxwell.pdf(v, scale=a1) + (1.0 - f) * st.maxwell.pdf(v, scale=a2))\n",
    "\n",
    "def ln_likelihood(velocities, a1, a2, f):\n",
    "    return np.sum(datum_ln_likelihood(velocities, a1, a2, f))\n",
    "\n",
    "def ln_posterior(parameters, velocities):\n",
    "    a1, a2, f = parameters\n",
    "    result = ln_prior(a1, a2, f) + ln_likelihood(velocities, a1, a2, f)\n",
    "    return -np.inf if np.isnan(result) else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walkers = 100\n",
    "n_dim = 3\n",
    "n_steps = 400\n",
    "\n",
    "# set the parameter\n",
    "a1_positions = np.random.uniform(0.0, 400.0, size=n_walkers)\n",
    "a2_positions = np.random.uniform(0.0, 400.0, size=n_walkers)\n",
    "f_positions = np.random.uniform(0.0, 1.0, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "positions = np.array([a1_positions, a2_positions, f_positions]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, log_prob_fn=ln_posterior, args=[sample_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98197b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = sampler.run_mcmc(positions, nsteps=n_steps, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "labels = [\"a1\", \"a2\", \"f\"]\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 100\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape     :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flatchain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a1_true, a2_true, f_true],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a_1$\", r\"$a_2$\", r\"$f$\"],\n",
    "                    show_titles=True, \n",
    "#                     range=[0.95]*3,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03519f",
   "metadata": {},
   "source": [
    "<font size=3><u>**What is the problem with this solution? Can we do something about it?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "    There is a degeneracy in the parameters: $a_1$ and $a_2$ can be exchanged, and the fraction $f$ can be for either of the two populations! The peaks we detect are not representative of the final distributions!\n",
    "    \n",
    "    This leads to an unwanted solution, and the walkers spending too much time moving back and forth.\n",
    "    \n",
    "    We can break the symmetry by requirying that $a_1 < a_2$. This will happen in the prior!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72a642",
   "metadata": {},
   "source": [
    "# Exercise 4: enforce a single solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d80a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior_a1(a1):\n",
    "    return -np.inf if a1 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_a2(a2):\n",
    "    return -np.inf if a2 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_f(f):\n",
    "    return -np.inf if f < 0 or f > 1 else 0.0\n",
    "\n",
    "def ln_prior(a1, a2, f):\n",
    "    return ln_prior_a1(a1) + ln_prior_a2(a2) + ln_prior_f(f)\n",
    "\n",
    "def datum_ln_likelihood(v, a1, a2, f):\n",
    "    return np.log(f * st.maxwell.pdf(v, scale=a1) + (1.0 - f) * st.maxwell.pdf(v, scale=a2))\n",
    "\n",
    "def ln_likelihood(velocities, a1, a2, f):\n",
    "    return np.sum(datum_ln_likelihood(velocities, a1, a2, f))\n",
    "\n",
    "def ln_posterior(parameters, velocities):\n",
    "    a1, a2, f = parameters\n",
    "    result = ln_prior(a1, a2, f) + ln_likelihood(velocities, a1, a2, f)\n",
    "    return -np.inf if np.isnan(result) else result\n",
    "\n",
    "n_walkers = 100\n",
    "n_dim = 3\n",
    "n_steps = 400\n",
    "\n",
    "# set the parameter\n",
    "a1_positions = np.random.uniform(100.0, 400.0, size=n_walkers)\n",
    "a2_positions = np.random.uniform(100.0, 400.0, size=n_walkers)\n",
    "f_positions = np.random.uniform(0.0, 1.0, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "positions = np.array([a1_positions, a2_positions, f_positions]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, log_prob_fn=ln_posterior, args=[sample_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sampler.run_mcmc(positions, nsteps=n_steps, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56445ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "labels = [\"a1\", \"a2\", \"f\"]\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9981df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 90\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879683df",
   "metadata": {},
   "source": [
    "* What do you notice about the burn-in phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5eab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape     :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flatchain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a1_true, a2_true, f_true],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a_1$\", r\"$a_2$\", r\"$f$\"],\n",
    "                    show_titles=True, \n",
    "#                     range=[0.95]*3,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52999f9c",
   "metadata": {},
   "source": [
    "# Exercise 5: try again with $a_1 = a_2$\n",
    "* You can somehow see a slight preference of using only one component (the peaks at 0 and 1 in the f distribution).\n",
    "* Also, when the f is 0 or 1, you see that a2 and a1 are better constrained respectively (you don't care of the parameter of a component that doesn't participate at all in the likelihood).\n",
    "* when f is not extreme, a1 and a2 take more or less the same value... to create an up-scaled version of the original distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d144c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_true = 265.0\n",
    "a2_true = 265.0\n",
    "f_true = 0.4\n",
    "N = 500\n",
    "sample_3 = make_velocity_sample(a1_true, a2_true, f_true, N)\n",
    "\n",
    "v_step = 20.0\n",
    "v_bins = np.arange(0.0, max(sample_3)+v_step, v_step)\n",
    "v_plot = np.linspace(0.0, max(sample_3), 1000)\n",
    "normalization = 1 # N * v_step\n",
    "plt.figure()\n",
    "plt.hist(sample_3, bins=v_bins, color=\"0.9\", ec=\"0.5\", label=\"Data\", density=True)\n",
    "plt.plot(v_plot, normalization * f_true * st.maxwell(scale=a1_true).pdf(v_plot), \"g-\", label=\"ECSN distribution\")\n",
    "plt.plot(v_plot, normalization * (1.0 - f_true) * st.maxwell(scale=a2_true).pdf(v_plot), \"r-\", label=\"CCSN distribution\")\n",
    "plt.xlabel(\"Velocity (km/s)\")\n",
    "plt.ylabel(\"Number of NSs\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811df9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior_a1(a1):\n",
    "    return -np.inf if a1 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_a2(a2):\n",
    "    return -np.inf if a2 <= 0 else 0.0\n",
    "\n",
    "def ln_prior_f(f):\n",
    "    return -np.inf if f < 0 or f > 1 else 0.0\n",
    "\n",
    "def ln_prior(a1, a2, f):\n",
    "    if a1 > a2:\n",
    "        return -np.inf\n",
    "    return ln_prior_a1(a1) + ln_prior_a2(a2) + ln_prior_f(f)\n",
    "\n",
    "def datum_ln_likelihood(v, a1, a2, f):\n",
    "    return np.log(f * st.maxwell.pdf(v, scale=a1) + (1.0 - f) * st.maxwell.pdf(v, scale=a2))\n",
    "\n",
    "def ln_likelihood(velocities, a1, a2, f):\n",
    "    return np.sum(datum_ln_likelihood(velocities, a1, a2, f))\n",
    "\n",
    "def ln_posterior(parameters, velocities):\n",
    "    a1, a2, f = parameters\n",
    "    result = ln_prior(a1, a2, f) + ln_likelihood(velocities, a1, a2, f)\n",
    "    return -np.inf if np.isnan(result) else result\n",
    "\n",
    "\n",
    "n_walkers = 100\n",
    "n_dim = 3\n",
    "n_steps = 400\n",
    "\n",
    "# set the parameter\n",
    "a1_positions = np.random.uniform(100.0, 400.0, size=n_walkers)\n",
    "a2_positions = np.random.uniform(100.0, 400.0, size=n_walkers)\n",
    "f_positions = np.random.uniform(0.0, 1.0, size=n_walkers)\n",
    "\n",
    "# take the initial values for each parameter and put them in columns of a 2D array\n",
    "positions = np.array([a1_positions, a2_positions, f_positions]).T\n",
    "\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=n_dim, log_prob_fn=ln_posterior, args=[sample_3])\n",
    "result = sampler.run_mcmc(positions, nsteps=n_steps, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30429dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "labels = [\"a1\", \"a2\", \"f\"]\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, :, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 50\n",
    "\n",
    "for param_i, label_i in zip(range(n_dim), labels):\n",
    "    plt.subplot(n_dim, 1, 1+param_i)\n",
    "    plt.ylabel(label_i)\n",
    "    for j in range(n_walkers):\n",
    "        chain = sampler.chain[j, n_burnin:, param_i]\n",
    "        plt.plot(chain, \"k-\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_chain = sampler.chain[:, n_burnin:, :]\n",
    "print(\"Converged chain shape     :\", converged_chain.shape)\n",
    "\n",
    "flat_converged_chain = converged_chain.reshape(converged_chain.shape[0] * converged_chain.shape[1], -1)\n",
    "print(\"Converged flatchain shape :\", flat_converged_chain.shape)\n",
    "\n",
    "fig = corner.corner(flat_converged_chain,\n",
    "                    quantiles=[0.16, 0.5, 0.84],\n",
    "                    truths=[a1_true, a2_true, f_true],\n",
    "                    truth_color=\"r\",\n",
    "                    labels=[r\"$a_1$\", r\"$a_2$\", r\"$f$\"],\n",
    "                    show_titles=True, \n",
    "                    range=[0.95]*3,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26efef",
   "metadata": {},
   "source": [
    "<font size=3><u>**Explain the structures in the corner plot**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "    When $a_1 = a_2$ the fraction parameter is unconstrained because it lacks meaning! However, then the fraction is 0 or 1, then the contribution of one component is minimal, and therefore, its scale is unconstrained. On the other hand, for intermediate values, the two scales converge to the correct value. The \"angle\" in the $a_1$-$a_2$ plot is created by the inequality enforced by the prior.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5d177",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]. Wikipedia contributors. (2019, May 9). Test functions for optimization. In Wikipedia, The Free Encyclopedia. Retrieved 20:20, June 12, 2019, from https://en.wikipedia.org/w/index.php?title=Test_functions_for_optimization&oldid=896257708\n",
    "\n",
    "[2]. Lee, Jaewook & Sung, Woosuk & Choi, Joo-Ho. (2015). Metamodel for Efficient Estimation of Capacity-Fade Uncertainty in Li-Ion Batteries for Electric Vehicles. Energies. 8. 5538-5554. 10.3390/en8065538.\n",
    "\n",
    "[3]. Foreman-Mackey, D., Hogg, D. W., Lang, D., & Goodman, J. 2013, PASP, 125, 306. https://ui.adsabs.harvard.edu/abs/2013PASP..125..306F/abstract\n",
    "\n",
    "[4]. Kouroumpatzakis et al. (2020), MNRAS, 494, 5967. https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.5967K/abstract\n",
    "\n",
    "[5]. Hobbs et al. (2005), MNRAS, 360, 974. https://ui.adsabs.harvard.edu/abs/2005MNRAS.360..974H/abstract\n",
    "\n",
    "[6]. Dessart et al. (2006), ApJ, 644, 1063. https://ui.adsabs.harvard.edu/abs/2006ApJ...644.1063D/abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
