{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**Fitting Methods and Interpolation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from astropy.table import Table\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn import neighbors\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS)\n",
    "\n",
    "You likely have seem least squares in the past - it is possibly the simplest thing one can do: fit a straight line to a bunch of data. We parameterize the line with a slope $m$ and intercept $b$, and you may have come across the OLS equation for the optimum values:\n",
    "\n",
    "$$ m = \\frac{\\sum_{i=1}^N (x_i - \\bar{x}) (y_i - \\bar{y}) }{\\sum_{i=1}^N (x_i-\\bar{x})^2} $$\n",
    "\n",
    "$$ b = \\bar{y} - m \\bar{x} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_OLS(x, y):\n",
    "    \"\"\"Calculate the ordinary least squares fit to data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy array\n",
    "        The list of independent variables\n",
    "    y : numpy array\n",
    "        The list of dependent variables\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    m : float\n",
    "        Slope of the best-fit line\n",
    "    b : float\n",
    "        Intercept of the best_fit line\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x_avg = np.mean(x)\n",
    "    y_avg = np.mean(y)\n",
    "    \n",
    "    m = np.sum((x-x_avg) * (y-y_avg)) / np.sum((x-x_avg)**2)\n",
    "    b = y_avg - m * x_avg\n",
    "    \n",
    "    return m, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we use astropy tables to load the data, which we obtained directly from CDS\n",
    "\n",
    "readme = './data/ReadMe'\n",
    "RR_lyrae_table2 = Table.read('./data/table2.dat', readme=readme, format='cds')\n",
    "\n",
    "# We will select just the 'FU' type RR Lyraes\n",
    "RR_lyrae = RR_lyrae_table2[np.where(RR_lyrae_table2['Mode'] == 'RRab')]\n",
    "\n",
    "# Now remove sources V20 and V21 due to blending\n",
    "RR_lyrae = RR_lyrae[np.where(RR_lyrae['ID'] != 'V20')[0]]\n",
    "RR_lyrae = RR_lyrae[np.where(RR_lyrae['ID'] != 'V21')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Axis limits\n",
    "for a in ax:\n",
    "    a.set_ylim(11.3, 10.35)\n",
    "    a.set_xlim(-0.15, 0.23)\n",
    "    a.set_xticks([-0.1, 0.0, 0.1, 0.2])\n",
    "\n",
    "# Plot 3.6 micron data\n",
    "ax[0].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], color='k', marker='.', label='Observations')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "m, b = calc_OLS(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'])\n",
    "ax[0].plot(tmp_x, m*tmp_x+b, label='OLS Line')\n",
    "\n",
    "\n",
    "# Plot 4.5 micron data\n",
    "ax[1].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], color='k', marker='.')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "m, b = calc_OLS(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'])\n",
    "ax[1].plot(tmp_x, m*tmp_x+b)\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot labels\n",
    "for a in ax:\n",
    "    a.set_xlabel('Log Period (days) + 0.26')\n",
    "    a.grid()\n",
    "    \n",
    "ax[0].set_ylabel('IRAC [3.6]')\n",
    "ax[1].set_ylabel('IRAC [4.5]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems we've done a good job, but what have we left out?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "This approach does not take into account error bars on the data!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve things, let's return to the linear algebra approach we saw earlier this morning. We first define our matrices so our data satisfy the equation $Y = A X$. \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "b \\\\\n",
    "m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ Y = \\left[ \n",
    "             \\begin{matrix} \n",
    "                y_1 \\\\\n",
    "                y_2 \\\\\n",
    "                \\vdots \\\\\n",
    "                y_N\n",
    "             \\end{matrix}\n",
    "    \\right] \n",
    "$$\n",
    "\n",
    "$$ X = \n",
    "    \\begin{bmatrix} \n",
    "        1 & x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        1 & x_N\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "If we had no error bars on the data, we could directly solve for $A$:\n",
    "\n",
    "$$ Y = X A $$\n",
    "\n",
    "$$ X^T Y = X^T X A $$\n",
    "\n",
    "$$ A = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "If you go through the calculation, you will find that this result for $A$ is identical to the OLS result above. Just to prove it, we'll perform the calculation and compare to our previous equation for the OLS fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_OLS_linear_algebra(x, y):\n",
    "    \"\"\"Calculate the ordinary least squares fit to data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy array\n",
    "        The list of independent variables\n",
    "    y : numpy array\n",
    "        The list of dependent variables\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    m : float\n",
    "        Slope of the best-fit line\n",
    "    b : float\n",
    "        Intercept of the best_fit line\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = np.matrix(y).T                              # The y-values\n",
    "    N = Y.size                                      # The number of observations\n",
    "\n",
    "    X = np.ones((N, 2))                             # Create a 2xN matrix of 1's\n",
    "    X[:,1] = x                                      # Set the second row to the x-values\n",
    "    X = np.matrix(X)                                # Convert numpy array to a numpy matrix\n",
    "    \n",
    "    A = inv(X.T * X) * (X.T * Y)                    # Our solution vector\n",
    "\n",
    "    return np.array(A)                              # Convert back from numpy matrix to numpy array\n",
    "\n",
    "\n",
    "# We need to remove stars that don't have data in the [3.6] micron band\n",
    "idx = np.where(np.array(RR_lyrae['[3.6]']) != 0)[0]\n",
    "A = calc_OLS_linear_algebra(RR_lyrae['logP'][idx]+0.26, RR_lyrae['[3.6]'][idx])\n",
    "\n",
    "m, b = calc_OLS(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'])\n",
    "\n",
    "print(\"OLS linear algebra approach:\", A)\n",
    "print(\"Traditional OLS equations:\", b, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, how do we add error bars? \n",
    "\n",
    "We include the covariance matrix C:\n",
    "\n",
    "$$ C = \n",
    "    \\begin{bmatrix} \n",
    "        \\sigma^2_{y_1} & 0 & \\dots & 0 \\\\\n",
    "        0 & \\sigma^2_{y_2} & \\dots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & \\dots & \\sigma^2_{y_N}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There will be more on this later this afternoon. For now, we will assume independent observations, so the covariance matrix only has diagonal elements. Now, we can rederive the solution for the best fit $A$. First we take our linear equation and left-multiply it by the inverse of the covariance matrix\n",
    "\n",
    "\n",
    "$$\n",
    "C^{-1} Y = C^{-1} X A\n",
    "$$\n",
    "\n",
    "Next, we left-multiply by X transpose\n",
    "\n",
    "$$\n",
    "X^T C^{-1} Y = X^T C^{-1} X A\n",
    "$$\n",
    "\n",
    "As a last step, we want to solve for $A$, so we left-multiply each side by $\\left[ A^T C^{-1} A \\right]^{-1}$ obtaining the simple result:\n",
    "\n",
    "$$ A =\n",
    "\\left[ X^T C^{-1} X \\right]^{-1} \\left[ X^T C^{-1} Y \\right]\n",
    "$$\n",
    "\n",
    "We can use the function from earlier this morning to calculate this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_fit(data_x, data_y, data_y_err):\n",
    "    \"\"\" Calculate the best fit line values \"\"\"\n",
    "    \n",
    "    Y = np.matrix(data_y).T                         # The y-values\n",
    "    N = Y.size                                      # The number of observations\n",
    "\n",
    "    X = np.ones((N, 2))                             # Create a 2xN matrix of 1's\n",
    "    X[:,1] = data_x                                 # Set the second row to the x-values\n",
    "    X = np.matrix(X)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    C = np.identity(N) * data_y_err                 # Create the covariance matrix\n",
    "    C = np.matrix(C)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    A = inv(X.T * inv(C) * X) * (X.T * inv(C) * Y)  # Our solution vector\n",
    "    \n",
    "    return np.array(A)                              # Convert back from numpy matrix to numpy array\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Axis limits\n",
    "for a in ax:\n",
    "    a.set_ylim(11.3, 10.35)\n",
    "    a.set_xlim(-0.15, 0.23)\n",
    "    a.set_xticks([-0.1, 0.0, 0.1, 0.2])\n",
    "\n",
    "# Plot 3.6 micron data\n",
    "ax[0].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], color='k', marker='.', label='Observations')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "m, b = calc_OLS(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'])\n",
    "ax[0].plot(tmp_x, m*tmp_x+b, label='OLS Line')\n",
    "b, m = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], RR_lyrae['e_[3.6]'])\n",
    "ax[0].plot(tmp_x, m*tmp_x+b, label='Best-fit Line')\n",
    "\n",
    "\n",
    "# Plot 4.5 micron data\n",
    "ax[1].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], color='k', marker='.')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "m, b = calc_OLS(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'])\n",
    "ax[1].plot(tmp_x, m*tmp_x+b)\n",
    "b, m = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], RR_lyrae['e_[4.5]'])\n",
    "ax[1].plot(tmp_x, m*tmp_x+b)\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot labels\n",
    "for a in ax:\n",
    "    a.set_xlabel('Log Period (days) + 0.26')\n",
    "    a.grid()\n",
    "    \n",
    "ax[0].set_ylabel('IRAC [3.6]')\n",
    "ax[1].set_ylabel('IRAC [4.5]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a slight difference between the OLS solution and best-fit line including the covariance. Discuss with your partner why that might be the case\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "The error bars on the data points are not all the same. There are some slight differences, so different points will be weighted a different amount. \n",
    "\n",
    "Some fun vocabulary:\n",
    "Homoscedastic: Data in which every data point has the same size uncertainty\n",
    "Heteroscedastic: Data in which every data point has its own uncertainty\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding to higher order solutions\n",
    "\n",
    "The linear algebra approach allows us to extend our fitting solution to an arbitrarily large order polynomial:\n",
    "\n",
    "$$ y = a_0 + a_1x + a_2x^2 + ... + a_m  x^m $$\n",
    "\n",
    "This is as easy as expanding the length of $A$:\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a_0 \\\\\n",
    "a_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and expanding our $X$ matrix:\n",
    "\n",
    "$$ X = \n",
    "    \\begin{bmatrix} \n",
    "        1 & x_1 & \\dots & x_1^m \\\\\n",
    "        1 & x_2 & \\dots & x_2^m \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_N & \\dots & x_N^m\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The solution to the linear algebra equation is the same, we just need to construct our function so it can flexibly take in the polynomial degree we are trying to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_fit(data_x, data_y, data_y_err, degree=1):\n",
    "    \"\"\" Calculate the best fit line values \"\"\"\n",
    "    \n",
    "    Y = np.matrix(data_y).T                         # The y-values\n",
    "    N = Y.size                                      # The number of observations\n",
    "\n",
    "    X = np.ones((N, degree+1))                      # Create a mxN matrix of 1's\n",
    "    \n",
    "    for i in range(degree+1):\n",
    "        X[:,i] *= data_x**i                         # Set each row to the degree polynomial of x-values\n",
    "    X = np.matrix(X)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    C = np.identity(N) * data_y_err                 # Create the covariance matrix\n",
    "    C = np.matrix(C)                                # Convert numpy array to a numpy matrix\n",
    "    \n",
    "    A = inv(X.T * inv(C) * X) * (X.T * inv(C) * Y)  # Our solution vector\n",
    "    \n",
    "    return np.array(A)                              # Convert back from numpy matrix to numpy array\n",
    "\n",
    "\n",
    "def calc_uncertainties(data_x, data_y_err, degree=2):\n",
    "    \"\"\" Calculate uncertainties on these values \"\"\"\n",
    "    \n",
    "    N = len(data_x)                                 # The number of observations\n",
    "    \n",
    "    X = np.ones((N, degree+1))                       # Create a 2xN matrix of 1's\n",
    "    for k in range(degree+1):\n",
    "        X[:,k] = data_x**k                          # Set the second row to the x-values\n",
    "    X = np.matrix(X)\n",
    "    \n",
    "    C = np.identity(N) * data_y_err                 # Create the covariance matrix\n",
    "    C = np.matrix(C)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    cov = inv(X.T * inv(C) * X)                     # Calculate the uncertainties\n",
    "\n",
    "    return np.array(cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Axis limits\n",
    "for a in ax:\n",
    "    a.set_ylim(11.3, 10.35)\n",
    "    a.set_xlim(-0.15, 0.23)\n",
    "    a.set_xticks([-0.1, 0.0, 0.1, 0.2])\n",
    "\n",
    "# Plot 3.6 micron data\n",
    "ax[0].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], color='k', marker='.', label='Observations')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "a = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], RR_lyrae['e_[3.6]'], degree=1)\n",
    "tmp_y = np.zeros(len(tmp_x))\n",
    "for i in range(len(a)):\n",
    "    tmp_y += a[i] * tmp_x**i\n",
    "ax[0].plot(tmp_x, tmp_y, label='Degree 1')\n",
    "a = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], RR_lyrae['e_[3.6]'], degree=2)\n",
    "tmp_y = np.zeros(len(tmp_x))\n",
    "for i in range(len(a)):\n",
    "    tmp_y += a[i] * tmp_x**i\n",
    "ax[0].plot(tmp_x, tmp_y, label='Degree 2')\n",
    "\n",
    "\n",
    "# Plot 4.5 micron data\n",
    "ax[1].scatter(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], color='k', marker='.')\n",
    "\n",
    "tmp_x = np.linspace(-0.15, 0.23, 10)\n",
    "a = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], RR_lyrae['e_[4.5]'], degree=1)\n",
    "tmp_y = np.zeros(len(tmp_x))\n",
    "for i in range(len(a)):\n",
    "    tmp_y += a[i] * tmp_x**i\n",
    "ax[1].plot(tmp_x, tmp_y)\n",
    "a = calc_best_fit(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], RR_lyrae['e_[4.5]'], degree=2)\n",
    "tmp_y = np.zeros(len(tmp_x))\n",
    "for i in range(len(a)):\n",
    "    tmp_y += a[i] * tmp_x**i\n",
    "ax[1].plot(tmp_x, tmp_y)\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot labels\n",
    "for a in ax:\n",
    "    a.set_xlabel('Log Period (days) + 0.26')\n",
    "    a.grid()\n",
    "    \n",
    "ax[0].set_ylabel('IRAC [3.6]')\n",
    "ax[1].set_ylabel('IRAC [4.5]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deeper discussion about OLS\n",
    "\n",
    "Discuss with your partner in what regimes OLS breaks down\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "Here are a few:\n",
    "    1. So far we have assumed the data are uncorrelated. If they are correlated, it can be taken into account, but the covariance matrix will have off-diagonal terms.\n",
    "    2. OLS breaks down if the x-axis also has error bars. So far, we have assumed errors only on the y-axis.\n",
    "    3. Outliers can really cause problems for OLS.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data here is actually pretty good. To demonstrate just how bad things can get, let's consider different ways of fitting a line to data. This figure is taken from \"Linear regression in astronomy\" by Isobe et al., 1990.\n",
    "\n",
    "![title](images/Isobe_fig1.png)\n",
    "\n",
    "The slope and variance of the slope is defined as:\n",
    "\n",
    "![title](images/Isobe_fig2.png)\n",
    "\n",
    "It is important to note that the five methods described here give regression coefficients that are theoretically different from each other, and are not five different estimates of the same quantity. Let's apply this to some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stolen from https://github.com/sbird/spb_common/blob/master/leastsq.py\n",
    "def leastsq(x,y, method=3):\n",
    "    \"\"\"\n",
    "       Compute the least squares fit to y = beta x + alpha,\n",
    "       using one of the 5 methods outlined in\n",
    "       http://adsabs.harvard.edu/abs/1990ApJ...364..104Is\n",
    "       Method 1 minimises distance from Y given X (ie, the standard least squares fit)\n",
    "       Method 2 minimises distance from X given Y\n",
    "       Method 3 (recommended) is the OLS bisector, which gives a line bisecting the above two.\n",
    "       Method 4 (Orthogonal regression) minimises perpendicular distance from the line to points\n",
    "       Method 5 is the geometric mean of the slopes from methods 1 and 2.\n",
    "       Method 6 is the Theil-Sen estimator: the median of the pairwise slopes.\n",
    "       (See Akritas 95,  http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476499)\n",
    "       Returns:\n",
    "              (alpha, beta, bvar), the intercept slope and variance of the slope\n",
    "    \"\"\"\n",
    "    #Define some sums\n",
    "    xbar = np.mean(x)\n",
    "    ybar = np.mean(y)\n",
    "    xdif = x-xbar\n",
    "    ydif = y-ybar\n",
    "    sxx = np.sum(xdif**2)\n",
    "    syy = np.sum(ydif**2)\n",
    "    sxy = np.sum(ydif*xdif)\n",
    "\n",
    "    #Check for zeros\n",
    "    if sxx == 0 or syy == 0 or sxy == 0:\n",
    "        raise ValueError(\"Least Squares ill-defined\")\n",
    "    if method > 6 or method < 1:\n",
    "        raise ValueError(\"Method not recognised\")\n",
    "\n",
    "    #These formulas are taken from Table 1 of Isobe et al, page 3\n",
    "    #Minimise distance from Y given X\n",
    "    beta1 = sxy/sxx\n",
    "    #Variance of b1\n",
    "    bvar1 = np.sum(xdif**2*(ydif-beta1*xdif)**2)/sxx**2\n",
    "    #Minimise distance from X given Y\n",
    "    beta2 = syy/sxy\n",
    "    #Variance of b2\n",
    "    bvar2 = np.sum(ydif**2*(ydif-beta2*xdif)**2)/sxy**2\n",
    "    #Covariance of b1 and b2\n",
    "    covb12 = np.sum(xdif*ydif*(ydif-beta2*xdif)*(ydif-beta1*xdif))/(beta1*sxx**2)\n",
    "\n",
    "    if method == 1:\n",
    "        beta = beta1\n",
    "        bvar = bvar1\n",
    "    if method == 2:\n",
    "        beta = beta2\n",
    "        bvar = bvar2\n",
    "    if method == 3:\n",
    "        #OLS bisector: line that bisects the above two.\n",
    "        beta1p1 = 1+beta1**2\n",
    "        beta2p1 = 1+beta2**2\n",
    "        beta = (beta1*beta2 - 1 + np.sqrt(beta1p1*beta2p1))/(beta1+beta2)\n",
    "        #Variance\n",
    "        prefac = beta**2 / ( (beta1 + beta2)**2 * beta1p1 * beta2p1)\n",
    "        var = beta2p1**2 * bvar1 + 2 * beta1p1 * beta2p1 * covb12 + beta1p1**2 * bvar2\n",
    "        bvar = prefac*var\n",
    "\n",
    "    if method == 4:\n",
    "        #Orthogonal: minimise perpendicular distance from line to points\n",
    "        beta = 0.5*((beta2-1./beta1)+np.sign(sxy)*np.sqrt(4+(beta2-1./beta1)**2))\n",
    "        prefac = beta**2 / (4*beta1**2 + (beta1*beta2 - 1)**2)\n",
    "        bvar = prefac * ( bvar1/beta1**2 + 2*covb12 + beta1**2*bvar2 )\n",
    "\n",
    "    if method == 5:\n",
    "        #Reduced major axis:\n",
    "        beta = np.sign(sxy)*np.sqrt(beta1*beta2)\n",
    "        bvar = 0.25 * (beta2/beta1 * bvar1 + 2*covb12 + beta1/beta2 * bvar2)\n",
    "\n",
    "    if method == 6:\n",
    "        #Theil-Sen estimator for uncensored data: the median of the slopes.\n",
    "        yy = np.subtract.outer(y,y)\n",
    "        xx = np.subtract.outer(x,x)\n",
    "        ind = np.where(xx != 0)\n",
    "        beta = np.median(yy[ind]/xx[ind])\n",
    "        #Can't find a formula for the variance\n",
    "        bvar = 0\n",
    "\n",
    "    #The intercept\n",
    "    alpha = ybar - beta*xbar\n",
    "\n",
    "    return (alpha, beta, bvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the data\n",
    "amp_lum = np.loadtxt('data/Ampl_Lum.dat', comments='#')\n",
    "\n",
    "rot_ampl = amp_lum[:,0]\n",
    "lumin    = amp_lum[:,1]\n",
    "\n",
    "# plotting\n",
    "figure = plt.figure(figsize=(8.0, 5.0))\n",
    "figure.subplots_adjust(hspace=0.1)\n",
    "fig1 = plt.subplot(111)\n",
    "\n",
    "plt.errorbar(rot_ampl,lumin,xerr=0,yerr=0,color='k', marker='o', markeredgecolor='k',\\\n",
    "             ms=8., capsize=0, linestyle='None', fmt='')\n",
    "\n",
    "\n",
    "# OLS(Y|X)\n",
    "alpha, beta, bvar = leastsq(np.log10(rot_ampl),np.log10(lumin),method=1)\n",
    "print(\"###################  OLS(Y|X)  #######################\")\n",
    "print(\"Slope = %.2f ± %.2f\"%(beta,bvar))\n",
    "print(\"intercept = %.2f\"%(alpha))\n",
    "x = np.linspace(89,350,100)\n",
    "y = 10**(beta * np.log10(x) + alpha)\n",
    "plt.plot(x,y,c='g',label='OLS(Y|X)')\n",
    "\n",
    "# OLS(X|Y)\n",
    "alpha, beta, bvar = leastsq(np.log10(rot_ampl),np.log10(lumin),method=2)\n",
    "print(\"###################  OLS(X|Y)  #######################\")\n",
    "print(\"Slope = %.2f ± %.2f\"%(beta,bvar))\n",
    "print(\"intercept = %.2f\"%(alpha))\n",
    "x = np.linspace(89,350,100)\n",
    "y = 10**(beta * np.log10(x) + alpha)\n",
    "plt.plot(x,y,c='b',label='OLS(X|Y)')\n",
    "\n",
    "# OLS bisector\n",
    "alpha, beta, bvar = leastsq(np.log10(rot_ampl),np.log10(lumin),method=3)\n",
    "print(\"###################  OLS bisector  #######################\")\n",
    "print(\"Slope = %.2f ± %.2f\"%(beta,bvar))\n",
    "print(\"intercept = %.2f\"%(alpha))\n",
    "x = np.linspace(89,350,100)\n",
    "y = 10**(beta * np.log10(x) + alpha)\n",
    "plt.plot(x,y,c='c',label='OSL bisect')\n",
    "\n",
    "# ODR\n",
    "# see also https://docs.scipy.org/doc/scipy/reference/odr.html\n",
    "alpha, beta, bvar = leastsq(np.log10(rot_ampl),np.log10(lumin),method=4)\n",
    "print(\"###################  ODR  #######################\")\n",
    "print(\"Slope = %.2f ± %.2f\"%(beta,bvar))\n",
    "print(\"intercept = %.2f\"%(alpha))\n",
    "x = np.linspace(89,350,100)\n",
    "y = 10**(beta * np.log10(x) + alpha)\n",
    "plt.plot(x,y,c='y',label='ODR')\n",
    "\n",
    "# RMA\n",
    "alpha, beta, bvar = leastsq(np.log10(rot_ampl),np.log10(lumin),method=5)\n",
    "print(\"###################  RMA  #######################\")\n",
    "print(\"Slope = %.2f ± %.2f\"%(beta,bvar))\n",
    "print(\"intercept = %.2f\"%(alpha))\n",
    "x = np.linspace(89,350,100)\n",
    "y = 10**(beta * np.log10(x) + alpha)\n",
    "plt.plot(x,y,c='m',label='RMA')\n",
    "plt.legend(prop={'size':6})\n",
    "\n",
    "\n",
    "plt.xlabel(r'$\\mathsf{\\Delta \\theta_{max}}$ (deg)')\n",
    "plt.ylabel(r'$\\mathsf{L_{\\gamma}^{peak}}$ ($\\mathsf{erg \\, sec^{-1}}$)')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlim([80,400])\n",
    "fig1.set_xscale('log')\n",
    "fig1.set_yscale('log')\n",
    "plt.xticks([100,200,300], ['100','200','300'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should we take away from this?\n",
    "\n",
    "The simple ordinary least-squares analysis taught in statistics classes really isn't so simple, and it hides a lot. This isn't meant to scare you. On the contrary, throughout this week we are going to teach you a lot of techniques that are (hopefully) useful in your everyday research.\n",
    "\n",
    "As for the example above, if you have a situation where you really are trying to fit a line to data, and you have errors on both the $x$- and $y$-axis, then we recommend using Bayesian methods, which we will be teaching tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we want to interpolate between data?\n",
    "\n",
    "Interpolation is inherently a flawed procedure. At its core, it requires one to derive a new dependent variable $y'$ from an independent variable or variables, $\\vec{x}'$ based on a set of known $\\{ \\vec{x_0}, y_0, \\vec{x_1}, y_1, ..., \\vec{x_n}, y_n\\}$. The idea that one can derive a new value without having prior theoretical knowledge or empical constraints is bonkers. Yet, it is occasionally -- even often -- unavoidable. Here, we'll discuss some interpolation techniques.\n",
    "\n",
    "\n",
    "## $k$-Nearest Neighbor Interpolation\n",
    "\n",
    "This method is exactly what it sounds like: Find a set of $\\{ \\vec{x}_k \\}$ that is close to $\\vec{x}'$, and use the associated $\\{ y_k \\}$ to derive an estimation of $y'$. Depending on the details of this procedure, multiple approaches are possible, with varying levels of complication.\n",
    "\n",
    "This method is the most simple of them all, but as we'll see below is already fraught with complication when $\\vec{x}$ is multi-dimensional.\n",
    "\n",
    "\n",
    "#### 1. Nearest Neighbor Interpolation\n",
    "This method is the simplest of them all, simply find the nearest neighbor and adopt that value of $y$ as an estimation of $y'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "n_neighbors = 1\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "plt.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "plt.plot(T, y_, color=\"navy\", label=\"prediction\")\n",
    "plt.axis(\"tight\")\n",
    "plt.legend()\n",
    "plt.title(\"Nearest Neighbor\")\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Average of the $k$-nearest neighbors\n",
    "A clear improvement to the nearest neighbor interpolation technique is to expand to multiple nearest neighbors. This method is denoted $k$-nearest neighbors, where $k$ is the number of nearest neighbors incorporated into the calculation. In its simplest realization, one could adopt the average of the $k$-nearest $y$ values as the value of $y'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "plt.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "plt.plot(T, y_, color=\"navy\", label=\"prediction\")\n",
    "plt.axis(\"tight\")\n",
    "plt.legend()\n",
    "plt.title(\"5-Nearest Neighbor\")\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "# plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors, weights))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Weighted average\n",
    "As a second method, one could add weights to the simple average. How should one choose the weights? Typically these are based on the distance, so closer points are weighted more heavily. Weights could be $1/d$ or even $1/d^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "plt.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "plt.plot(T, y_, color=\"navy\", label=\"prediction\")\n",
    "plt.axis(\"tight\")\n",
    "plt.legend()\n",
    "plt.title(\"5-Nearest Neighbor, distance-weighted\")\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Kernel Method\n",
    "Rather than weight by the distance, one could use a kernel method. The most obvious is to use a Gaussian kernel, but other options are viable. The critical characteristics of a kernel are: 1) it is positive for all values, and 2) it asymptotes to zero at large distances. Kernels all have a characteristic size or \"bandwidth\", which acts as a smoothing length. Mathematically, this is described as:\n",
    "$$\n",
    "f(x) = \\frac{\\sum^N_{i=0} K\\left(\\frac{||x_i - x||}{h}\\right) y_i}{\\sum^N_{i=0} K\\left(\\frac{||x_i - x||}{h}\\right)}\n",
    "$$\n",
    "\n",
    "This is equivalent to a weighted average\n",
    "$$\n",
    "f(x) = \\frac{1}{N} \\sum^N_{i=0} w_i y_i,\n",
    "$$\n",
    "where the weights, $w_i$, are determined from:\n",
    "$$\n",
    "w_i = \\frac{K\\left(\\frac{||x_i - x||}{h}\\right) }{\\sum^N_{i=0} K\\left(\\frac{||x_i - x||}{h} \\right)}\n",
    "$$\n",
    "\n",
    "Note that, generally, the bandwidth is more important than the exact functional form of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(x, x_set, h, kernel):\n",
    "    \n",
    "    weights = kernel(x, x_set, h)\n",
    "\n",
    "    return weights/np.sum(weights)\n",
    "\n",
    "def gaussian_kernel(x, x_set, h):\n",
    "    \n",
    "    u = np.abs(x-x_set) / h\n",
    "    \n",
    "    return 1/np.sqrt(2*np.pi) * np.exp(-0.5*u**2)\n",
    "\n",
    "def truncnorm_kernel(x, x_set, h):\n",
    "    \n",
    "    u = np.abs(x-x_set) / h\n",
    "    \n",
    "    return truncnorm.pdf(u, -1, 1)\n",
    "\n",
    "def tophat_kernel(x, x_set, h):\n",
    "    \n",
    "    u = np.abs(x-x_set) / h\n",
    "    \n",
    "    kernel = np.zeros(len(x_set))\n",
    "    kernel[np.where(u < 1)[0]] = 0.5\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "def epanechnikov_kernel(x, x_set, h):\n",
    "    \n",
    "    u = np.abs(x-x_set) / h\n",
    "    \n",
    "    kernel = np.zeros(len(x_set))    \n",
    "    kernel[np.where(u < 1)[0]] = 3/4 * (1 - u[np.where(u < 1)[0]]**2)\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "def triangle_kernel(x, x_set, h):\n",
    "    \n",
    "    u = np.abs(x-x_set) / h\n",
    "    \n",
    "    kernel = np.zeros(len(x_set))    \n",
    "    kernel[np.where(u < 1)[0]] = 1 - u[np.where(u < 1)[0]]\n",
    "\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = np.linspace(-2.5, 2.5, 100)\n",
    "\n",
    "plt.plot(tmp_x, gaussian_kernel(0, tmp_x, 1), label='Gaussian')\n",
    "plt.plot(tmp_x, truncnorm_kernel(0, tmp_x, 1), label='Truncated Gaussian')\n",
    "plt.plot(tmp_x, tophat_kernel(0, tmp_x, 1), label='Tophat')\n",
    "plt.plot(tmp_x, epanechnikov_kernel(0, tmp_x, 1), label='Epanechnikov')\n",
    "plt.plot(tmp_x, triangle_kernel(0, tmp_x, 1), label='Triangular')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.ylim(0, 1.1)\n",
    "plt.xlim(-2.5, 2.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14,6))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40))\n",
    "T = np.linspace(0, 5, 500)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "\n",
    "labels = ['Tophat', 'Gaussian', 'Truncated Gaussian', 'Epanechnikov', 'Triangular']\n",
    "kernels = [tophat_kernel, gaussian_kernel, truncnorm_kernel, epanechnikov_kernel, triangle_kernel]\n",
    "h_set = [0.15, 0.5, 2]\n",
    "titles = ['Kernel h=0.15', 'Kernel h=0.5', 'Kernel h=2']\n",
    "\n",
    "for k, h in enumerate(h_set):\n",
    "    \n",
    "    for j in range(len(labels)):\n",
    "        y_ = np.zeros(len(T))\n",
    "        for i in range(len(T)):\n",
    "            weights = calc_weights(T[i], X, h, kernels[j])\n",
    "            y_[i] = np.average(y, weights=weights)\n",
    "        ax[k].plot(T, y_, label=labels[j])\n",
    "        \n",
    "    ax[k].set_title(titles[k])\n",
    "\n",
    "    ax[k].scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlim(0, 5)\n",
    "    a.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss with your partner the differences between the different kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Kernel with a variable scale\n",
    "For many data sets, there are significant variations in the sparsity of data across the domain. Ideally, when data are more sparsely populated, one can adopt a larger kernel size, while more densely packed data ought to have a smaller kernel. The typical method here is to set the kernel size to be equal to the distance to the $k$-nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epanechnikov_kernel_variable_bandwidth(x, x_set, h, k=5):\n",
    "    \n",
    "    dist = np.sort(np.abs(x-x_set))\n",
    "    \n",
    "    u = np.abs(x-x_set) / dist[k]\n",
    "    \n",
    "    kernel = np.zeros(len(x_set))    \n",
    "    kernel[np.where(u < 1)[0]] = 3/4 * (1 - u[np.where(u < 1)[0]]**2)\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "\n",
    "\n",
    "y_ = np.zeros(len(T))\n",
    "for i in range(len(T)):\n",
    "    weights = calc_weights(T[i], X, 0.0, epanechnikov_kernel_variable_bandwidth)\n",
    "    y_[i] = np.average(y, weights=weights)\n",
    "plt.plot(T, y_, label='Model')\n",
    "\n",
    "plt.title('Variable bandwidth')\n",
    "\n",
    "plt.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: parametric methods\n",
    "\n",
    "While kernel methods are non-parametric, regression methods are parametric: one first seeks to find a function $f(\\vec{x})$ which best describes the dataset -- either locally or globally -- then evaluates that function at some new point $\\vec{x}'$.\n",
    "\n",
    "Since we've already gone through the math and defined the functions, we are ready to apply our methods. One of the nice things about regression methods is that we can properly incorporate error bars in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_polynomial_fit(x_data, y_data, degree=2):\n",
    "    \n",
    "    # get data size\n",
    "    N = len(x_data)\n",
    "\n",
    "    # Construct matrices\n",
    "    Y = np.matrix(y_data).T\n",
    "    A = np.ones((N, degree+1))\n",
    "    for k in range(degree+1):\n",
    "        A[:,k] = x_data**k\n",
    "    A = np.matrix(A)\n",
    "    \n",
    "    I = np.matrix(np.identity(N))\n",
    "    \n",
    "    # solve matrix math to obtain our slope and intercept\n",
    "    model = inv(A.T * I * A) * (A.T * I * Y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "for i, a in enumerate(ax):\n",
    "\n",
    "    degree = degrees[i]\n",
    "    model = calc_polynomial_fit(X, y, degree=degree)\n",
    "\n",
    "    model_x = np.linspace(0, 5, 500)\n",
    "    model_y = np.zeros(len(model_x))\n",
    "    for k in range(degree+1):\n",
    "        model_y += model[k,0]*model_x**k\n",
    "\n",
    "    a.plot(model_x, model_y)\n",
    "\n",
    "    a.set_title('degree-%i Polynomial Fit'%degree)\n",
    "    \n",
    "    a.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "\n",
    "    a.set_xlim(0, 5)\n",
    "    a.grid()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data_x = np.sort(5 * np.random.rand(40))\n",
    "data_y = np.sin(data_x).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "data_y_err = 0.1*np.random.normal(size=40)\n",
    "data_y += data_y_err\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and plot best fit model\n",
    "degree = 3\n",
    "model = calc_best_fit(data_x, data_y, data_y_err, degree=degree)\n",
    "\n",
    "model_x = np.linspace(0, 5, 500)\n",
    "model_y = np.zeros(len(model_x))\n",
    "for k in range(degree+1):\n",
    "    model_y += model[k,0]*model_x**k\n",
    "\n",
    "plt.plot(model_x, model_y, label='degree-%i Polynomial Fit'%degree)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and plot best fit model with uncertainties\n",
    "model_cov = calc_uncertainties(data_x, data_y_err, degree=degree)\n",
    "x_out = np.random.multivariate_normal(model.flatten(), model_cov, size=100)\n",
    "\n",
    "\n",
    "# Plot lines\n",
    "for i in np.arange(100):\n",
    "    model_y = np.zeros(len(model_x))\n",
    "    for k in range(degree+1):\n",
    "        model_y += x_out[i,k]*model_x**k\n",
    "\n",
    "    plt.plot(model_x, model_y, color='r', alpha=0.05)\n",
    "\n",
    "\n",
    "plt.scatter(X, y, color=\"darkorange\", label=\"data\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Estimation\n",
    "\n",
    "Let's apply our understanding of \n",
    "\n",
    "Our understanding of Kernels and interpolation can be translated to density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting import hist \n",
    "\n",
    "rng = np.random.RandomState(10)  # deterministic random data                    #\n",
    "a = np.hstack((rng.normal(size=1000), rng.normal(loc=5, scale=2, size=1000)))   #\n",
    "                                                                                #\n",
    "plt.subplot(221)                                                                #\n",
    "hist(a, histtype='step')                                         #\n",
    "plt.title('Blocks')                                                             #\n",
    "plt.subplot(222)                                                                #\n",
    "hist(a, histtype='step', bins='knuth')                                          #\n",
    "plt.title('Knuth')                                                              #\n",
    "plt.subplot(223)                                                                #\n",
    "hist(a, histtype='step', bins='scott')                                          #\n",
    "plt.title('Scott')                                                              #\n",
    "plt.subplot(224)                                                                #\n",
    "plt.title('Freedman')                                                           #\n",
    "hist(a, histtype='step', bins='freedman')                                       #\n",
    "plt.gcf().set_size_inches(6, 6)                                                 #\n",
    "                                                                                #\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the progression of histograms to kernels\n",
    "np.random.seed(1)\n",
    "N = 20\n",
    "X = np.concatenate((np.random.normal(0, 1, size=int(0.3 * N)),\n",
    "                    np.random.normal(5, 1, size=int(0.7 * N))))[:, np.newaxis]\n",
    "X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n",
    "bins = np.linspace(-5, 10, 10)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "# histogram 1\n",
    "ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', density=True)\n",
    "ax[0, 0].text(-3.5, 0.31, \"Histogram\")\n",
    "\n",
    "# histogram 2\n",
    "ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', density=True)\n",
    "ax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n",
    "\n",
    "# tophat KDE\n",
    "kde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)\n",
    "log_dens = kde.score_samples(X_plot)\n",
    "ax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\n",
    "ax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n",
    "\n",
    "# Gaussian KDE\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)\n",
    "log_dens = kde.score_samples(X_plot)\n",
    "ax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\n",
    "ax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n",
    "\n",
    "for axi in ax.ravel():\n",
    "    axi.plot(X[:, 0], np.zeros(X.shape[0]) - 0.01, '+k')\n",
    "    axi.set_xlim(-4, 9)\n",
    "    axi.set_ylim(-0.02, 0.34)\n",
    "\n",
    "for axi in ax[:, 0]:\n",
    "    axi.set_ylabel('Normalized Density')\n",
    "\n",
    "for axi in ax[1, :]:\n",
    "    axi.set_xlabel('x')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "380132c689814f0c84a2760d02d761db",
   "lastKernelId": "de937239-811d-4c12-9be3-dc819a55f100"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
