{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A small introduction\n",
    "\n",
    "## What are Convolutional Neural Networks (CNN)\n",
    "\n",
    "> Is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. <br>\n",
    ">\n",
    "> _[A Comprehensive Guide to Convolutional Neural Networks â€” the ELI5 way, by Sumit Saha](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)_\n",
    "\n",
    "## Example of CNN architecture\n",
    "\n",
    "![CNN_schematic](https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n",
    "\n",
    "The design of CNN allows to apply similar concepts to Neural Networks with special data processing techniques on data and between layers to learn from image data. \n",
    "\n",
    "## Components\n",
    "\n",
    "**Convolutional Layers**\n",
    "\n",
    "The convultion in CNNs is a technique inspired by the organization of the visual cortex, as neurons respond to stimulius in a given field of view. The convolution is a way to propogate information from nearby pixels in an image. \n",
    "\n",
    "The aim of CNN is to **reduce the dimensions** and **keep the importan features** that help in good predictions.  \n",
    "\n",
    "Essentially a convolution is a matrix multiplication between the image and a *kernel* (another matrix, smaller than the image). Note, the shape of your input data has changed after going through the convolution (_'valid padding'_, in contrast to _'same padding'_ where the original dimensions are kept)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/kernel_snapshot.png\"> </img>\n",
    "    </div>\n",
    "\n",
    "![convolution](https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "Convoluting a 5x5x1 image with a 3x3x1 kernel to get a 3x3x1 convolved feature. The kernel (shown in yellow) takes into account only the pixels in the two diagonals (marked as 'x1' in the lower right corner of the yellow matrix). Therefore, in the first (frozen) image there are 9 pixels with the kernel considering 5 of them with values : 1+1+0+1+1 = 4 (the value trasnfered to the convoled feature).\n",
    "</div>\n",
    "\n",
    "The kernel is not necessary to move one pixel at a time. By chaning the _stride_ we can select any kind of movement, which includes both the width and the height. A (1,1) stride will move one pixel right (stating always from the top left corner) and after completing the row it will move one pixel down (and left again). A (2,2 will do the similar thing but with two pixels moves. Hoever, in this case we also **downsampling** the extracted feature. \n",
    "\n",
    "**Activaltion function**\n",
    "\n",
    "The function used to impose a non-linear transformation to the input data. Perhaps the most typical one used is the ReLU (Rectified Linear Unit), which has the advantage of not activating all neurosn at the same time.  \n",
    "\n",
    "**Pooling**\n",
    "\n",
    "Sometimes data is big and we want to speed up the process. Can we *pool* some cells together to reduce our data size between convolutions? Yes! The technique is called (obviously...) _pooling_ and it can be performed by either taking the average of all the pixels that the pooling layer is over the feature layer (**average pooling**) or the maximum value found in any of the pixels (**max pooling**). \n",
    "\n",
    "![pooling_2](https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png)\n",
    "<div style=\"text-align: center;\">\n",
    "Examples of max and average pooling. \n",
    "</div>\n",
    "\n",
    "![pooling_1](https://miro.medium.com/max/792/1*uoWYsCV5vBU8SHFPAPao-w.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "A 3x3 max pooling acting over a 5x5 feature map. \n",
    "</div>\n",
    "\n",
    "The benefits of pooling layers are: i. the **decrease of dimensions** that help the decrease the computational power, ii. they extract the most **dominant features which are rotational and positional invariant**. \n",
    "\n",
    "There are two flavors of pooling layers, either local (with dimensions smaller that the feature dimensions) or _global_ that act on the whole feature layer (and they actually convert it to a single value), which is more aggressive. \n",
    "\n",
    "**Fully connected layers**\n",
    "\n",
    "Once input images have been convolved and reduced into an appropriate size, they can be pushed through a fully connected layer into the familiar NN architecture. This layer learns converts all high-level features as identified from the previous part of the network into the final output (classes) \n",
    "\n",
    "**Dropout**\n",
    "\n",
    "One way to prevent overfitting is the dropout method - remove individual nodes from the network (with some probability) at each training stage. This could be at the level of the input node or at hidden layers.\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "Each layer's weights (and therefore outputs) are updated every training iteration. More layers can mean larger changes down the network (nonlinear behavior), for small changes in weights, so small learning rates may be needed which makes training hard. Instead we may enforce each layer to prduce **predictable** output from layer to layer using batch normalization giving more stable behavior and reducing training time. Predictbale in this case means that the distribution of outputs from the previous layer has specific properties: unit variance, zero mean. In other words it is a technique to standardize the input to a layer. ([Ioffe & Szegedy 2015](https://arxiv.org/abs/1502.03167))\n",
    "\n",
    "(Source Images and material Most of this is from [this web article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53).\n",
    "\n",
    "\n",
    "\n",
    "**Example classification networks**\n",
    "\n",
    "AlexNet & LeNet: image classification networks - \"In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010, AlexNet was trained to classify 1.2 million high-resolution images into 1000 different classes. It achieved top-1 and top-5 error rates of 37.5% and 17%, which outperforms state-of-the-art methods at that time.\" [article](https://medium.com/mlearning-ai/alexnet-and-image-classification-8cd8511548b4)\n",
    "    \n",
    "![example network architectures](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Galaxy morphology estimation\n",
    "\n",
    "**TASK 1: Build a network to classify stars, spiral and elliptical galaxies from synthetic data with noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import keras\n",
    "from IPython.display import clear_output\n",
    "import keras.utils as ult\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization,Conv3D, MaxPooling3D, Dense, Add, Activation\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an auxiliary function to plot the accuracy and loss value during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.losses2 = []\n",
    "        self.val_losses2 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.losses2.append(logs.get('categorical_accuracy'))\n",
    "        self.val_losses2.append(logs.get('val_categorical_accuracy'))\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.x, self.losses2, label=\"Training accuracy\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses2, label=\"Testing accuracy\",linestyle='--')\n",
    "        plt.ylim(0,1)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.x, self.losses, label=\"Training loss\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses, label=\"Testing loss\",linestyle='--')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,galaxy_labels):\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[0]))\n",
    "    plt.imshow(images[0,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[0,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[0,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[1]))\n",
    "    plt.imshow(images[1,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[1,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[1,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[5]))\n",
    "    plt.imshow(images[5,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[5,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[5,:,:,2], vmax=255)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_trans(label_id):\n",
    "    if label_id==0: return \"star\"\n",
    "    if label_id==1: return \"spiral galaxy\"\n",
    "    if label_id==2: return \"elliptical galaxy\"\n",
    "    else: return \"unknown\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "**data**: are images at different wavelenghts i.e. 3D with 2 spatial (41x41 pixels) and 1 spectral (3 bands) dimension     \n",
    "**labels**: take values 0: star, 1: spiral galaxy, 2: elliptical galaxy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/galaxy_cubes.npz') as data:\n",
    "    images = data['images']\n",
    "    galaxy_labels = data['labels']\n",
    "\n",
    "print([images.shape, galaxy_labels.shape])\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add white noise to observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= images+np.random.randn(10000,41,41,3)*20\n",
    "images= np.clip(images, 0, 255)\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "## Create training and testing (validation) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** to test various architectures fast keep the train/test sizes rather small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples =   \n",
    "num_test_examples  =   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train=images[0:num_train_examples,:,:,:]\n",
    "images_test=images[2500:2500+num_test_examples,:,:,:]\n",
    "\n",
    "Train_data=images_train.reshape(num_train_examples, images.shape[1],images.shape[2],images.shape[3],1)\n",
    "Test_data=images_test.reshape(num_test_examples, images.shape[1],images.shape[2],images.shape[3],1)\n",
    "\n",
    "train_labels=galaxy_labels[0:num_train_examples]\n",
    "test_labels=galaxy_labels[2500:2500+num_test_examples]\n",
    "\n",
    "train_labels_cat=ult.to_categorical(train_labels,num_classes=3)\n",
    "test_labels_cat=ult.to_categorical(test_labels,num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network layers and characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((images.shape[1], images.shape[2], images.shape[3], 1),name='main_input')\n",
    "\n",
    "conv00  = Conv3D(16, (3, 3, 2), strides=(1, 1, 1), padding='same', name='conv00')(inputs)\n",
    "act00 = Activation('relu')(conv00)\n",
    "pool00  = MaxPooling3D(pool_size=(3, 3, 1), strides=(2, 2, 1), padding='same')(act00)\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "(add more layers according to your taste!)\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "fl0 = Flatten(name='fl0')(  NAME OF LAST LAYER HERE )\n",
    "#Do0 = Dropout(rate=0.5)(fl0)  # example of a dropout layer if needed\n",
    "fc0 = Dense( PICK A VALUE ,activation='linear')(fl0)\n",
    "\n",
    "fc1 = Dense( PICK A VALUE ,activation='linear')(fc0)\n",
    "\n",
    "\n",
    "Dn0 = Dense( PICK A VALUE ,activation='softmax', name='Dn0' )(fc1)\n",
    "\n",
    "model_1 = Model(inputs=[inputs], outputs=[Dn0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optzr =  Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer=optzr, metrics =['categorical_accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** to test training fast keep the batch_size larger, and the epochs smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()        \n",
    "history=model_1.fit( ... , ... , \n",
    "                    batch_size=  ... , \n",
    "                    epochs= ... ,\n",
    "                    validation_data=[ ... , ... ],\n",
    "                    callbacks=[plot_losses],shuffle=True)\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls,acc=model_1.evaluate( ... , ... )\n",
    "print(\"Loss value: %.2f\" % (ls))  \n",
    "print(\"Accuracy: %.1f\" % (acc*100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict label for particular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select object\n",
    "obj = 3        # mush be < len(Test_data)\n",
    "\n",
    "preds=model_1.predict(Test_data[obj:obj+1,:,:,:,:])\n",
    "print(preds)\n",
    "print(test_labels[obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use intemediate layers as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1=model_1.layers[1].output  \n",
    "lr2=model_1.layers[4].output\n",
    "lr3=model_1.layers[7].output\n",
    "\n",
    "activation_model_lr1 = Model(inputs=[inputs], outputs=lr1)\n",
    "activation_model_lr2 = Model(inputs=[inputs], outputs=lr2)\n",
    "activation_model_lr3 = Model(inputs=[inputs], outputs=lr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the activations for particular inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = np.random.randint(0,100)\n",
    "print(s)\n",
    "\n",
    "plt.imshow(Test_data[s,:,:,0,0])\n",
    "plt.title('Input image')\n",
    "plt.show()\n",
    "\n",
    "activations_lr1 = activation_model_lr1.predict(Test_data[s:s+1,:,:,:,:]) \n",
    "activations_lr2 = activation_model_lr2.predict(Test_data[s:s+1,:,:,:,:]) \n",
    "activations_lr3 = activation_model_lr2.predict(Test_data[s:s+1,:,:,:,:]) \n",
    "\n",
    "for i in range(16):\n",
    "    img=activations_lr3[0,:,:,0,i]\n",
    "    plt.imshow(img)\n",
    "    plt.title('Number ' + str(i))\n",
    "    plt.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transfer Learning\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/ResNet-50-architecture.png\"> \n",
    "Figure 4.1. The ResNet-50 architecture. <br>\n",
    "(Credit: <a href=\"https://www.researchgate.net/figure/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and_fig1_338603223\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">source</a>)\n",
    "    </img>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        # New FC layer, random init\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(205, 205, 3))\n",
    "\n",
    "FC_LAYERS = [256]\n",
    "dropout = 0.5\n",
    "\n",
    "finetune_model = build_finetune_model(base_model, dropout=dropout, fc_layers=FC_LAYERS, num_classes=3)\n",
    "\n",
    "finetune_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "n_new_imgs = 100  # 3000\n",
    "images2=np.empty([n_new_imgs,205,205,3])\n",
    "for i in range(n_new_imgs):\n",
    "    for c in range(3):\n",
    "        tmp=images[i,:,:,c]\n",
    "        img = Image.fromarray(tmp)\n",
    "        img2 = img.resize((205,205),Image.BICUBIC)\n",
    "        images2[i,:,:,c]=img2    \n",
    "\n",
    "#show_figures(images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if n_new_imgs = 3000\n",
    "# Train_data=images2[0:num_train_examples,:,:,:]\n",
    "# Test_data=images2[2500:2500+num_test_examples,:,:,:]\n",
    "\n",
    "# for faster computation time\n",
    "num_train_examples = int(n_new_imgs*0.7)\n",
    "num_test_examples = n_new_imgs-num_train_examples\n",
    "Train_data=images2[0:num_train_examples,:,:,:]\n",
    "Test_data=images2[num_train_examples:num_train_examples+num_test_examples,:,:,:]\n",
    "\n",
    "\n",
    "print(Train_data.shape)\n",
    "print(Test_data.shape)\n",
    "\n",
    "train_labels=galaxy_labels[0:num_train_examples]\n",
    "test_labels=galaxy_labels[2500:2500+num_test_examples]\n",
    "\n",
    "train_labels_cat=ult.to_categorical(train_labels,num_classes=3)\n",
    "test_labels_cat=ult.to_categorical(test_labels,num_classes=3)\n",
    "\n",
    "print(train_labels_cat.shape)\n",
    "print(test_labels_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** to test training fast keep the batch_size larger, and the epochs smaller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.compile(optzr, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "history = finetune_model.fit(Train_data,train_labels_cat, batch_size= ..., epochs= ... ,validation_data=[Test_data,test_labels_cat],callbacks=[plot_losses],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls,acc=finetune_model.evaluate(Test_data,test_labels_cat)\n",
    "print(\"Loss value: %.2f\" % (ls))  \n",
    "print(\"Accuracy: %.1f\" % (acc*100))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "If the training accuracy is very close to 1 and the training loss always systematically under the test loss then the model is overfitting!\n",
    "    \n",
    "Check <a href='https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/' target=\"_blank\" rel=\"noopener noreferrer\"> this site</a> for how to diagnosize the various cases.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
