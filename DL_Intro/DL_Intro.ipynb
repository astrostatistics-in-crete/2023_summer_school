{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-release",
   "metadata": {},
   "source": [
    "<font size=6>**Deep Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-performance",
   "metadata": {},
   "source": [
    "**_Deep Learning_** is a type of Machine Learning which is characterized by being **deep**.\n",
    "\n",
    "Meaning, it uses **multiple layers** to process the input information (Figure 0).\n",
    "<img src=\"images/Simple_vs_Deep.png\">\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Simple_vs_Deep.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 0.  A simple <i>feedforward</i> Neural Network compared with a Deep <i>feedforward</i> Neural Network.<br>\n",
    "            (From <a href=\"https://thedatascientist.com/what-deep-learning-is-and-isnt/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The actual way the depth is designed can be very different. It could be achieved e.g. by **stacking** sequential layers (_feedforward neural networks_), via **recurrent** layers (_recurrent neural networks_), via a \"**mix**\" of these two approaches (_U-nets_), and many other ways.\n",
    "\n",
    "Don't worry: we will explain how to _computationally_ create neurons/layers [later](#Generic_Architecture_and_Neurons)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-radius",
   "metadata": {},
   "source": [
    "# Why Deep Learning is cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-calculator",
   "metadata": {},
   "source": [
    "    It is not, we are geeks, and that's the truth.\n",
    "\n",
    "However ... we do live in the era of \"Big Data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_surveys = pd.DataFrame([\n",
    "    ['2MASS',                                  1997,    20, 25.4],\n",
    "    ['Sloan Digital Sky Survey (SDSS)',        2000,   200, 50],\n",
    "    ['Large Synoptic Survey Telescope (LSST)', 2023,  30e3, 200e3],\n",
    "    ['Square Kilometer Array (SKA)',           2027, 150e3, 4.6e6]\n",
    "], columns=['Sky Survey Project', 'First Light', 'Velocty (GB/day)', 'Volume (TB)']).reset_index(drop=True)\n",
    "\n",
    "df_surveys[df_surveys.columns[1:]] = df_surveys[df_surveys.columns[1:]].astype(int)\n",
    "\n",
    "display(df_surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cutecharts.charts as ctc\n",
    "\n",
    "chart = ctc.Line(\"Survey size evolution\", width='500px')\n",
    "chart.set_options(labels=list(df_surveys['First Light']), x_label='Year', y_label='Volume (TB)')\n",
    "chart.add_series('year',list(df_surveys['Volume (TB)']))\n",
    "chart.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-murder",
   "metadata": {},
   "source": [
    "We cannot expect to humanly inspect these data and derive the intuition for the rules which categorize them.\n",
    "\n",
    "$\\rightarrow$ We have to leverage on:\n",
    "\n",
    "- the **large number** of examples\n",
    "\n",
    "- algorithms that can abstract **arbitrarily complex** rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-assurance",
   "metadata": {},
   "source": [
    "## So how does Deep Learning address big data issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-worker",
   "metadata": {},
   "source": [
    "The basic idea is that layers construct **new features**.\n",
    "\n",
    "In practice, Deep Learning systems include implicit **feature engeneering** _on top_ of the learning task (e.g., _classificaton_ or _regression_).<br>\n",
    "\n",
    "In this way, they are a step forward with respect to \"classic\" ML approaches (Figure 1).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Feature_Engeneering.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.  A Deep Neural Network seen as a combination of feature extractor + learner (e.g. classifier or regressor).<br>\n",
    "            (From <a href=\"https://stats.stackexchange.com/questions/562466/neural-networks-automatically-do-feature-engineering-how/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "From this perspective, the connections between the network neurons represent **potential correlations** betweeen features.\n",
    "\n",
    "<u>What are the implications?</u>\n",
    "\n",
    "The scientist **does _not_ have to get detailed insight of the problem** to build the proper features or select the proper classifier<br>\n",
    "$\\rightarrow$ the DL system does it all for us!\n",
    "\n",
    "This comes particularly handy when we deal with databases with **millions of objects** and **hundreds of features**!\n",
    "\n",
    "<u>References</u>\n",
    "\n",
    "In case you are curious, it has been proven that Deep Neural Networks are indeed \"**_universal approximators_**\"\n",
    "(e.g. [Kurt Hornik (1991), Neural Networks, 4, 2](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub)), meaning that they can in principle explain any linear or non-linear relation beteen the features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-separation",
   "metadata": {},
   "source": [
    "## Some example applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-accessory",
   "metadata": {},
   "source": [
    "Indeed, Deep Learning (hereafter, **DL**) is being used to solve very _different_ problems, e.g.:\n",
    "\n",
    "- **Self-Driving cars**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_Self_Driving.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.2.a.  NVIDIA's driverless car simulator.<br>\n",
    "            (From <a href=\"https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf/\">\"End to End Learning for Self-Driving Cars\" (2016)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Protein Structure Prediction**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_AlphaFold.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.2.b.  Deep Mind's Alpha Fold network for the prediction of molecular structures of proteins.\n",
    "            Original paper: <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Jumper, J., Evans, R., Pritzel, A. et al. 2021,  Nature, 596, 583</a>.<br>\n",
    "            (From <a href\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Natural Language Processing, translation, and text generation**\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_NLP.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.2.c.  Google's unified text-to-text transformer.<br>\n",
    "            (From <a href\"https://arxiv.org/abs/1910.10683\">Raffel et al. 2021, arxiv/1910.10683</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Computer Vision (lots and lots of it!)**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "    <img src=\"https://scontent-vie1-1.xx.fbcdn.net/v/t39.2365-6/10000000_3947476245325303_7673388906041049088_n.png?_nc_cat=107&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=to_z8oxJaisAX_1lJ8s&_nc_ht=scontent-vie1-1.xx&oh=00_AfDyNwVlX9BSki5r28HOjoteYlR7lyR3I_Hidq3rxC9Gcg&oe=6473E54D\" alt=\"Detectron example\">\n",
    "        <center>\n",
    "        Figure 1.2.d.  Facebook's Detectron2 for multiple computer vision tasks.<br>\n",
    "        (From <a href\"https://ai.facebook.com/tools/detectron2/\">Meta AI blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "... and many, many other _scary_ applications like:\n",
    "\n",
    "- **Deep Fakes**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_DeepFake.jpg\">\n",
    "        <center>\n",
    "        Figure 1.2.e.  Deep Fakes can be used to bring back actors from when the cinema was actually good (i.e., before 1999!), but also to produce false evidence.  Luckily, there are already ML efforts to uncover Deep Fakes, e.g. <a href\"https://arxiv.org/abs/2101.01456/\">Zi et al. 2021, arXiv/2101.01456\n",
    "</a>.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **AI chats**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ChatGPT.png\">\n",
    "        <center>\n",
    "        Figure 1.2.f.  Large Language Models applied as chat tools necessarily bring along the creators' moral judgment.\n",
    "        (From <a href\"https://openai.com/blog/chatgpt\">OpenAI's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Video Games**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"https://assets-global.website-files.com/621e749a546b7592125f38ed/62271e2f604e640534eeca99_AlphaStar%2003.gif\">\n",
    "        <center>\n",
    "        Figure 1.2.g.  Deep Mind's Alpha Star absolutely demolishng a human player who later claimed ... erhm ... that the internet connection was bad that day because ... ehrm, mmmh ... someone in the house was watching Netflix.<br>\n",
    "        (From <a href\"https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "Catching up with all the new DL developments is becoming physically impossible, but you can follow great channels like [Two Minute Papers](https://www.youtube.com/c/K%C3%A1rolyZsolnai/featured) to try and stay updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-tractor",
   "metadata": {},
   "source": [
    "## Deep Learning in Astronomy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-sigma",
   "metadata": {},
   "source": [
    "The application of DL in Astronomy is still at an **amatour level**, with respect to what happens in the industry (_prejudice against the \"black box\"?_).<br>  However ... \n",
    "\n",
    "Astronomy is the perfect ML lab because it offers:\n",
    "- tough problems to solve\n",
    "- large data\n",
    "\n",
    "In fact, Deep Learning publications are **exploding** in Astronomy (Figure 2)!\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Learning_astro_papers.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.a. Number of refereed astronomy papers containing the text \"Deep Learning\" in their abstracts.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/search/filter_property_fq_property=AND&filter_property_fq_property=property%3A%22refereed%22&fq=%7B!type%3Daqp%20v%3D%24fq_property%7D&fq_property=(property%3A%22refereed%22)&q=abs%3A%22deep%20learning%22%20year%3A2015-2022&sort=date%20desc%2C%20bibcode%20desc&p_=0\">NASA ADS</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<font size=3><u>**Some notable examples**</u><font>\n",
    "\n",
    "**Galaxy Classification**\n",
    "    \n",
    "- [Dieleman et al. (2015), MNRAS, 450, 1441](https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract) $-$ calculate probabilities for the 37 Galaxy Zoo possible answers\n",
    "\n",
    "    - **training**: classification of 61,578 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: standard CNN\n",
    "\n",
    "<table><tr>\n",
    "    <td width=420>\n",
    "        <img src=\"images/Galaxy_Zoo_flowchart.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.b. Galaxy Zoo classification tree.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2013yCat..74352835W/abstract\">Willet et al. (2013)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dieleman_Fig11.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.c. Activation of the CNN layers.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract\">Dieleman et al. (2015)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "- [Ackerman et al. 2017, MNRAS, 479, 415](https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract) $-$ identify mergers\n",
    "\n",
    "    - **training**: classification of ~4000 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: CNN with transfer learning\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Ackerman_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.d. Some galaxy pairs confidently identified as mergers.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "**Galaxy Morphology**\n",
    "    \n",
    "- [Aragon-Calvo et al. 2020, MNRAS, 498, 3713](https://ui.adsabs.harvard.edu/abs/2020MNRAS.498.3713A/abstract) $-$ obtain structural parameters via self-supervised learning\n",
    "\n",
    "    - **training**: re-produce parameters used to generate artificial galaxies\n",
    "    - **architecture**: semantic autoencoder\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Aragon_Semantic_Autoencoder.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.e. Fitting of morpological structural parameters with an Autoencoder.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "    \n",
    "**Serendipitous Detection**  \n",
    "    \n",
    "- [Lanusse et al. 2018, MNRAS, 473, 3895](https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract) $-$ spot gravitational lenses\n",
    "\n",
    "    - **training**: 20,000 LSST-like observations\n",
    "    - **architecture**: CNN + ResNet\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/DeepLens_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.f. Some images correctly identified as hosting lenses.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract\">Lanusse et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "- [Dekany & Grebel al. 2020, ApJ, 898, 46](https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract) $-$ spot fundamental-mode RR Lyrae stars \n",
    "\n",
    "    - **training**: 10$^7$$-$10$^8$ near-IR photometric time-series\n",
    "    - **architecture**: RNN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dekani_Fig4.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.g. Spatial distribution of the objects used as training set.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract\">Dekany & Grebel al. (2020)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "    \n",
    "**Image Reconstruction**\n",
    "       \n",
    "- [Schawinski et al. 2017, MNRAS, 467, 110](https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract) $-$ image denoising\n",
    "\n",
    "    - **training**: 4550 nearby SDSS galaxies\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Schawinski_Fig2.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.h. Degraded image details reconstructed by a GAN.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract\">Schawinski et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "**Cosmological Simulations**\n",
    "    \n",
    "- [Rodríguez et al. 2018, ComAC, 5, 4](https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract) $-$ create computationally _cheap_ cosmological simulations\n",
    "\n",
    "    - **training**: 10 independent L-PICOLA simulation boxes\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Rodriguez_Fig1.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.i. Comparison between the results of a N-body simulation (<i>left</i>) and from a GAN (<i>right</i>).<br>\n",
    "            (Adapted from <a herf=\"https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract\">Rodríguez et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "**Source Density Predicition**\n",
    "\n",
    "- [Xu et al. 2023, ApJ preprint](https://ui.adsabs.harvard.edu/abs/2023arXiv230401670X/abstract) $-$ get a census of Giant Molecular Clouds (GMCs) from their $N_H$ column-density images\n",
    "    \n",
    "    - **training**: 7179 high-res MHD simulations of clouds\n",
    "    - **architecture**: Diffusion Models\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/Xu_Fig2.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.3.j. The true GMC number density (<i>bottom-left</i>) is iteratively reconstructed, conditioned on the line-of-sight $N_H$ column density (<i>top-left</i>) .<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract\">Rodríguez et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-thread",
   "metadata": {},
   "source": [
    "# Neural Networks (NN) Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-compromise",
   "metadata": {},
   "source": [
    "## Generic Architecture and Neurons\n",
    "<a id='Generic_Architecture_and_Neurons'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-methodology",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Generic_Architecture.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 11.  A simple, generic <i>feedforward</i> deep neural architecture.  Neurons of a layers might be connected to al the neurons of the neighboring layers, like in this example (<i>fully-connected</i> layers), or not.<br>\n",
    "            (Adapted from <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-arbor",
   "metadata": {},
   "source": [
    "<font size=3><u>**Nomenclature**</u><font>\n",
    "    \n",
    "**Neuron**: A simple element in a network, carrying 1 value.\n",
    "    \n",
    "**Layers**: A collection of neurons activated simulataneusly.<br>\n",
    "    \n",
    "    Layers are represented diffrently depending on the architecture.\n",
    "    E.g., fully-connected layers (as in the Figure above), appear as vertical stripes of neurons.\n",
    "  \n",
    "- **input layer**: the data\n",
    "- **hidden layers**: the internal layers (\"_hidden_\" from the point of view of the NN user)\n",
    "- **output layer**: the variable(s) of interest (e.g., class(es) or $y$)\n",
    "    \n",
    "    \n",
    "    E.g., if we provide an image as input, each pixel is 1 neuron of the input layer.\n",
    "\n",
    "\n",
    "Contemporary NNs contain hundreds to thousands of layers, with million to billion of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-munich",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-relations",
   "metadata": {},
   "source": [
    "The core of the functioning of any NN is how the **information flows** through a neuron.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Weights_and_Biases.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 12.  How the information is propagated through a neuron. Don't get confused with $\\hat{y}$: in this image, it only represents the neuron's output, not the target variabe (e.g. the <i>class</i>)<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "1. **The first stage is <u>linear</u>:**<br>\n",
    "    A neuron takes all the inputs (values) x$_i$ directed into it, multiplies each of them by a different _weight_ ($w_i$), and takes the sum.<br>\n",
    "    Then, it adds a _bias_ ($b$).\n",
    "<br>\n",
    "\n",
    "2. **The second stage is (usually) <u>non-linear</u>:**<br>\n",
    "    The summation is passed to an **activation function**.<br>\n",
    "    The activation function acts as a filter, basically deciding when and how the information shall flow. \n",
    "\n",
    "The neuron output is therefore:\n",
    "\n",
    "   $$ \\hat{y} = f(\\sum{\\textbf{w}\\cdot{}\\textbf{x} + b}) $$\n",
    "\n",
    "<u>**Important**</u>\n",
    "\n",
    "The _weights_ and _biases_ are <u>the</u> elements that are fit during the training of the model! \n",
    "\n",
    "Fitting a model == optimizing **all** the _weights_ and _biases_ within the NN, in order to **approximate** the desired output $y$ given a corresponding example $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-poison",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-might",
   "metadata": {},
   "source": [
    "Activation functions are what make NNs so **efficient** as universal tools.\n",
    "\n",
    "The introduce <u>non-linearities</u> $\\rightarrow$ a NN can create an arbitrarily complex model.\n",
    "\n",
    "They can be basically **any** _filter_-like function, but they better posses some features:\n",
    "\n",
    "- **computationally inexpensive** $\\leftarrow$ hence simple, since they get executed at each neuron\n",
    "\n",
    "- **zero-centered** $\\leftarrow$ not to shift values towards a preferential direction\n",
    "\n",
    "- **differentiable** $\\leftarrow$ because NNs work with [Backpropagation](#Backpropagation)\n",
    "\n",
    "- **avoid vanishing when chained** $\\leftarrow$ more correctly, we need to avoid vanishing gradients<br>\n",
    "  (see [Gradient Descent and Loss](#Gradient-Descent-and-Loss))\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Activation_Functions.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 13.  A collection of commonly used activation functions<br>\n",
    "            (Adapted from <a href=\"https://wandb.ai/lavanyashukla/vega-plots/reports/Natural-Language-Processing--Vmlldzo2Nzk2Ng\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-evidence",
   "metadata": {},
   "source": [
    "## NN Architecture Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-vacation",
   "metadata": {},
   "source": [
    "NN come in **countless architectures**, and even trying to classify them is a tough task ...\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/NN_Zoo.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 14.  A comprehensive scheme of NN architectures.<br>\n",
    "            (Image credit: <a href=\"https://www.asimovinstitute.org/author/fjodorvanveen/\">Fjodor van Venn</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Morpheus.jpg\">\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-scholar",
   "metadata": {},
   "source": [
    "# Training NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-shannon",
   "metadata": {},
   "source": [
    "As mentioned above:\n",
    "\n",
    "    training a NN = optimize its weights and biases\n",
    "    \n",
    "in order to produce the desired output (class or values) given a corresponding input.\n",
    "\n",
    "- - -\n",
    "\n",
    "The details of the training method depends on the DL learning problem:\n",
    "\n",
    "- **supervised:** we have labelled examples \n",
    "- **unsupervised:** no labeles are available \n",
    "- **reinforced:** the examples are associated to a \"reward\" \n",
    "\n",
    "We will focus on the **supervised** case to illustrate the NN mechanics:<br>\n",
    "$\\rightarrow$ Let's assume we have some predicting variables $X$, and labels $y$.\n",
    "\n",
    "- - -\n",
    "\n",
    "How can we _tell_ the network in which way it shall modify weights (and biases)?<br>\n",
    "Let's break down the NN rationale:\n",
    "\n",
    "1. We initialize the weights to some arbitrary value\n",
    "2. We take a sub-sample (batch) of the data $X$, $X_{batch}$ \n",
    "3. We propagate $X_{batch}$ through the network, obtaining a predicted $\\hat{y}_{batch}$\n",
    "4. We assess the **error** between $\\hat{y}_{batch}$ and the true $y_{batch}$\n",
    "5. We need to **backpropagate** back the information about the difference\n",
    "6. We need to **update** the weights in the right direction\n",
    "7. Repeat from step 2 untill all data are used\n",
    "\n",
    "The critical steps are #4, #5 and #6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-quebec",
   "metadata": {},
   "source": [
    "## Error Function, Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-municipality",
   "metadata": {},
   "source": [
    "Let's consider an **errorr** (a.k.a. **loss**, or **cost**) function:\n",
    "\n",
    "$$ E (\\hat{y}, y) $$\n",
    "\n",
    "which assesses the intensity of the error.  For _example_, we might adopt: $E (\\hat{y}, y) = {1\\over2}(\\hat{y} - y)^2 $.\n",
    "\n",
    "To be precise, $\\hat{y}$ is itself a function of the input **x**, and of the NN parameters $\\theta$ (all the $w$ and $b$ of each node):\n",
    "\n",
    "$$ \\hat{y} = f(\\textbf{x}, \\theta) $$\n",
    "\n",
    "therefore:\n",
    "\n",
    "$$ E = E (x, y, \\theta) $$\n",
    "\n",
    "- - -\n",
    "\n",
    "The **gradient** of $E$ with respect to $\\theta$, i.e. $\\nabla E (x, y, \\theta)$, is a vector _roughly_ pointing towards the minimum of $E$:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Gradient_Descent.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 15.  Iterative update of a weight via the Gradient Descent method.<br>\n",
    "            (From <a href=\"https://ekamperi.github.io/machine%20learning/2019/07/28/gradient-descent.html\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "We can therefore **update** the weights by adding a vector proportional to $\\nabla E$:\n",
    "\n",
    "$$ \\theta^\\prime = \\theta - \\eta \\cdot \\nabla E (x, y, \\theta) ~~~~~(1)$$\n",
    "\n",
    "where the proportionality constant $\\eta$ is called **learning rate** because it regulates how fast we shall proceed over the mininum (and possibly _overshooting_ it, if $\\alpha$ is too large).\n",
    "\n",
    "This is the **Gradient Descent** method.\n",
    "\n",
    "- - - \n",
    "\n",
    "So we need to calculate $\\nabla E~$:\n",
    "\n",
    "$$ \\nabla E (x, y, \\theta) =\n",
    "      \\left( \\frac{\\partial E}{\\partial \\theta_1},\n",
    "             \\frac{\\partial E}{\\partial \\theta_2},\n",
    "             . . .,\n",
    "             \\frac{\\partial E}{\\partial \\theta_n}\n",
    "\\right)$$\n",
    "\n",
    "But what are the elements $\\frac{\\partial E}{\\partial \\theta_i}$?\n",
    "\n",
    "Let's start from the output neuron, and consider only the parameter $i = 1$, i.e. $\\theta_i = w_1$.<br>\n",
    "We can _re_-write $\\frac{\\partial E}{\\partial \\theta_1}$ as:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\theta_1} = \n",
    "  \\frac{\\partial E}{\\partial w_1} = \n",
    "    \\frac{\\partial E}{\\partial \\hat{y}} \\cdot\n",
    "    \\frac{\\partial \\hat{y}}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "where we applied the **chain rule** for derivatives.\n",
    "\n",
    "We have the ingredients to calculate the two components because we saw above their functional form:\n",
    "\n",
    "- $ \\hat{y} = f(\\sum{\\textbf{w}\\cdot{}\\textbf{x} + b}) $\n",
    "\n",
    "- $E (\\hat{y}, y) = {1\\over2}(\\hat{y} - y)^2 $.\n",
    "\n",
    "You can imagine **propagating** the chain rule back to the original input: all the steps are either _linear_ or passing through a _differentiable_ activation function!\n",
    "\n",
    "\n",
    "A visual explanation of the **Backpropagation Algorithm** is given in this\n",
    "\n",
    "> [Google Developers webpage](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-discovery",
   "metadata": {},
   "source": [
    "<font size=3><u>**Stochastic Gradient Descent**</u><font>\n",
    "\n",
    "The Gradient Descent (GD) presented above is the \"**minibatch GD**\", where the gradient is calculated as the average gradient over the **batch** of data, $X_{batch}$.\n",
    "\n",
    "$$ \\nabla E (x, y, \\theta) = {1 \\over N}\\sum_i^{N_{batch}} \\nabla E (x_i, y_i, \\theta) $$\n",
    "\n",
    "_NOTE: Confusingly enough, the \"**batch GD**\" is the one using <u>all</u> data at once._\n",
    "    \n",
    "It is arguably the most used technique, and the minibatch helps reducing the large number of calculations involved in Backprop.\n",
    "\n",
    "    \n",
    "However in some cases, especially with large datasets, it might be convenient to use an **estimate** of the Gradient, e.g. with a **Stochastic Gradient Descent (SGD)**.\n",
    "    \n",
    "In SGD, we:\n",
    "1. Propagate 1 example through the NN\n",
    "2. Calculate its gradient and update the weigths\n",
    "3. Repeat for all the training examples\n",
    "\n",
    "**PROs**:\n",
    "- starts converging earlier because of more frequent updates $\\rightarrow$ quick insights\n",
    "- stochasticity can help avoiding local minima\n",
    "    \n",
    "**CONs**:\n",
    "- the Error function oscillates more    \n",
    "- reaches a sub-optimal minimum compared to batch GD\n",
    "- it is slower because it is sequential (cannot be parallelized)\n",
    "\n",
    "    \n",
    "Read more on [this post](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/).\n",
    "    \n",
    "_NOTE: To add confusion, it is common to call the minibatch GD as \"SGD\", since in practice it applies the stochasticity by picking minibatches.  In the remainder we will subtend the same nomenclature._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-meditation",
   "metadata": {},
   "source": [
    "## Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-difference",
   "metadata": {},
   "source": [
    "A NN model can easily contain millions of parameters.\n",
    "\n",
    "How can we efficiently **explore** the parameter space, to find the global minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-peace",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "This method takes in account the **previous values** of the gradient.\n",
    "\n",
    "It is particularly useful when exploring _monotonic_ parameter spaces, because it can skip faster through large areas of the landscape.\n",
    "    \n",
    "The parameter update is a variation Equation 1:\n",
    "    \n",
    "$$ v^\\prime = \\alpha v  - \\eta \\cdot \\nabla E (x, y, \\theta) $$\n",
    "\n",
    "$$ \\theta^\\prime = \\theta + v $$\n",
    "\n",
    "- $v$ represents the current \"velocity\" of the gradient.<br>\n",
    "- The larger $\\alpha\\in$ [0, 1], the more previous steps are taken into account.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1000>\n",
    "        <img src=\"images/Momentum.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 16.  Representation of the influence of Momentum on the GD.<br>\n",
    "            (From <a href=\"https://mlfromscratch.com/optimizers-explained/#/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-wheel",
   "metadata": {},
   "source": [
    "### Adaptive learning rates\n",
    "\n",
    "There are multiple alternatives to SGD, here we list a few famous ones.<br>\n",
    "\n",
    "Their differ on how they dynamically **adjust** the **Learning Rate** (**LR**; $\\eta$), which is otherwise a tricky hyperparameter to calibrate.\n",
    "\n",
    "<font size=3><u>**AdaGrad**</u><font>\n",
    "\n",
    "AdaGrad **separately** scales the LR for each parameter (i.e. for each \"direction\" in the parameter space).<br>\n",
    "$\\rightarrow$ Each direction gets a \"personalized\" search.\n",
    "\n",
    "- Scaling is performed inversely to the **sum of the squares** of _all_ the historical gradients.\n",
    "- Directions with larger gradient see their LR decrease faster.\n",
    "    \n",
    "\n",
    "<font size=3><u>**RMSProp**</u><font>\n",
    "\n",
    "Same concept as AdaGrad, but it scales with an **exponentially weighted moving average** of previous gradients (decay of importance).<br> (_Instead of using the plain sum, as for AdaGrad_).\n",
    "\n",
    "\n",
    "<font size=3><u>**Adam**</u><font>\n",
    "\n",
    "Adam (i.e., Adaptive Moments) is sort of **RMSProp + momentum**.\n",
    "\n",
    "But it uses both the **first** and the **second** moment of past gradients.\n",
    "\n",
    "- - -\n",
    "\n",
    "_NOTE: Don't confuse \"moment\" of a quantity with \"momentum\":_\n",
    "    \n",
    "$$ moment\\_n = E~[X^n]$$\n",
    "\n",
    "- _moment_1 = **average**_\n",
    "- _moment_2 = **uncentered variance** (variance without centering around the mean)_\n",
    "   \n",
    "_In other words, RMSProp  uses the first moment, just with $E[]$ being an exponentially-weighted average._\n",
    "    \n",
    "_**Adam also** makes use of the second moments of the historical gradients._\n",
    "\n",
    "- - -\n",
    "    \n",
    "**TL;DR:** Adam is arguably the **best-performing** optimizer on average and quite **robust** with respect to the choice of hyperpars.      \n",
    "\n",
    "$\\rightarrow$ Safe choice!\n",
    "    \n",
    "    \n",
    "<font size=3><u>**Reading material**</u><font>\n",
    "    \n",
    "At this [page](https://mlfromscratch.com/optimizers-explained/#/) you will find graphical explanations of these optimizers as well as their mathematical formulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-phone",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "In DL, it is common to cycle the training on the same data $N$ times $-$ Each pass is called \"**Epoch**\".\n",
    "\n",
    "Training rationale with **N_epochs** epochs, and **N_batches** batches:\n",
    "\n",
    "    0. set epoch = 0\n",
    "    1. Train on batch 1, update gradient\n",
    "       Continue train on batch 2, update gradient\n",
    "       Continue train on batch 3, update gradient\n",
    "       .. .\n",
    "       Continue train on batch N_batches, update gradient\n",
    "    3. epoch += 1\n",
    "       (epoch completed)\n",
    "    4. if epoch <= N_epochs: continue from 1 else stop\n",
    "    \n",
    "- - -\n",
    "\n",
    "<u>NOTE:</u>\n",
    "_This introduces bias towards the training data, but it usually more than compensated by a longer gradient descent._\n",
    "\n",
    "- - -\n",
    "\n",
    "We can display the values of the Loss as a function of  \"**time**\" $\\rightarrow$ **Learning Curves** (**Validation Curves**)\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 16.  Learning curves for train and validation set.<br>\n",
    "            (From <a href=\"https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "We can use them to spot **overfitting** (train goes much better than validation) or **underfitting** (training shows trend to to better):\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Good.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 17.A.  <b>A good fit</b>.<br>\n",
    "            Training and validation converge <u>and</u> they flatten.<br>\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Underfitting.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 17.B.  <b>Underfitting</b>.<br>\n",
    "            The training set shows a downward trend at the right edge: maybe a few more epochs could yield the best performance.<br>\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Learning_Curves_Overfitting.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 17.C.  <b>Overfitting</b>.<br>\n",
    "            The validation curve cannot keep up with the training curve.\n",
    "            (From <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">here</a>) \n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-purple",
   "metadata": {},
   "source": [
    "# What you will see in the Workshops\n",
    "\n",
    "You will explore 2 domains of DL networks:\n",
    "\n",
    "- **Supervised Learning - Regression** $~~~~\\rightarrow$ **Fully-connected Layers**\n",
    "\n",
    "- **Supervised Learning - Classification** $~\\rightarrow$ **Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "- **Unsupervised Learning - Generation**  $~\\rightarrow$ **Diffusion Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e9e4e",
   "metadata": {},
   "source": [
    "## Supervised Learning: CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746c292",
   "metadata": {},
   "source": [
    "**CNNs** excel at classifying images thanks to their **Convolutional Layers**:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1000>\n",
    "        <img src=\"images/CNN_Architecture.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 18.  A prototypical CNN architecture.<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- <font color='darkred'>**Convolution**</font> $~\\rightarrow$ Filters that scan the image to detect different features\n",
    "- <font color='darkgreen'>**Pooling**</font> $~~~~~~~~\\rightarrow$ Reduce dimensionality to increase abstraction\n",
    "- <font color='darkblue'>**Flattening**</font> $~~~~\\rightarrow$ Encodes features into variables\n",
    "- <font color='purple'>**Dense Layers**</font> $\\rightarrow$ Feature classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-meter",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "CNNs require lots of data $\\rightarrow$  **Transfer Learning** helps to address this issue:\n",
    "\n",
    "    1. Pre-train the convolutional/pooling with any extended image dataset\n",
    "    2. Freeze those parameters\n",
    "    3. Fit the classifier part on your astro images\n",
    "\n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/CNN_Transfer.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 19.  Don't have enough data?  Just add cats and dogs, SMH ...<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "> The pre-training will teach the CNN to **recognize features** in images (shapes, edges, etc.).\n",
    "\n",
    "See [Ackerman et al. 2017, MNRAS, 479, 415](https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract) $-$ identify mergers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-wrong",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Autoencoders\n",
    "\n",
    "**Autoencoders** allow to _consistently_ **encode** data into a lower-dimensional space $\\rightarrow$ can **summarize** data properties.\n",
    "\n",
    "They are \"consistent\" because they learn to **match** an _input_ and its _reconstruction_ from the embedding space.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Autoencoder.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.  A prototypical Aoutoencoder architecture.\n",
    "            <br>\n",
    "            (From <a href=\"https://www.assemblyai.com/blog/introduction-to-variational-autoencoders-using-keras/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-weekly",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Dinosaurs.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "There are several libraries that implement **Deep Learning routines** in Python.\n",
    "\n",
    "For the Woskshop we will focus on [**Tensorflow**](https://www.tensorflow.org/) and [**Keras**](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-watch",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Tensorflow_Logo.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "**Tensorflow** the _heavy lifting_ for us:\n",
    "- pre-defined **functions** and **layers**\n",
    "- automatically computes the **derivatives** (for Backprop!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-press",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "**Keras** is an **API (Application Programming Interface)** $-$ it supports multiple libraries, e.g.:\n",
    "- Tensorflow (Google)\n",
    "- CNTK (Microsoft)\n",
    "- MXNet (Apache)\n",
    "- Theano\n",
    "\n",
    "It provides _high-level_ functionalities built on e.g. Tensorflow.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Keras.png\">\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69502564",
   "metadata": {},
   "source": [
    "You can see it imported in different ways, because it is **mantained separately** by both Tensorflow and Keras (functionalities _might_ differ slightly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: Anaconda -> Tensorflow -> Keras\n",
    "\n",
    "import keras.models\n",
    "# and\n",
    "from keras import backend as K\n",
    "# use Keras repository code\n",
    "\n",
    "# .. OR\n",
    "\n",
    "import tensorflow.keras\n",
    "# use TensorFlow repository code itself (recommended: better maintaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ae999",
   "metadata": {},
   "source": [
    "It can be coded in 2 \"_styles_\":\n",
    "\n",
    "<u>Sequential API</u>\n",
    "\n",
    "  >**PROs:** Simpler (stack of layers)<br>\n",
    "  >**CONs:** Single-input, single-output\n",
    "\n",
    "<u>Functional API</u>\n",
    "\n",
    "  > **PROs:** More flexible, multi-input, multi-output<br>\n",
    "  > **CONs:** Steep learning curve, need to understand tensor programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c7cc2",
   "metadata": {},
   "source": [
    "### Keras Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "#model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa11e16",
   "metadata": {},
   "source": [
    "### Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03682252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "inputs = keras.Input(shape=(10,))\n",
    "x = layers.Dense(20, activation='relu')(inputs)\n",
    "x = layers.Dense(20, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "#model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
