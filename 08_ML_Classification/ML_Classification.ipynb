{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**ML Classification**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we focus on another important application for machine learning, the **Classification**. <br>\n",
    "We will provide a simple problem (with multiple **intermediate tasks**):\n",
    "\n",
    "- to demonstrate basic **visualization** of the data \n",
    "- to show the basic steps in an **application** of an algorith (*or two!*)\n",
    "- to present the necessary performance **metrics**\n",
    "- to investigate differences in **binary and multiclass** approaches\n",
    "\n",
    "$\\rightarrow$ The target is to have an practical application example of an algorith to <u>understand the approach</u> and (hopefully!) use it for your own research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Types of Classification\n",
    "\n",
    "Let's go back and check a plot from the previous session (on Clustering).\n",
    "\n",
    "<center><img src=\"images/example_of_classes-clustering.png\" width=650> \n",
    "Figure 1.1. A clustering result from the previous session. <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**: What do you notice here ?_\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "If - for some unclassified objects - we managed to get measurements of these two lines that we are able to <b>assign</b> a class to them. Therefore, we can use clustering as a <b>classification</b> approach!\n",
    "    \n",
    "In Bayesian terminology, we can use class as a prior!\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flavors\n",
    "\n",
    "Classification can come in two major flavors, based on the type of intervention by the user:\n",
    "\n",
    "- **Unsupervised**: The classification is defined \"unsupervised\" when the user does not provide labels during the training process and the machine learns the definition of each class from the data. This is exactly the case of \"Clustering\" that we saw in the previous session.  \n",
    "\n",
    ">    _In practice, the machine learns to cluster objects with similar properties._\n",
    "\n",
    "\n",
    "- **Supervised**: The classification is defined \"supervised\" when the user provides a label for each object in the training set. In this case, the idea is that we can train the model to associate the label with some given characteristics of the training data.\n",
    "\n",
    ">    _In practice, the machine learns to find similarities between objects with the same label._\n",
    "\n",
    "## Terminology\n",
    "\n",
    "Keep in mind that:\n",
    "\n",
    "- **class** or **label** are used interchangeably\n",
    "- **samples** (i.e. the items) can be of any arbitrary type (e.g. a string, an image, or numbers) - however, in astronomy we mainly refer to **objects or sources**, since samples correspond to their collections <br>(so... samples in ml terminology and objects/sources in astronomy!) \n",
    "- the objects/samples are categorized based on their **properties** or **features** (the actual information that we pass to the machine, e.g. the pixel intensities in the case of an image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The sample for our classification example\n",
    "\n",
    "To properly classify a star we need a spectrum. However, this is time consuming and limited to one (single slit spectroscopy) or a few tens of sources (multi-object spectroscopy). On the contrary, imaging in various different filters can be done easily and for thousands of sources per image. Photometry at different wavelengths result in a very low-resolution \"spectrum\".</br> \n",
    "\n",
    "<center><img src=\"images/Girardi2002-photometric systems.gif\"> \n",
    "Figure 2.1. The filter+detector transmission curves for a number of different systems, along with indicative spectra of Vega, the Sun, and a M5 giant. <br>\n",
    "(Fig 3. from <a href=\"https://ui.adsabs.harvard.edu/abs/2002A%26A...391..195G/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Girardi et al. (2002)</a>)</center>\n",
    "\n",
    "In this example we are using a set of photometric measurements (from optical to mid-IR bands) for a sample of massive evolved stars in the Large Magellanic Cloud (based on these works: [Bonanos et al. (2009) AJ, 138, 1003](https://ui.adsabs.harvard.edu/abs/2009AJ....138.1003B/abstract), [Neugent et al. (2012), ApJ, 749, 177](https://ui.adsabs.harvard.edu/abs/2012ApJ...749..177N/abstract), and [Davies, Crowther & Beasor (2018), MNRAS, 478, 313](https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.3138D/abstract)). \n",
    "\n",
    "<center><img src=\"images/Massey2013-HRD.png\" width=600> \n",
    "Figure 2.2. A slightly modified version of Fig. 1 from <a href=\"https://ui.adsabs.harvard.edu/abs/2013NewAR..57...14M/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Massey et al. (2013)</a>)</center>\n",
    "\n",
    "Our aim is to use a method that will help us **distinguish different classes** of objects. For our purposes we will use OBA stars (main-sequense objects), OBAe (a subcategory of OBA stars with circumstellar disks and emission lines), Wolf-Rayet stars (hot evolved stars with strong stellar winds that actually strip their envelopes), Yellow and Red supergiants (evolved massive stars). For convenience we will use OBA, OBAe, WR, YSG, and RSG, respectively, as labels.\n",
    "\n",
    "&#9733; A similar, but more elaborated, implementation is performed in [Maravelias et al. (2022), A&A, 666, 122](https://ui.adsabs.harvard.edu/abs/2022A%26A...666A.122M/abstract).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9755; You do not need ```pandas``` to do everything with files! ```genfromtxt``` is very powerful but has its tweaks! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = \"data/LMC_phot_data.csv\"\n",
    "miss_value = -999.0  # when entries are missing\n",
    "\n",
    "data = np.genfromtxt(dfile, dtype=None, encoding='utf8', \n",
    "                     comments='#', delimiter=',', \n",
    "                     filling_values = miss_value, \n",
    "                     names=True, autostrip='Yes')\n",
    "\n",
    "#examine data\n",
    "print(\"Let us see what we have:\\n\")\n",
    "print(\"The column names:\")\n",
    "print(data.dtype.names)\n",
    "print(\"-\"*25)\n",
    "print(\"Let's print the spectral types only:\")\n",
    "print(data['SpT'])\n",
    "print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT_: We want to group the different spectral classes found, and for this we are using the indeces and not the data values directly. in that way we can select all data for the same objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "classes = defaultdict(list)\n",
    "\n",
    "for i in range(0,len(data['SpT'])): \n",
    "#    print(i, data['SpT'][i])\n",
    "    classes[data['SpT'][i]].append(i)\n",
    "\n",
    "#print(classes)\n",
    "unique_cls = sorted(set(classes.keys()))\n",
    "print(unique_cls)\n",
    "print(\"> SUMMARY of loaded data:\")\n",
    "print(\"=========================\")\n",
    "for sptype in unique_cls:\n",
    "    number = len(classes[sptype])\n",
    "    print(f\"{sptype:-<6s}--> {number:>3} stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [b for b in data.dtype.names[3:-1] if 'e_' not in b]\n",
    "\n",
    "def reminder():\n",
    "    \"\"\" \n",
    "    A simple function to print all bands\n",
    "    and classes available.\n",
    "    \"\"\"\n",
    "    print('Available bands to use: ')\n",
    "    print(','.join(bands))\n",
    "    print('-'*25)\n",
    "    print('Available classes to use:')\n",
    "    print(','.join(unique_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, Column\n",
    "\n",
    "print(f'Available photometry for: {\", \".join(bands)}')     \n",
    "\n",
    "# Constructing the table for the statistics \n",
    "phot_data_col_names = ['Class', 'All'] + [bb for bb in bands]\n",
    "phot_data_per = Table( names = phot_data_col_names, dtype = ['S3']+['i4']+['f2']*(len(bands)))\n",
    "\n",
    "for spt in unique_cls:\n",
    "    indcs = classes[spt]\n",
    "    starsWbands = defaultdict(list) # keep those with measurements across all\n",
    "    for star in indcs:\n",
    "#        print(spt, star)\n",
    "        for bnd in bands:\n",
    "            mag = data[star][bnd]\n",
    "#            print(mag)\n",
    "            if mag!=miss_value:\n",
    "                starsWbands[bnd].append(star)    \n",
    "    row_data_per = [spt, len(indcs)] + [(len(starsWbands[bb])/len(indcs))*100 for bb in bands]\n",
    "    phot_data_per.add_row ( row_data_per )\n",
    "    \n",
    "print(\"\\nNumber of stars per band (in %)\\n\")\n",
    "phot_data_per    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9755; 36, 45, 58, 89, 24 corresponds to the 3.6um, 4.5um, 5.8um, 8.9um, 24um bands of *Spitzer*. One common representation of these bands is [3.6], [4.5], [5.8], [8.0], [24] but the use of \"[]\" and \".\" is inconvenient, so we drop them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**: What do you notice here ?_\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "Many sources come without measurements across all filters. This is a particular problem for algortihms that need values to work with. \n",
    "    \n",
    "This problem of <b>\"missing values\"</b> is actually the norm than the exception. There are ways to tackle this, e.g. by taking the mean values of the corresponding filters or more elaborated techinques. Check <a href=\"https://scikit-learn.org/stable/modules/impute.html\" target=\"_blank\" rel=\"noopener noreferrer\">sklearn's imputation of missing values</a> for more.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data - select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selmags( band1, band2, cls):\n",
    "    \"\"\"\n",
    "    Function to select sources of a specific\n",
    "    spectral class (cls) and return the magnitudes\n",
    "    that correspond to bands 1 and 2.\n",
    "        \"\"\"\n",
    "    # all indeces of the particular class\n",
    "    cls_indcs = np.asarray( classes[cls] ) \n",
    "    # selecting those indeces of the class\n",
    "    # that do not contain missing values, ie -999\n",
    "    sel_cls_indcs = np.where( (data[band1][cls_indcs]!=miss_value)\n",
    "                        & (data[band2][cls_indcs]!=miss_value) )[0]\n",
    "\n",
    "    sel_indcs = cls_indcs[sel_cls_indcs]\n",
    "    rem_indcs = len(cls_indcs)-len(sel_indcs)\n",
    "    print(f'-- {cls}: excluding {rem_indcs} out of {len(cls_indcs)} sources ({rem_indcs/len(cls_indcs)*100:.1f}%)')\n",
    "    mag1, mag2 = data[band1][sel_indcs], data[band2][sel_indcs]    \n",
    "    \n",
    "    return mag1, mag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "\n",
    "selected_spt = unique_cls              # if you want to print all\n",
    "#selected_spt = ['RSG', 'OBA', 'WR']     # put your selection here\n",
    "\n",
    "# plot 1\n",
    "band1_1 = ''\n",
    "band1_2 = ''\n",
    "\n",
    "# plot 2\n",
    "band2_1 = ''\n",
    "band2_2 = ''\n",
    "\n",
    "print('- plot1:')\n",
    "for s in selected_spt: \n",
    "    plt1 = selmags( band1_1, band1_2, s)\n",
    "\n",
    "    ax[0,0].plot(plt1[0], plt1[1], 'o', label=f'{s}: {len(plt1[0])}')\n",
    "    ax[0,0].set_xlabel(f'{band1_1}') #'-{band1_2}')\n",
    "    ax[0,0].set_ylabel(band1_2)\n",
    "    ax[0,0].invert_yaxis()\n",
    "    ax[0,0].invert_xaxis()\n",
    "    ax[0,0].legend()\n",
    "\n",
    "    ax[0,1].plot(plt1[0]-plt1[1], plt1[1], 'o', label=f'{s}: {len(plt1[0])}')\n",
    "    ax[0,1].set_xlabel(f'{band1_1}-{band1_2}')\n",
    "    ax[0,1].set_ylabel(band1_2)\n",
    "    ax[0,1].invert_yaxis()\n",
    "    ax[0,1].legend()\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('- plot2:')        \n",
    "for s in selected_spt:\n",
    "    plt2 = selmags( band2_1, band2_2, s)\n",
    "    ax[1,0].plot(plt2[0], plt2[1], 'o', label=f'{s}: {len(plt2[0])}')\n",
    "    ax[1,0].set_xlabel(f'{band2_1}') #'-{band2_2}')\n",
    "    ax[1,0].set_ylabel(band2_2)\n",
    "    ax[1,0].invert_yaxis()\n",
    "    ax[1,0].invert_xaxis()    \n",
    "    ax[1,0].legend()\n",
    "    \n",
    "    ax[1,1].plot(plt2[0]-plt2[1], plt2[1], 'o', label=f'{s}: {len(plt2[0])}')\n",
    "    ax[1,1].set_xlabel(f'{band2_1}-{band2_2}')\n",
    "    ax[1,1].set_ylabel(band2_2)\n",
    "    ax[1,1].invert_yaxis()\n",
    "    ax[1,1].legend()\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time: Experiment with various combinations and try to answer\n",
    "\n",
    "**1. What happens if you start increasing the number of classes to consider ?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "The task becomes more difficult as classes start to overlap. Some combinations may prove better to separate some classes but there is not a single optimal one.      \n",
    "</details>\n",
    "    \n",
    "**2. How the selection of bands (e.g. adding) influence the objects to keep ?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "In order to plot a CMD we need sources with measuremets in both bans. By enforcing this we actually exclude objects with missing data. That results in decreasing number of sources.\n",
    "</details>\n",
    "\n",
    "**3. Would you prefer to use  combinations with few or more objects ?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "In principle we want to use all available information from all sources, so that the algorithm will \"learn\" better the properties of each class. Hence we would like to maximize the number of sources used. In problems with already small numbers (like in this example) this becomes even more critical (because we do not want to leave out objetcs, i.e. information).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><center>**What have we achieved so far?**</center></font>\n",
    "<center>To <i>examine our data</i> and <i>select which features</i> to use. <br>The time has come to <i>select an algorithm</i>! </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Support Vector Machine (SVM) algorithm\n",
    "\n",
    "## The boundary\n",
    "\n",
    "Support vector machine (SVM) is a way of choosing a decision boundary between different classes.\n",
    "\n",
    "The classification boundary is provided by the hyperplane maximizing the distance between the hyperplane itself and the closest point from either class. This distance is called **margin**. Points on the margins are called **support vectors**.\n",
    "\n",
    "\n",
    "<center>\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_1.png\">\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_2.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "    Figure 3.1. Left: Hyperplane (dashed line) separating two classes (_red_ and _green_). Right: The closest points to the hyperplane from each class constitute the \"tip\" of the support vectors.\n",
    "</center>\n",
    "\n",
    "The left panel of Figure 3.1 a shows two different classes distributing in a scatter plot according to variable $x_1$ and $x_2$. The right panel of Figure 3.1 explains the origin of the name support vectors: the closest points _support_ the hyperplanes (solid lines) equally distant from the decision hyperplane (dashed line).\n",
    "\n",
    "Infinite possible boundaries can separate the two classes. SVM algorithms find the one that maximizes the distance between the supported hyperplanes.\n",
    "\n",
    "## Minimizing the loss function - again !\n",
    "\n",
    "The supported hyperplanes (solid-lines in Figure 3.1) can be defined as:\n",
    "\n",
    " $$ w \\cdot x + b = +1 $$\n",
    "\n",
    " $$ w \\cdot x + b = -1 $$\n",
    "\n",
    "where x is the coordinate on the (x1, x2) plane, w is a 2$\\times$1 matrix and b a scalar. It turns out that these hyperplanes are separated by a distance 2 / ||w||. Finding the _ideal classification boundary_, i.e. the one maximizing the distance, is therefore a problem of **minimizing ||w||**\n",
    "\n",
    "$$ min(Loss) = min(||w||) $$.\n",
    "\n",
    "This is what SVM algorithms do.\n",
    "\n",
    "&#9733; For a complete mathematical formulation, consult the [Idiotâ€™s guide to Support vector\n",
    "machines, by Robert Berwick]( http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf).\n",
    "\n",
    "## Separatable classes (or not)\n",
    "\n",
    "We cannot always assume that 2 classes are separable without \"contamination\". That is why SVM algorithms includes a tunable parameter ($C$) which penalizes misclassifications.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/svm-parameter-c-example.png\" width=800> \n",
    "Figure 3.2. The effect of <i>C</i> parameter in the misclassifications.<br>\n",
    "(Credit: <a href=\"https://learnopencv.com/svm-using-scikit-learn-in-python/\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">SVM: What makes it superior to the Maximal-Margin and Support Vector Classifiers?, by Shivam Sharma</a>)\n",
    "    </img>\n",
    "    </div>\n",
    "    \n",
    "- Small $C$ &#8594; wide margin &#8594; allows more misclassification <br>\n",
    "- Large $C$ &#8594; narrow margin &#8594; allows less misclassification    \n",
    "\n",
    "However, the SVM finds the hyperplane that maximizes the margin, and indirectly minimizes the misclassifications. In other words, SVM is not designed to minimize the contamination _per se_.\n",
    "\n",
    "\n",
    "## Multiple classes\n",
    "\n",
    "The SVM method can be applied for multiple classes as well.\n",
    "\n",
    "<center><img src=\"images/svm_many_classes.png\" width=400> \n",
    "Figure 3.3. SVM applied to 3 different classes.<br>\n",
    "</center>\n",
    "\n",
    "## Multiple dimensions\n",
    "\n",
    "If our sample characterized by three parameters (X, Y, Z), then the scatter plot has 3 dimensions. The boundary between the classes in the 3-D plot is a plane. Because of the fact that the method can be extrapolated at N-dimensions, the boundary is a *hyperplane*.\n",
    "\n",
    "<img src=\"images/svm_3d.png\" width=400>\n",
    "<center>\n",
    "    Figure 3.4: Support vector machine applied for 3-D features and three classes.\n",
    "</center>\n",
    "\n",
    "## Non-linear boundaries\n",
    "\n",
    "Sometimes, linear boundaries may not be optimal and a non-linear SVM should be used instead. The left panel of Figure 3.5 shows an 2D scatter plot of two different classes (e.g. red and green stars with different radii and temperatures) which cannot be linearly separated.\n",
    "\n",
    "In order to find non-linear boundaries we can tackle the problem in an higher dimensional space. We use a process called **kernelization**, which consists in using a kernel function to attribute to our data a value in the additional dimension. Then, we draw the decision hyperplane into this higher dimensional space.\n",
    "\n",
    "The central panel of Figure 3.5 shows that once the 2D data are mapped to a 3D space by attributing a $z$ value through a Gaussian-like function, the classes are easily separable by a 3D hyperplane. Projecting back the plane in 2D, we obtain the non-linear boundary (Figure 3.5, rght panel).\n",
    "\n",
    "<img src=\"images/kernel.png\" width=800>\n",
    "<center>\n",
    "    Figure 3.5. When no linear boundaries can be used the SVM method can be applied by using kernel.\n",
    "</center>\n",
    "\n",
    "## Choosing the kernel function\n",
    "\n",
    "Useful kernel functions shall satisfy specific conditions, so that in practice only a few are used. In the example of Figure 3.5, the Gaussian Radial Basis Function is used:\n",
    "\n",
    "$$K(x,y) = e^{-\\gamma(x-y)^2}$$\n",
    "\n",
    "where $\\gamma$ is a hyperparameter which shall be learned (in our example we use an arbitrary value but in principle we should use cross-validation methods).\n",
    "\n",
    "## Final remarks on SVM\n",
    "\n",
    "**Pros**\n",
    "* Good at dealing with high dimensional data\n",
    "* Works well on small data sets\n",
    "\n",
    "**Cons**\n",
    "* Picking the right kernel and parameters can be computationally intensive\n",
    "* It suffers from contamination\n",
    "\n",
    "&#9733; For further information on SVM, consult [Support Vector Machine - Classification, by Saed Sayad](http://www.saedsayad.com/support_vector_machine.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The binary problem\n",
    "\n",
    "In this case we examine a binary classification problem where we select one class (or more that will group into a single one) and the rest classes as contaminants. The purpose is to check if we can separate efficiently these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data( bands2use, binary_classes2use  ):\n",
    "    \"\"\"\n",
    "    Process input data to return arrays \n",
    "    of magnitudes and (consecutive) colors\n",
    "    based on the input bands (band2use).\n",
    "\n",
    "    Option to prepare data for binary classification\n",
    "    if binary_classes2use contains classes or not.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_ml_data_mags = []   # working with magnitudes directly\n",
    "    pd_ml_data_clrs = []   # taking color terms, i.e. mag1-mag2\n",
    "    pd_ml_labels    = []\n",
    "    pd_ml_objects   = []\n",
    "\n",
    "    print(f'# stars with mags in: {\",\".join([bb for bb in bands2use])}')\n",
    "    print(\"=========================\")\n",
    "    print(\"Type    initial    final \")\n",
    "    print(\"-------------------------\")\n",
    "    init = 0 # initial total number of stars (added after each iteration)\n",
    "\n",
    "    for sptype in unique_cls:\n",
    "        indcs = classes[sptype]\n",
    "        kept = []\n",
    "        init += len(indcs)\n",
    "        for star in indcs:\n",
    "            mag_list = list(data[star][bands2use])\n",
    "            # rejecting stars with missing values\n",
    "            if miss_value in mag_list:\n",
    "                #print('REJECTING!!! <',data[star])\n",
    "                continue\n",
    "            else:\n",
    "                # creting the magnitude list\n",
    "                mag = [ i for i in mag_list ] #data[star][bands_selected] ]\n",
    "\n",
    "                # creating the color term (index)\n",
    "                clr = [mag[i]-mag[i+1] for i in range(len(mag)-1)]\n",
    "\n",
    "                pd_ml_data_clrs.append(clr)\n",
    "                pd_ml_data_mags.append(mag)\n",
    "                pd_ml_objects.append(data[star]['Name'])\n",
    "                kept.append(sptype)           \n",
    "\n",
    "                # selecting class(es) to examine for binary classifier\n",
    "                if len(binary_classes2use)!=0:\n",
    "                    if sptype in binary_classes2use:\n",
    "        #                print(f'. keeping {sptype}')\n",
    "                        label_sptype = 'SEL'\n",
    "                    else: \n",
    "        #                print(f'. not considering {sptype}')\n",
    "                        label_sptype = 'CON'\n",
    "                    pd_ml_labels.append(label_sptype)\n",
    "                else:\n",
    "                    pd_ml_labels.append(sptype)\n",
    "\n",
    "        print(f'{sptype:<4}  {len(indcs):>9} {len(kept):>8}')\n",
    "    print('-'*24)\n",
    "    print(f'TOTAL:  {init:>7}  {len(pd_ml_data_mags):>7}') \n",
    "    if len(binary_classes2use)!=0:\n",
    "        print('='*24)\n",
    "        print(f'classifying:  {len(pd_ml_labels)-pd_ml_labels.count(\"CON\"):>10}') \n",
    "        print(f'contaminants:  {pd_ml_labels.count(\"CON\"):>9}') \n",
    "\n",
    "\n",
    "    pd_ml_data_mags = np.asarray(pd_ml_data_mags)\n",
    "    pd_ml_data_clrs = np.asarray(pd_ml_data_clrs)\n",
    "    pd_ml_objects   = np.asarray(pd_ml_objects)\n",
    "    pd_ml_labels    = np.asarray(pd_ml_labels)\n",
    "          \n",
    "    return pd_ml_data_mags, pd_ml_data_clrs, pd_ml_objects, pd_ml_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select here the class(es) you would like to distinguish from the rest, along with the bands to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2keep = ['RSG']\n",
    "# Select the bands you want to use here:\n",
    "bands_selected = [ ... ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected, class2keep)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE_: the process_data() function examines and keeps only the sources with values across all bands. In other words, we remove sources with missing values (according to the bands selected). \n",
    "\n",
    "Let's print the labels to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE_: If you have more than 2 bands selected the following plot will use the first two. Modify accordingly to plot other combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "conts = np.where( ml_labels=='CON' )[0]\n",
    "clasf = np.where( ml_labels!='CON' )[0]\n",
    "\n",
    "plt.plot( ml_data_mags[conts][:,0], ml_data_mags[conts][:,1], 'o', \n",
    "             label='Contaminants')\n",
    "plt.plot( ml_data_mags[clasf][:,0], ml_data_mags[clasf][:,1], '*', \n",
    "             label=f'Selected ({\"+\".join(class2keep)})')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel(f'{bands_selected[0]}') #'-{bands_selected[1]}')\n",
    "plt.ylabel(bands_selected[1])\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing data splitting and the Golden rule\n",
    "\n",
    "In supervised approaches we want to \"teach\" the algorithms what they need to learn, before start the predictions. In this case we want the SVM to identify the common properties of the two sub groups (SEL including all possible classes used, and CON as contaminants). However, if we provide all data the algorithm will learn this \"by heart\" and fit them perfectly (called **overfitting**), and when new data appear will probably misclassify. \n",
    "\n",
    "So, what can we do ? We have to **split the data** and follow the **Golden rule**.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Train_Validation_Test.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.1.  Indicative recipe for splitting a dataset into the analysis sets.<br>\n",
    "            For very large datasets (above 10$^5$ entries), the percentages of validation and test can be substantially smaller.\n",
    "            <br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "What are the different sets for? \n",
    "\n",
    "- **Train** set $~~~~~~\\rightarrow$ **Learn** the model parameters\n",
    "\n",
    "- **Validation** set $\\rightarrow$ Check that the learnt model is not **overfitting/underfitting** the train set\n",
    "\n",
    "- **Test** set $~~~~~~~\\rightarrow$ **Assess** the model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><center>**Golden Rule of Machine Learning**</center></font>\n",
    "<center>Treat the <i>test</i> data as if they come from the <i>future</i>!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's go on and split our data now. \n",
    "\n",
    "_NOTE_: We will only use the train/test split as our purpose now is not to optimize the model (you will see more on that in **ML_Practices**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( ... , ... , \n",
    "                        test_size= ... ) #, random_state=42) \n",
    "\n",
    "print(f'- From {len(ml_objects)} sources:')\n",
    "print(f'   {len(X_train)} (training)')\n",
    "print(f'   {len(X_test)} (test)') \n",
    "print()\n",
    "print(f'Test labels: {y_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "\n",
    "**1. What happens when you repeat the process?**\n",
    "\n",
    "_HINT_: Use Ctrl+Enter to run a cell without continuing to the next one\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "The process is random so every time different objects are selected as train to test. \n",
    "</details>\n",
    "\n",
    "**2. How can we solve this?**\n",
    "\n",
    "_HINT_: Check the online documentation ([train-test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "We can use <code>random_state</code> to have the same split all the time (this is mainly for developing and debugging pusposes, as it helps to reproduce the results).\n",
    "</details>\n",
    "\n",
    "**3. Where does 42 comes from?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "Whaat? You do not know?<br>\n",
    "<ul>\n",
    "    <li> shame on you!\n",
    "    <li> read <i>The Hitchhiker's Guide to the Galaxy</i> by Douglas Adams!!!\n",
    "    </ul>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the classifier to fit our training set (X_train) and predict the classes of the test xamples (X_test). We will print some metrics to check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = SVC(kernel='linear') \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "#print(y_pred)\n",
    "\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**but**... _aaarg, what are all these metrics about ?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation metrics - a mini discussion!\n",
    "\n",
    "There is a number of assessment metrics for the performance of a classifier. We start by introducing the idea of the **confusion matrix**. \n",
    "\n",
    "<center><img src=\"images/confusion_matrix-mod.png\" width=400> \n",
    "Figure 4.2. Confusion matrix for classification.<br>\n",
    "(Credit: <a href=\"https://towardsdatascience.com/precision-vs-recall-386cf9f89488\"  target=\"_blank\" rel=\"noopener noreferrer\">Precision vs Recall by Shruti Saxena</a>)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some metrics:\n",
    "\n",
    "$$ \\rm{Precision} =  \\frac{\\rm{True~Positives}}{\\rm{Actual~Results}} = \\frac{\\rm{True~ Positives}}{\\rm{True~Positives\\,+\\,False~Positives}} $$ \n",
    "\n",
    "$$ \\rm{Recall} = \\frac{\\rm{True~Positives}}{\\rm{Predicted~Results}} = \\frac{\\rm{True~ Positives}}{\\rm{True~Positives + False~Negatives}} $$ \n",
    "\n",
    "$$ \\rm{F1-score} = 2 \\times \\frac{\\rm{Precision}\\times\\rm{Recall}}{\\rm{Precision}+\\rm{Recall}}$$\n",
    "\n",
    "$$ \\rm{Accuracy} = \\frac{\\rm{True~Positives}\\,+\\,\\rm{True~Negatives}}{\\rm{Total}} $$ \n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "Support: number of test objects per class<br><br>\n",
    "Macro avg: averaging the unweighted mean per label<br><br>\n",
    "Weighted avg: averaging the support-weighted mean per label\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "_Note 1_: You may also encounter the terms _sensitivity_ and _specificity_ which corresponds to the recall of the positive and the negative class, respectively, in binary problems. \n",
    "\n",
    "_Note 2_: In astrophysics we use the terms _completeness_ and _contamination_ (see [Classification, by Andy Connolly](http://connolly.github.io/introAstroML/blog/classification.html)):\n",
    "\n",
    "$$ \\rm{completeness} = \\frac{\\rm{True~Positives}}{\\rm{All~real~Positives}} = \\frac{True~Positives}{True~Positives + False~Negatives} = recall$$\n",
    "\n",
    "$$ \\rm{contamination} = \\frac{False~Positives}{All~detected~Positives} = \\frac{False~Positives}{True~Positives + False~Positives}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Question:** What **metric of performance** shall we maximize, to optimize our model parameters?<br>\n",
    "It depends on the **objective of the model**.\n",
    "\n",
    "Example 1: We want to identify best follow-up candidates of GW sources, without wasting precious time on wrong targets\n",
    "\n",
    "    TP: identified as GW, and it is GW\n",
    "    FP: identified as GW, but not GW\n",
    "    TN: not identified as GW, and not GW\n",
    "    FN: not identified as GW, but is GW\n",
    " \n",
    "<details>\n",
    "<summary>Which metric is more important? (click!) </summary>\n",
    "Observing GW candidates means that no time should be spent on FP, so we need to minimize this and <b>maximize Precision</b>.\n",
    "</details>\n",
    "\n",
    "Example 2: We want to create a catalog of OB stars, so we would like to retrieve the whole population\n",
    "\n",
    "    TP: identified as OB, and it is OB\n",
    "    FP: identified as OB, but not OB\n",
    "    TN: not identified as OB, and not ON\n",
    "    FN: not identified as OB, but is OB\n",
    "    \n",
    "<details>\n",
    "<summary>Which metric is more important? (click!) </summary>\n",
    "We would like to be complete on the catalog, so we should not miss any of the OB population (even at the expense of some FP). Therefore, we need to minimize FN, and <b>maximize Recall</b>.\n",
    "\n",
    "</details>\n",
    "\n",
    "An interesting presentation of this topic can be found [here](https://towardsdatascience.com/precision-and-recall-a-comprehensive-guide-with-practical-examples-71d614e3fc43). \n",
    "\n",
    "- - -\n",
    "\n",
    "But there is an important detail: we need to decide the **threshold**: i.e.,  the _probability score_ at which an object is classified as class 0 or 1.\n",
    "\n",
    "<br>\n",
    "<center><i>By default, we and <code>sklearn</code> classifiers assume <b>50%</b> as the score threshold</i></center>\n",
    "\n",
    "... but this can change! For example, considering the previous scenarios:\n",
    "\n",
    " 1. to identify best follow-up candidates of GW sources, we want to waste no precious time on wrong targets --> we can **increase** the threshold to be stricter in our selection (we lower FP) and we **increase Precision**. But now we reject more (**FN increases**) and **Recall decreases**. \n",
    "\n",
    " 2. to create a catalog of OB stars, we want to be as complete as possible, so in order to recover the OB population we **decrease FN** (**increased Recall**) but at the expense of **more FP**, and therefore **decreased Precision**.\n",
    " \n",
    " Therefore, the  **metrics** change based on the chosen threshold.\n",
    "<br><br>\n",
    "\n",
    "**Question:** Can we visualize my classifier's expected behaviour when changing the threshold?<br>\n",
    "Yes, plot the **Receiver Operating Characteristic (ROC)** curve.\n",
    "\n",
    "The ROC shows the classifier's TPR and FPR across all possible classification thresholds.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ROC.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.3.  Example of a Receiver Operating Characteristic (ROC) with an AUC of 0.79.\n",
    "            <br>\n",
    "            (From <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\"><code>sklearn</code> ROC page</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "**Question:** In the case of <u>multiple models</u> (each one with its ROC curve) which one is the _best_ model **over all thresholds**?!<br>\n",
    "We can claim it is the one that strikes the <u>best balance</u> between TPRs and FPRs across all possible classification thresholds.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ROCs.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.4.  ROC curves for different classifiers.\n",
    "            <br>\n",
    "            (From <a href=\"https://cuitandokter.com/\"><code>sklearn</code> ROC page</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "It is also possible to do the same with Precision-Recall plots (more accurate especially with imbalanced data). \n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/precision-recall_plot.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.5. Precision-Recall plot for different classes .\n",
    "            <br>\n",
    "            (Adapted from Fig. 4 by <a href=\"https://ui.adsabs.harvard.edu/abs/2022A%26A...666A.122M/abstract\">Maravelias et al. (2022)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "**Question:** Is there a way to do that without looking at the plots? </br>\n",
    "Sure, the **Area Under the ROC Curve (AUC)** measures exactly this.\n",
    "\n",
    "_NOTES_: \n",
    "\n",
    "- The 1-to-1 line in Figure 2 indicates a **random** classifier (in binary case, it means it randomly assigns class 1 or 0).<br>\n",
    "  For this reason, the minimal AUC is _de facto_ 0.5.\n",
    "- The **furthest** from the 1-to-1 line, the **better** the classifier.\n",
    "- The **ideal** classifier has AUC = 1.\n",
    "\n",
    "For more info about the interpretation of the AUC, you can read [this page](https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077) and [this post on Google Developers](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question**: Is AUC a metric **superseeding** all the others?\n",
    "\n",
    "- In general: <u>no</u>, they measure different things\n",
    "    \n",
    "  _AUC gives performance across all thresholds, but in practice you you pick **one** threshold when you use the model_  \n",
    "  \n",
    "  \n",
    "- If your classes are heavily unbalanced: <u>probably</u>\n",
    "\n",
    "  _In this case it is possible that you don't know the right threshold anyways_\n",
    "    \n",
    "You can find an interesting discussion about this topic [here](https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "And now let's go on with our example... plotting a prettier presentation of the previous confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "                  \n",
    "                  \n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, alpha=0.5)\n",
    "#    plt.title(title)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('# sources', fontsize=16)\n",
    "    cbar.ax.tick_params(labelsize=16) # (fontsize=15)\n",
    "   \n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=15)\n",
    "        plt.yticks(tick_marks, target_names, fontsize=15)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.3f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\", color=\"black\", fontsize=14 )\n",
    "                     #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\", color=\"black\") \n",
    "#                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=16)\n",
    "    plt.xlabel('Predicted label (accuracy={:0.2f})'.format(accuracy, misclass), fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix( metrics.confusion_matrix( y_test, y_pred),\n",
    "                      ['SEL','CON'],\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns fraction, False raw numbers\n",
    "                      ) # YlOrBr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time: Experiment with some selections\n",
    "\n",
    "**1. What happens if we start adding more classes into the selected one ?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "The accuracy drops as more classes results in a more complex problem and the classes may overlap in the feature space (confusing the classifier even more!).\n",
    "</details>\n",
    "\n",
    "**2. What happens when we start changing the $C$ parameter?**\n",
    "\n",
    "_HINT_: the default value is 1, so start increasing it. \n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "A lower value off C allows for more misclassifications, i.e. more sources are found away from the diagonal in the confusion matrix. As we increase its value the decrease but we will lose in prediction power.  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multi-class problem\n",
    "\n",
    "Now we approach the same problem as a multi-class one, i.e. we are using the SVM as a classifier that can handle all classes simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_selected = [ ... ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected,[])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we print the labels again, we notice that the array is totally different and the multiclass output is evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test sets for multi-class\n",
    "\n",
    "We are using again the train-test split approach but considering all classes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(ml_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split( ... , ... , \n",
    "                                test_size= ... ) #, random_state=42) \n",
    "\n",
    "print(f'> From {len(ml_objects)} sources we use {len(X_train)} for training and {len(X_test)} for testing.') \n",
    "\n",
    "print('\\nStatistics per class:')\n",
    "k = 0\n",
    "for c in unique_cls:\n",
    "    items_train = np.where( y_train==c )[0]\n",
    "    items_test  = np.where( y_test==c )[0]\n",
    "    items_total = np.where( ml_labels==c )[0]\n",
    "    print(f'> For {c} there are {len(items_total)} sources split in {len(items_train)} (train) and {len(items_test)} (test) samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time: Check the train-test splitting\n",
    "\n",
    "**1. Run train-test split a few times as it is  - do you notice anything strange?** \n",
    "\n",
    "_HINT_: try to reduce the test_size, for the evidence to become clearer. \n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "It is possible to get 0 test objects for some classes. This is due to the randomness of the selection and the low number of sources in some of the classes. \n",
    "</details>\n",
    "\n",
    "**2. How can we correct for the effect in the previous question?**\n",
    "\n",
    "_HINT_: Check the documentation ([train-test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) and find out which parameters help.\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "Add shuffle=True and stratify ensures that the test sample will keep the demographics of the original one. \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><center>Important take-away message #1</center></font>\n",
    "<center>sklearn <i>documentation</i> is your friend !</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce some results now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SVC(kernel='linear') \n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "#print(y_pred)\n",
    "\n",
    "#confmatrix = metrics.confusion_matrix( y_test, y_pred)\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "plot_confusion_matrix( metrics.confusion_matrix( y_test, y_pred),\n",
    "                      unique_cls,\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns precent, False raw numbers\n",
    "                      ) # YlOrBr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "\n",
    "**1. How does the result changes with respect to the binary case?** \n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "We see a drop in the accuracy, because we try to classify more classes which are actually not easilly separated in the feature space. In other words, some classes may have similar magnitudes and colors, which makes it difficult to separate them. \n",
    "</details>\n",
    "\n",
    "**2. How does the result change with kernel ?** \n",
    "\n",
    "_HINT_: check sklearn.svm.SVC\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "Adding more classes (and bands) increases the dimensionality of the problem, therefore, there is a more complex representation of the input data. Kernels help to fit them more efficiently since they are able to \"see\" in more dimensions.   \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><center>Important take-away message #2</center></font>\n",
    "<center>Accuracy is <i>not</i> the best metric to use when we have imbalanced datasets.</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Random Forests algorithm\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "A **Decision Tree** (**DT**) is simply a top-to-bottom tree-like structure where each node corresponds to a question (or a set of features more generally) that distinguishes objects to two groups, left and right from the node. A decision tree presents the drawback of learning extremely well the training set. That means that DTs overfit the data and they cannot predict very accurately new data.\n",
    "\n",
    "<center><img src=\"images/DecisionTree.jpg/\" width=400> \n",
    "Figure 5.1. Quick introduction to Decition Trees - how to fulfill an everyday need.<br>\n",
    "(Credit: G. Maravelias)</center>\n",
    " \n",
    "\n",
    "##  Random Forests\n",
    "\n",
    "**Random Forests** (**RF**) or Random Decision Trees ([Breiman (2001), Machine Learning, 45, 5](https://doi.org/10.1023/A:1010933404324)) is a generalization of the DTs, as it utilises a multitude of decision trees. When RF are used as a classification method, for each input datum the final output is the class (/value) given by the mode of the classes of the individual trees.\n",
    "\n",
    "\n",
    "<center><img src=\"images/RandomForests.jpg\" width=800> \n",
    "Figure 5.2. Schematic description of the Random Forest classifier.<br>\n",
    "(Credit: G. Maravelias)</center>\n",
    "\n",
    "\n",
    "RF creates a large number of DTs through random selection of a subset of the training set as well as a random selection of features. This randomness reduces the correlation between the different DTs. Since the DTs have different conditions in their nodes and different overall structures, this diversity yield overall robust predictions. \n",
    "\n",
    "Once the RF has been trained, the data of an **unlabeled** source (to be classified) are  fed into all DTs of the forest. According to its properties and the nodes in each DT, it follows a specific path which leads to a given class. The final output of the RF (the prediction) is an aggregation of all DTs by means of a majority vote.\n",
    "\n",
    "The fact that RF combines the prediction for a number of individual trees makes it an **ensemble** method.\n",
    "\n",
    "&#9733; [Reis, Baron, & Shahaf (2019), AJ, 157, 16](https://ui.adsabs.harvard.edu/abs/2019ascl.soft03009R/abstract) provide an excellent description of RFs (for a two-class problem) and present a  probabilistic RF method which takes into account the uncertainties on the data and their labels.\n",
    "\n",
    "## Final remarks on Random Forests\n",
    "\n",
    "In machine learning most of the effort is actually spent on the **sample** selection and, most importantly, on the selection of the **features** used for the classification (feature engineering).\n",
    "\n",
    "RF partially overcome the latter problem by training each DT on a different sub-set of features, hence training the algorithm to recognize the features which mostly differentiate the objects.\n",
    "\n",
    "**PROS**\n",
    "- No need of scaling or transformation of the initial data.\n",
    "- Implicit feature selection.\n",
    "- Suitable for large datasets with many features.\n",
    "\n",
    "\n",
    "**CONS**\n",
    "- Not easily interpretable.\n",
    "- Hyperparameter needs good tuning for high accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Random Forests in practice\n",
    "\n",
    "Using the same dataset we are now approaching the same problem using the Random Forest (both as a binary and a multiclass classifier).\n",
    "\n",
    "---\n",
    "\n",
    "**TASK 1: Complete the train-test split and select magnitudes to work with.**\n",
    "\n",
    "**TASK 2: Find the proper function for RF and make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2keep_RF=[ ... ]  # add any class if you want to use RF as binary classifier, or keep it []\n",
    "bands_selected_RF = [ ... ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected_RF, class2keep_RF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data on magnitudes or colors\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( ... , \n",
    "                                                     ... ,\n",
    "                                shuffle= ... , stratify= ..., \n",
    "                                test_size= ... ) #,  random_state=42) \n",
    "\n",
    "print(f'> From {len(ml_objects)} sources we use {len(X_train)} for training and {len(X_test)} for testing.') \n",
    "\n",
    "# check if in binary or multi-label mode\n",
    "if 'CON' in ml_labels: # binary\n",
    "    confmat_classes = class2keep_RF+['CON']\n",
    "else:\n",
    "    confmat_classes = unique_cls\n",
    "\n",
    "    print('\\nStatistics per class:')\n",
    "    k = 0\n",
    "    for c in unique_cls:\n",
    "        items_train = np.where( y_train==c )[0]\n",
    "        items_test  = np.where( y_test==c )[0]\n",
    "        items_total = np.where( ml_labels==c )[0]\n",
    "        print(f'> For {c} there are {len(items_total)} sources split in {len(items_train)} (train) and {len(items_test)} (test) samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfrf = RandomForestClassifier()\n",
    "clfrf.fit(X_train, y_train)\n",
    "y_pred = clfrf.predict(X_test)\n",
    "\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report( ... , ... )}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix( ... , ... )}\")\n",
    "\n",
    "plot_confusion_matrix( metrics.confusion_matrix( ... , ...),\n",
    "                      confmat_classes,\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns percent, False raw numbers\n",
    "                      ) # YlOrBr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "\n",
    "**1. Why the results change by re-running it ?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "RF as the name suggest create always a different set of trees with random eselection of features and data. Consequently, the RF from one run to the other is not the same and there are small variations. However, the results are robust because they are averaging the results from all trees. \n",
    "</details>\n",
    "\n",
    "**2. What is the difference on the accuracy of the algorithm if we use the colors instead of the magnitudes? Why do it?**\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "Depending on the classes and bands selected the accuracy will (slightly) change. RF are rather insensitive to the input data (scaled or not). \n",
    "    \n",
    "However, colors are distance-independent features, which means that a model build with these features could be applicable to other galaxies also. Colors are distance-independent features, which means that the model could be applicable to other galaxies also. On the contrary, if we use only magnitudes this would mess up the prediction as with distance the sources become fainter.  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Initial steps to follow when building a classification model\n",
    "\n",
    "> <font size=4>1. Examine / visualize / preprocessing data\n",
    "> \n",
    "> 2. Select features\n",
    "    \n",
    "> 3. Select algorithm\n",
    "    \n",
    "> 4. Split data and follow Golden rule \n",
    "    \n",
    "> 5. Select metrics\n",
    "    \n",
    "> 6. Examine results and re-iterate \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms overview\n",
    "\n",
    "This is a presentation of the set of [**sklearn**](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) algorithms with toy datasets, and then describe some of them. \n",
    "\n",
    "This serves as a showcase of the available methods and how they compare. You can easily adapt any of these methods to the following examples or your own problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We provide the code fully here for completeness although it does not run\n",
    "# # due to the fact that DecisionBoundaryDisplay function is in the experimental version\n",
    "\n",
    "# ######################################################################################\n",
    "\n",
    "# # Code source: GaÃ«l Varoquaux\n",
    "# #              Andreas MÃ¼ller\n",
    "# # Modified for documentation by Jaques Grobler\n",
    "# # License: BSD 3 clause\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# # Generate toy dataset\n",
    "\n",
    "# X, y = make_classification(\n",
    "#     n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    "# )\n",
    "# rng = np.random.RandomState(2)\n",
    "# X += 2 * rng.uniform(size=X.shape)\n",
    "# linearly_separable = (X, y)\n",
    "\n",
    "# datasets = [\n",
    "#     make_moons(noise=0.3, random_state=0),\n",
    "#     make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "#     linearly_separable,\n",
    "# ]\n",
    "\n",
    "# # Setting up the classification algorithms (parameters)\n",
    "\n",
    "# names = [\n",
    "#     \"Nearest Neighbors\",\n",
    "#     \"Linear SVM\",\n",
    "#     \"RBF SVM\",\n",
    "#     \"Gaussian Process\",\n",
    "#     \"Decision Tree\",\n",
    "#     \"Random Forest\",\n",
    "#     \"Neural Net\",\n",
    "#     \"AdaBoost\",\n",
    "#     \"Naive Bayes\",\n",
    "#     \"QDA\",\n",
    "# ]\n",
    "\n",
    "# classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "#     SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "# ]\n",
    "\n",
    "# # Running and plotting\n",
    "\n",
    "# figure = plt.figure(figsize=(27, 9))\n",
    "# i = 1\n",
    "# # iterate over datasets\n",
    "# for ds_cnt, ds in enumerate(datasets):\n",
    "#     # preprocess dataset, split into train and test part\n",
    "#     X, y = ds\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.4, random_state=42\n",
    "#     )\n",
    "\n",
    "#     x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "#     y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "#     # just plot the dataset first\n",
    "#     cm = plt.cm.RdBu\n",
    "#     cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "#     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#     if ds_cnt == 0:\n",
    "#         ax.set_title(\"Input data\")\n",
    "#     # Plot the training points\n",
    "#     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "#     # Plot the testing points\n",
    "#     ax.scatter(\n",
    "#         X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "#     )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     i += 1\n",
    "\n",
    "#     # iterate over classifiers\n",
    "#     for name, clf in zip(names, classifiers):\n",
    "#         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         score = clf.score(X_test, y_test)\n",
    "#         DecisionBoundaryDisplay.from_estimator(\n",
    "#             clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "#         )\n",
    "\n",
    "#         # Plot the training points\n",
    "#         ax.scatter(\n",
    "#             X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "#         )\n",
    "#         # Plot the testing points\n",
    "#         ax.scatter(\n",
    "#             X_test[:, 0],\n",
    "#             X_test[:, 1],\n",
    "#             c=y_test,\n",
    "#             cmap=cm_bright,\n",
    "#             edgecolors=\"k\",\n",
    "#             alpha=0.6,\n",
    "#         )\n",
    "\n",
    "#         ax.set_xlim(x_min, x_max)\n",
    "#         ax.set_ylim(y_min, y_max)\n",
    "#         ax.set_xticks(())\n",
    "#         ax.set_yticks(())\n",
    "#         if ds_cnt == 0:\n",
    "#             ax.set_title(name)\n",
    "#         ax.text(\n",
    "#             x_max - 0.3,\n",
    "#             y_min + 0.3,\n",
    "#             (\"%.2f\" % score).lstrip(\"0\"),\n",
    "#             size=15,\n",
    "#             horizontalalignment=\"right\",\n",
    "#         )\n",
    "#         i += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/sklearn_classification.png\" width=1200> \n",
    "Figure 2.1. Overview of the clustering algorithms in sklearn\n",
    "</img>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizz time: take a few moments and explore the results - what do you notice? \n",
    "\n",
    "Write some points here:\n",
    "\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important take-away points\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary> An indicative list: </summary>\n",
    "<ul>\n",
    "    <li> There is not a single best algorithm. \n",
    "    <li> Some algorithms are more able to separate classes.\n",
    "    <li> Some algorithms have limited ability (e.g. the linear SVM with non-linear distributed data). \n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further notes / resources\n",
    "\n",
    "For the descriptio and the application of k-Nearest Neighbors on the same example look into the [Summer School for Astrostatistics 2022 material](https://github.com/astrostatistics-in-crete/2022_summer_school)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrostat23",
   "language": "python",
   "name": "astrostat23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
