{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**ML Intro and Clustering**</font> </h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we provide a short introduction to **Machine Learning**. We also present what one major application **Clustering** through a simple example. <br>\n",
    "The goals are:\n",
    "\n",
    "- to get a grasp of what **machine learning** is \n",
    "- to show the basic concept of **data exploration** and **interpretetion**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit of introduction to ML\n",
    "\n",
    "## What is machine learning ?\n",
    "\n",
    "Arthur Samuel (1959): \n",
    "\n",
    ">*\"Field of study that gives computers the ability to learn without being explicitly programmed.\"*\n",
    "\n",
    "## How did we get here? \n",
    "\n",
    "<center><<img src=\"images/ml_history.png\" width=800> \n",
    "    Figure 1.1. Timeline from Artificial Intelligence, to Machine Learning, and Deep learning.<br>\n",
    "(Credit: <a href=\"https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/\" target=\"_blank\" rel=\"noopener noreferrer\"> Nvidia blog - (an interesting read!</a>)</center>\n",
    "\n",
    "## and ... why now? \n",
    "\n",
    "- More powerful, abundant, and cheap computation (CPUs/GPUs).\n",
    "- Growing data sets.\n",
    "- Advancements in underlying algorithms and implementation.\n",
    "\n",
    "This is true for both everyday and Astronomy applications !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML branches\n",
    "\n",
    "<center><<img src=\"images/ml_branches.png\" width=800> \n",
    "Figure 2.1. Branches and example applications of Machine Learning<br>\n",
    "(Credit: <a href=\"https://www.cognub.com/index.php/cognitive-platform/\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\"> CogHub</a>)</center>\n",
    "\n",
    "- **Supervised**: Labelled data where the algorithms learn to predict the output from the input data.\n",
    "- **Unsupervised**: Non-labelled data where the algorithms learn to identify structures from the input data.\n",
    "- **Semi-supervised**: Some labelled data - most is not - and a mixture of supervised and unsupervised techniques can be implemented.\n",
    "\n",
    "## Unsupervised approaches in particular:\n",
    "\n",
    "- Clustering: discover groupings or/and structures in the data, i.e. concentrations of datapoints or overdensities (e.g. the locations of galaxies in a BPT diagram)\n",
    "- Association:  discover the rule(s) describing between variables or features in a dataset (e.g. customer recommendations)\n",
    "- Dimentionality reduction: tool to reduce the number of input variables or features in a dataset (the more you have the more challenging it becomes to build a predictive model - referred to as *curse of dimentionality*) - also usefulf for visualization purposes.  \n",
    "\n",
    "## Pros \n",
    "+ They can make new discoveries, as often enough we don’t know what they’re looking for in data.\n",
    "+ They do not require training, which saves (huge) time on producing labels (manual classification tasks such as spectroscopic classification).\n",
    "+ It reduces the chance of human error and bias, which could occur during manual labeling processes.\n",
    "+ Unlabeled data is much easier and faster to get.<br>\n",
    "\n",
    "## Cons\n",
    "- Output needs careful proper interpretation: \n",
    "    - the groups may not match informational classes\n",
    "    - extra effort has to be made to validate the groups. \n",
    "- Less accurate predictive results, as the labels are not part of the process and the method has to learn it by itself. \n",
    "- More time is needed to train these algorithms:\n",
    "    - they need time to analyze and calculate all possibilities\n",
    "    - the deal with huge datases that may increase computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most critical take-home points:\n",
    "\n",
    "- No matter which algorithm you pick, the goal of ML is to make **predictions** and **classifications**.\n",
    "- There is **no optimal** algorithm, it all depends on your specific problem!<details>\n",
    "<summary>( Click for an illustration of this point )</summary>\n",
    "<center><img src=\"images/my-precious-not.jpg\"> \n",
    "Figure 3.1. There is not a single algorithm to rule them all !!!<br>\n",
    "(Credit: <a href=\"https://knowyourmeme.com/memes/my-precious\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\"> knowyourmeme.com</a>)</center>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "\n",
    "    or: \"What can data tell us ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a tool - the K-means algorithm\n",
    "\n",
    "The K-means algorithm tries to partition a sample of N observations (with each observation being a $d$-dimensional vector) into $k$ individual clusters $C_k$. \n",
    "\n",
    "We first need to find a metric, i.e. to define the **loss function**, which (in this case) is the within-cluster sum-of-squares of the observations:\n",
    "\n",
    "$$ Loss = \\sum_{k=1}^{K} \\sum_{i \\epsilon C_k} ||x_i-\\mu_k||^2$$,\n",
    "\n",
    "where $\\mu_k=\\frac{1}{N_k}\\sum_{i \\epsilon C_k} x_i$ is the mean/centroid of the $N_k$ points included in each of the $C_k$ clusters. \n",
    "\n",
    "The solution comes from **minimizing** the above function, i.e.: \n",
    "\n",
    "$$ min (Loss) = \\min_{\\mu_k} \\left (  \\sum_{k=1}^{K} \\sum_{i \\epsilon C_k} ||x_i-\\mu_k||^2 \\right )$$. \n",
    "\n",
    "**IMAGE BEFORE OR AFTER ?**\n",
    "\n",
    "<center><<img src=\"images/kmeans.gif\"> \n",
    "Figure 5.1. Evolution of K-means centroids through iterations. <br>\n",
    "(Credit: <a href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"  target=\"_blank\" rel=\"noopener noreferrer\">Clustering with Scikit with GIFs, by David Sheehan</a>)</center>\n",
    "\n",
    "**Steps**:\n",
    "1. Initiate algorithm by selecting $k$ means <br>\n",
    "    *e.g. select randomly $k$ observations as initial means - see also [Wiki:K-means initialization](http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods)*\n",
    "2. Assign each observation to the nearest cluster\n",
    "3. Calculate the new mean value for each cluster $C_k$ according to the new observations assiged\n",
    "4. Repeat steps 2 and 3 up to the point that there are no updates in the assigments to the clusters.\n",
    "\n",
    "A globally optimal minimum is not guaranteed (might converge to a local minimum). This is highly dependent on the initialization of the centroids. This is why, in practice, K-means is run multiple times with different starting values selecting the result with the lowest sum-of-squares error. To improve on that we can initially select centrodis that are generally distant from each other (sklearn implementation by using `init='k-means++'` parameter). For more see [Grouping data points with k-means clustering, by Jeremy Jordan](https://www.jeremyjordan.me/grouping-data-points-with-k-means-clustering/).\n",
    "\n",
    "**Complexity**\n",
    "\n",
    "$O(knT)$, where k, n and T are the number of clusters, samples and iterations, respectively.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Simple and intuitive\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- The number of clusters (K) must be provided (or cross-validated)\n",
    "- There is an inherit assumption of isotropic clusters (i.e. not well fitted for elongated clusters, or manifolds with irregular shapes)\n",
    "- Inertia is not a normalized metric: lower values are better , but as the dimensions increase so does the inertia\n",
    "\n",
    "An alternative for faster implementation - Mini Batch K-means\n",
    "\n",
    "For faster computations the sklearn offers the [Mini Batch K-means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans) method which simply breaks the initial set of observations/data points to smaller randomly selected subsamples.\n",
    "\n",
    "For each subsample in the mini batch the assigned centroid is updated by taking into account the average of that subsample and all previous subsamples assigned to that centroid. This is repeated until the predefined number of iterations is reached. Its results are generally only slightly worse then the standard algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example from stellar sources\n",
    "\n",
    "The following dataset originates from measurements of two spectral lines for a sample of stars. The strength of a spectral line can be measured by its equivalent width (EQW).  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/equivalent_width-wiki.jpg\"> \n",
    "Figure 12.1. Defining EQW: the width of a line with intensity equal to the local continuum and total flux equal to that of the line.   <br>\n",
    "(Credit: <a href=\"https://en.wikipedia.org/wiki/Equivalent_width\" \n",
    " target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia: Equivalent Width, by Szdori </a>)\n",
    "    </img>\n",
    "    </div>\n",
    "\n",
    "Therefore, the larger the EQW the stronger the line is. The presence of spectral lines depends on the temperature of the stellar sources. Because of this, we see a developmet of different spectral lines as we move from the hottest to the cooler stars. This corresponds to moving from earlier spectral types (O-type stars; 50-25kK) to later ones (M-type; 3.5-2.5kK) [see the Morgan-Keenan spectral classification scheme](https://en.wikipedia.org/wiki/Stellar_classification)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18}) # updating font size\n",
    "from matplotlib import colors\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def flospecConv(arg):\n",
    "    \"\"\"\n",
    "    Function to convert from spectral types to \n",
    "    float numbers (e.g. B0,O9.5 to 20.0,19.5)\n",
    "    and backwards.\n",
    "    \"\"\" \n",
    "    try:\n",
    "        float(arg)\n",
    "        if str(arg)[0]=='1':\n",
    "            sp = 'O'\n",
    "        elif str(arg)[0]=='2':\n",
    "            sp = 'B'\n",
    "        elif str(arg)[0]=='3':\n",
    "            sp = 'A'\n",
    "        else:\n",
    "            sys.exit(' ! ERROR: more than O/B stars! Adjust conversion function.')\n",
    "        new_arg = sp+str(arg)[1:]\n",
    "    except ValueError:\n",
    "        if arg[0]=='O' or arg[0]=='o':\n",
    "            fl = '1'\n",
    "        elif arg[0]=='B' or arg[0]=='b':\n",
    "            fl = '2'\n",
    "        elif arg[0]=='A' or arg[0]=='a':\n",
    "            fl = '3'\n",
    "        else:\n",
    "            sys.exit(' ! ERROR: Check input! If more than O/B stars adjust conversion function.')\n",
    "        new_arg = float(arg.replace(arg[0],fl))\n",
    "\n",
    "    return new_arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data file and selecting lines\n",
    "# when selecting lines do it in pairs, and with shorter line first\n",
    "\n",
    "PATH_data = \"data/stellar_types.dat\"\n",
    "\n",
    "sellines = ['HeII/4200', 'HeI/4471'] # in order of wavelength\n",
    "#sellines = ['HeI/4471', 'MgII/4481'] # in order of wavelength\n",
    "\n",
    "stars=defaultdict(list)\n",
    "with open(PATH_data,'r') as inf:\n",
    "    for line in inf:\n",
    "        cols = line.split()\n",
    "        objt = cols[0]\n",
    "        spln = cols[1]\n",
    "        if spln in sellines: \n",
    "#            print(splin)\n",
    "            eqw = cols[2]\n",
    "            stars[objt].append(eqw)             # the shorter line is appended first!\n",
    "            \n",
    "#print(stars)\n",
    "\n",
    "# Creating data structures:\n",
    "\n",
    "sptype, flosptype, eqwA, eqwB = [], [], [], []\n",
    "for s in stars.keys():\n",
    "    sptype.append(s.split('-')[0])\n",
    "    flosptype.append(flospecConv(s.split('-')[0]))\n",
    "    eqwA.append(float(stars[s][0]))                 # A is the shorter line\n",
    "    eqwB.append(float(stars[s][1]))                 # B is the other line (obviously!)\n",
    "    \n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X = np.column_stack((eqwA,eqwB))\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"___________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' samples x ' + str(X.shape[1]) + ' diagnostics' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "scat = plt.scatter(eqwA, eqwB, c='k')\n",
    "\n",
    "plt.xlabel(r\"EQW of line A \")\n",
    "plt.ylabel(r\"EQW of line B \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**: So, how many clusters can you identify?_\n",
    "<br>\n",
    "Make a sketch of what YOU think!\n",
    "\n",
    "Now let's try the clustering algorith to see what we get... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK : change the values and make a note of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib\n",
    "\n",
    "Clusters_kmeans = ...\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=Clusters_kmeans)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "print(\"Cluster centers:\")\n",
    "print(kmeans_model.cluster_centers_)\n",
    "\n",
    "cc_x = kmeans_model.cluster_centers_[:,0]\n",
    "cc_y = kmeans_model.cluster_centers_[:,1]\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.plot(cc_x, cc_y, 'k+', ms=100)\n",
    "\n",
    "new_map = matplotlib.cm.gray.from_list('whatever', ('blue', 'red'), N=Clusters_kmeans)\n",
    "scat2 = plt.scatter(eqwA, eqwB, c=kmeans_model.labels_, edgecolors='face', cmap=new_map)\n",
    "cb = plt.colorbar(scat2, ticks=range(0,Clusters_kmeans+1,1))   # number of clusters\n",
    "cb.set_ticklabels(range(1,Clusters_kmeans+2,1))\n",
    "cb.set_label('Cluster Label')\n",
    "\n",
    "plt.xlabel(r\"EQW of line A\")\n",
    "plt.ylabel(r\"EQW of line B\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**: How does the algorithm performs wrt your guess ?_ \n",
    "\n",
    "_HINT_: this highly depends on the number of clusters you input and selected by visual inspection.\n",
    "<br>\n",
    "\n",
    "_**Question**: What can you say about the results if you start increasing the number of clusters?_\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "In the case of 2 clusters we see that the grouping is done based on the EQW of line B mainly. \n",
    "In the case of 3 clusters we notice that line A stars to contribute. \n",
    "If we start increasing the number of clusters we run into the problem of properly interpreting the results. In other words, we need to understand if these groupings correspond to physical gorups or not. This is exactly the point where interpretation of the resuls is needed. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK : find best k value - the \"elbow\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "inert = []\n",
    "xclusters = range(... , ...)\n",
    "for i in xclusters:\n",
    "    print(f'-- working with {i} clusters...')\n",
    "    kmeans_model = KMeans(n_clusters= ... , random_state=0 )\n",
    "    kmeans_model.fit(X)\n",
    "    inert.append(kmeans_model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( ... , ...)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia (loss) ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look on the actual data now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "scat = plt.scatter(eqwA, eqwB, c=flosptype, edgecolors='face', cmap=\"viridis\")\n",
    "\n",
    "# plotting all spectral types\n",
    "specrange = np.arange(15,30,1)\n",
    "cb = plt.colorbar(scat, ticks=specrange)   # range of available spectral types\n",
    "cb.set_ticklabels([flospecConv(a) for a in specrange])\n",
    "# or if you want to present a smaller or indicative number of types\n",
    "# cb = plt.colorbar(scat, ticks=[15,22,29])   # range of available spectral types\n",
    "# cb.set_ticklabels(['O5','B2','B9'])\n",
    "cb.set_label('Spectral Types')\n",
    "\n",
    "plt.xlabel(r\"\"+f\"EQW of { sellines[0].split('/')[0]} $\\lambda${sellines[0].split('/')[1]}\")\n",
    "plt.ylabel(r\"\"+f\"EQW of { sellines[1].split('/')[0]} $\\lambda${sellines[1].split('/')[1]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question**: Given this information how can you interpret the previous results?_\n",
    "    \n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "In the case of 2 clusters we see that the grouping is done based on the EQW of HeI 4471 line, which is able to separate the sample into late B-type stars (cooler) and early B-types plus O-types (hotter). \n",
    "If we consider 3 clusters then the HeII line 4200 helps to separate the early-B types from the (hotter) O-type stars.\n",
    "A larger number of clusters will only perplex the image. \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9755; You will see more advanced techinques on data visualization in the **ML_Practices** session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering algorithms overview\n",
    "\n",
    "We start by presenting a set of [**sklearn clustering**](http://scikit-learn.org/stable/modules/clustering.html) algorithms with toy datasets, and then we continue by applying some of them in various astrophyiscal datasets.\n",
    "\n",
    "This serves as a showcase of the available methods and how they compare. You can easily adapt any of these methods to the following examples or your own problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "# Ignore sklearn warnings (remove when ready!):\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up clustering parameters\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and plotting\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz time: take a few moments and explore the results - what do you notice? \n",
    "\n",
    "Really... write down some points here (and even better send them to slack!):\n",
    "\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NOTE: The last dataset is an example of a 'null' situation for clustering, i.e. the data is homogeneous. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some take-away points\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary> An indicative list: </summary>\n",
    "<ul>\n",
    "    <li> There is not a single best algorithm. \n",
    "    <li> Not all the algorithms identify the same number of clusters.\n",
    "    <li> Some algorithms are better to detect arbitrary cluster shapes than others.\n",
    "    <li> Some algorithms can be faster.\n",
    "    <li> The intuitive clustering might not apply to very high dimensional data.\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classify Star-forming objects in a BPT diagram\n",
    "\n",
    "The \"Baldwin, Phillips & Terlevich\" (BPT) diagrams are used to distinguish sources based on specific spectral emission lines. The strengths of these lines depend on the heating source (see e.g. [BPT diagram, NED](https://ned.ipac.caltech.edu/level5/Glossary/Essay_bpt.html)).\n",
    "\n",
    "BPTs allow to distinguish:\n",
    "- AGNs (Seyfert)\n",
    "- LINERs\n",
    "- Star-forming galaxies\n",
    "- Composite objects\n",
    "\n",
    "<center><img src=\"images/BPT.png\"> \n",
    "Figure 10.1. Example of classification via BPT diagram.<br> Theoretical or observationally-calibrated curves allow to distinguish the different subpopulations.<br>\n",
    "(From <a href=\"https://ui.adsabs.harvard.edu/abs/2010ApJ...720..555P/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Parra et al. (2010), ApJ, 720, 555</a>)</center>\n",
    "\n",
    "## The sample\n",
    "\n",
    "We will use the data by [Stampoulis et al. (2019), MNRAS, 485, 1085](https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.1085S/abstract), which provides the OIII, NII, SII, and OI diagnostics for ~130 000 objects.\n",
    "\n",
    "The work also gives classifications, which we will use as a reference.\n",
    "\n",
    "--- \n",
    "\n",
    "**TASK 1: Find the best clustering algorithm for separating star-forming objects**\n",
    "\n",
    "**TASK 2: Plot results and check consistency with BPT theoretical curves**\n",
    "\n",
    "**CHALLENGE: Consult the sklearn's pages to tune more hyperparameters!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data and classifications from Stampoulis+19:\n",
    "\n",
    "PATH_Stampoulis_data = \"data/Stampoulis+19_Table_2.csv\"\n",
    "\n",
    "data = np.genfromtxt(PATH_Stampoulis_data, delimiter=\",\")\n",
    "# The data file is organized in 138799 lines (i.e. different objects), and 12 columns\n",
    "\n",
    "# To check file dimensions:\n",
    "# print(data.shape)\n",
    "\n",
    "ID               = data[:,0]  # object ID\n",
    "NII_diagnostic   = data[:,3]  # log10 ( NII_6584  / H_alpha )\n",
    "SII_diagnostic   = data[:,4]  # log10 ( SII_6717  / H_alpha )\n",
    "OI_diagnostic    = data[:,5]  # log10 ( OI        / H_alpha )\n",
    "OIII_diagnostic  = data[:,6]  # log10 ( OIII_5007 / H_beta )\n",
    "\n",
    "labels = np.genfromtxt(PATH_Stampoulis_data, delimiter=',', usecols=-1, dtype=str)\n",
    "# reading labels from last column\n",
    "# Activity class labelling scheme:\n",
    "#   0 <-> SFG (Star Forming Galaxy)\n",
    "#   1 <-> SEY (Seyfert)\n",
    "#   2 <-> LIN (LINER)\n",
    "#   3 <-> COM (Composite)\n",
    "\n",
    "# Dictionary containg class name and associated label:\n",
    "from collections import OrderedDict\n",
    "classes = OrderedDict()\n",
    "classes[\"SFG\"] = 0\n",
    "classes[\"SEY\"] = 1\n",
    "classes[\"LIN\"] = 2\n",
    "classes[\"COM\"] = 3\n",
    "\n",
    "labels = [int(float(label)) for label in labels]\n",
    "# converting labels from strings to integers\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X_sample = np.stack((OIII_diagnostic,NII_diagnostic,SII_diagnostic,OI_diagnostic),axis=-1)\n",
    "y_sample = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only 1 every \"sampling_factor\" objects for two reasons:\n",
    "\n",
    "- to speed up the exercise\n",
    "- to avoid crashes due to memory limitations.\n",
    "\n",
    "You can try to use the full sample when confident with the setup (and your computer power!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUGGESTION: Use only 1 every <sampling_factor> objects \n",
    "\n",
    "sampling_factor = 50\n",
    "# sample 1 every <sampling_factor> data to avoid computational delay\n",
    "\n",
    "X = X_sample[::sampling_factor]\n",
    "y = y_sample[::sampling_factor]\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"_____________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' samples x ' + str(X.shape[1]) + ' diagnostics' )\n",
    "print(\"-----|-------------------------------\")\n",
    "print('  y  | ' + str(len(y)) + ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although available, we will not use the labels for the analysis, but only for the first representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "Essentially reproducing Figure 5 in Stampoulis+ 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "from matplotlib import colors\n",
    "\n",
    "# Limit scatter plots (not histograms) in showing a maximum of <N_plot> objects:\n",
    "# for full sample size, use: N_plot = len(X)\n",
    "N_plot = 5000\n",
    "# NOTE: reducing the sample in the plot helps visualizing the density\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   red    <-> SFG\n",
    "#   yellow <-> SEY\n",
    "#   blue   <-> LIN\n",
    "#   green  <-> COM\n",
    "cmap = mpl.colors.ListedColormap(['red','orange','blue','green'])\n",
    "\n",
    "# Remember that the sample X is organized as:\n",
    "#  X[:,0] <-> OIII_diagnostic\n",
    "#  X[:,1] <-> NII_diagnostic\n",
    "#  X[:,2] <-> SII_diagnostic\n",
    "#  X[:,3] <-> OI_diagnostic\n",
    "\n",
    "\n",
    "# PLOT THE DIAGNOSITCS\n",
    "\n",
    "# > Classification lines\n",
    "#   NII:\n",
    "x1 = np.linspace(-2, 0.05, 100)\n",
    "x2 = np.linspace(-2, 0.47, 100)\n",
    "x3 = np.linspace(-0.1839, 1)\n",
    "ke01_NII = 0.61 / (x1-0.05) + 1.3   # Kewley+01\n",
    "ka03_NII  = 0.61 / (x2-0.47) + 1.19 # Kuffmann+03\n",
    "sc07_NII  = 1.05 * x3 + 0.45        # Schawinski+07\n",
    "#   SII:\n",
    "x4 = np.linspace(-2, 0.05, 100)\n",
    "x5 = np.linspace(-0.3, 1)\n",
    "ke01_SII  = 0.72 / (x4-0.32) + 1.3  # Kewley+01\n",
    "ke06_SII  = 1.89 * x5 + 0.76        # Kewley+06\n",
    "#   OI:\n",
    "x6 = np.linspace(-2, -0.8, 100)\n",
    "x7 = np.linspace(-1.1, 0)\n",
    "ke01_OI = 0.72 / (x6+0.59) + 1.33  # Kewley+01\n",
    "ke06_OI = 1.18 * x7 + 1.30         # Kewley+06\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ylim = [-1.2,1.5] # OIII_diagnostic range\n",
    "\n",
    "# > left plot\n",
    "\n",
    "xlim = [-2,1] # NII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "im = ax.scatter(X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([NII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x1, ka03_NII, \"--\", color='red',  linewidth = 1.0, label='Ka03')\n",
    "ax.plot(x2, ke01_NII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x3, sc07_NII, \"-\",  color='blue', linewidth = 1.0, label='Sc07')\n",
    "ax.legend()\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.25, \"SFG\", color='red',    transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.20, \"SEY\", color='orange', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.15, \"LIN\", color='blue',   transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"COM\", color='green',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > central plot\n",
    "\n",
    "xlim = [-1.4,0.7] # SII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "im = ax.scatter(X[-N_plot:, 2], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([SII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x4, ke01_SII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x5, ke06_SII, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# > right plot\n",
    "\n",
    "xlim = [-2.2,0.0] # OI_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "im = ax.scatter(X[-N_plot:, 3], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([OI]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x6, ke01_OI, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x7, ke06_OI, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# NOTE: Ignore the warning, due to the plotting of the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running clustering algorithms and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtrcs( myX, algo_labels):  \n",
    "    sil = metrics.silhouette_score( myX, algo_labels, metric='euclidean')\n",
    "    ch = metrics.calinski_harabasz_score( myX, algo_labels )    \n",
    "    db = metrics.davies_bouldin_score( myX, algo_labels)\n",
    "    \n",
    "    return sil, ch, db \n",
    "\n",
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "#from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9 * 2 + 3, 8))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "# Setting up clustering parameters\n",
    "\n",
    "params = {'quantile': ... ,\n",
    "            'eps': ... ,\n",
    "            'damping': ... ,\n",
    "            'preference': ... ,\n",
    "            'n_neighbors': ... ,\n",
    "            'n_clusters': ... }\n",
    "\n",
    "# normalize dataset for easier parameter selection\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# estimate bandwidth for mean shift\n",
    "bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "# connectivity matrix for structured Ward\n",
    "connectivity = kneighbors_graph(\n",
    "    X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "# make connectivity symmetric\n",
    "connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "# ============\n",
    "# Create cluster objects\n",
    "# ============\n",
    "ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "k_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "ward = cluster.AgglomerativeClustering(\n",
    "    n_clusters=params['n_clusters'], linkage='ward',\n",
    "    connectivity=connectivity)\n",
    "average_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"average\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "complete_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"complete\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "spectral = cluster.SpectralClustering(\n",
    "    n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "    affinity=\"nearest_neighbors\")\n",
    "dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "affinity_propagation = cluster.AffinityPropagation(\n",
    "    damping=params['damping'], preference=params['preference'])\n",
    "birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "clustering_algorithms = (\n",
    "    ('MiniBatchKMeans', k_means),\n",
    "    ('AffinityPropagation', affinity_propagation),\n",
    "    ('MeanShift', ms),\n",
    "    ('SpectralClustering', spectral),\n",
    "    ('Ward', ward),\n",
    "    ('AgglomerativeClustering', average_linkage),\n",
    "    ('DBSCAN', dbscan),\n",
    "    ('Birch', birch),\n",
    "    ('GaussianMixture', gmm)\n",
    ")\n",
    "\n",
    "# restoring normalization for plotting\n",
    "X_plot = X_sample[::sampling_factor]\n",
    "\n",
    "all_scores = defaultdict(list)\n",
    "for name, algorithm in clustering_algorithms:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # catch warnings related to kneighbors_graph\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"the number of connected components of the \" +\n",
    "            \"connectivity matrix is [0-9]{1,2}\" +\n",
    "            \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "            category=UserWarning)\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"Graph is not fully connected, spectral embedding\" +\n",
    "            \" may not work as expected.\",\n",
    "            category=UserWarning)\n",
    "        algorithm.fit(X)\n",
    "\n",
    "    t1 = time.time()\n",
    "    if hasattr(algorithm, 'labels_'):\n",
    "        y_pred = algorithm.labels_.astype(int)\n",
    "    else:\n",
    "        y_pred = algorithm.predict(X)\n",
    "        \n",
    "    plt.subplot(1, len(clustering_algorithms), plot_num)\n",
    "    plt.title(name, size=18)\n",
    "\n",
    "    plt.plot(x1, ka03_NII, \"--\", color='grey',  linewidth = 1.0, label='Ka03')\n",
    "    plt.plot(x2, ke01_NII, \"-\",  color='grey',  linewidth = 1.0, label='Ke01')\n",
    "    plt.plot(x3, sc07_NII, \"-\",  color='grey', linewidth = 1.0, label='Sc07')\n",
    "    \n",
    "    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                         '#f781bf', '#a65628', '#984ea3',\n",
    "                                         '#999999', '#e41a1c', '#dede00']),\n",
    "                                  int(max(y_pred) + 1))))\n",
    "    plt.scatter(X_plot[:, 2], X_plot[:, 0], s=2, color=colors[y_pred])\n",
    "    plt.scatter(X_plot[y_pred==-1,2], X_plot[y_pred==-1,0], s=100, facecolors='none', edgecolors='black', color='black', label='background', alpha=0.1, zorder=0)\n",
    "    # marking background class for DBSCAN\n",
    "\n",
    "    plt.gca().set_xlim(-2,1)     # NII_diagnostic range\n",
    "    plt.gca().set_ylim(-1.2,1.5) # OIII_diagnostic range\n",
    "\n",
    "    plt.xticks(np.arange(xlim[0], xlim[1], step=1))\n",
    "    plt.yticks(np.arange(ylim[0], ylim[1], step=0.5))\n",
    "\n",
    "    plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n",
    "    plot_num += 1\n",
    "    \n",
    "    all_scores[name].append(mtrcs(X_plot,y_pred ))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Which algorithm performs best?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "The Ward algorithm seems to be the best in separating Star-Forming Galaxies.\n",
    "\n",
    "Other algorithms (e.g. Affinity Propagation) might be considered valid if we join some clusters.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics fo blind selection of cluster number\n",
    "\n",
    "Now, given that we do not know anything apriori for the numbers of clusters, there are some metrics that can used to determine the performance in each case. \n",
    "\n",
    "### Silhouette Score\n",
    "\n",
    "Using the distances between the points in the same cluster and theirs with all other points in the next nearer cluster:\n",
    "\n",
    "$$ s = \\frac{b - a}{max(a,b)} $$ \n",
    "\n",
    "where a corresponds to the mean distance between a sample and all other samples _within the same cluster_, while b is the mean distance of a sample (in the examined cluster) with all other points in the _next nearest cluster_. \n",
    "\n",
    "&#10509; A **higher** value indicates better performance.\n",
    "\n",
    "### Calinski-Harabasz Index\n",
    "\n",
    "Similar to the silhouette score, but using the ratio of the sum of dispersion of the samples within each cluster and the dispersion of the clusters in total. For a set of data $E$ of size $n_E$ which has been clustered into $k$ clusters, it is defined as:\n",
    "\n",
    "$$ s = \\frac{tr(B_k)}{tr(W_k)} \\times \\frac{n_E-k}{k-1} $$\n",
    "\n",
    "where $tr(B_k)$ is the trace of the between group dispersion matrix, and $tr(W_k)$ the trace of the within-cluster dispersion matrix, defined as:\n",
    "\n",
    "$$ W_k = \\sum_{q=1}^{K} \\sum_{x \\in C_q} (x - c_q)(x - c_q)^T \\\\\n",
    " B_k = \\sum_{q=1}^{k} n_q(c_q-c_E)(c_q-c_E)^T $$\n",
    " \n",
    "with $C_q$ the set of points in cluster $q$, $c_q$ the center of cluster $q$, $c_E$ the center of $E$, and $n_q$ the number of points in cluster $q$.  \n",
    "\n",
    "&#10509; A **higher** value indicates better performance (denser and well separated clusters). \n",
    "\n",
    "### Davies-Bouldin Index\n",
    "\n",
    "It is an index that calculates the 'similarity' between clusters, and actually easures how the distance between clusters compares with the sizes of the clusters themselves.  \n",
    "\n",
    "$$ DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{i \\neq j} R_{ij} $$\n",
    "\n",
    "where $R_{ij} = \\frac{s_i+s_j}{d_{ij}}$ corresponds to the similarity measurement, $s_i$ is the average distance between each point of cluster $i$ and the centroid of that cluster (diameter), and $d_{ij}$ is the distance between cluster centroids $i$ and $j$. The form chosen for $R_{ij}$ is a nonnegative and symmetric. \n",
    "\n",
    "&#10509; A **lower** value (closer to 0) indicate better performance.\n",
    "\n",
    "---\n",
    "\n",
    "You can (should!) check [sklearn's clustering metrics page](https://scikit-learn.org/stable/modules/clustering.html) for more details regrding the drawbacks and aadvantages of each metric).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ALGORITHM: Silhouette Score -- Calinski-Harabasz Index -- Davies-Bouldin Index  ')\n",
    "print('='*80)\n",
    "for algo, scores in all_scores.items():\n",
    "    print(f'{algo:<26}: {\" -- \".join([f\"{s:7.2f}\" for s in scores[0]])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Which algorithm performs best according to the metrics?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "In this case it seems that Agglomerative is the best according to the silhouette score and Davies-Bouldin Index. This looks different from our 'visual' metric but those indeces can catch more details and they examine all classes (while we focused on the star-forming galaxies). \n",
    "\n",
    "However, when examining the Calinski-Harabasz Index other algorithms seem to perform better (Spectral, Affinitiy, MiniBatchKMeans). This highlights that the problem is comples and more careful and detaild work is needed. \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further notes / resources\n",
    "\n",
    "For the description of some other algorithms as well as some applications in astrophysical problems, take a look at the [Summer School for Astrostatistics 2022 material](https://github.com/astrostatistics-in-crete/2022_summer_school)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
