{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**Machine Learning Practises**</font>\n",
    "\n",
    "    or: \"How I Learned to Stop Worrying and Love the Black Box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will focus on the fundamentals of the **Machine Learning** (**ML**) approach.<br>\n",
    "We will:\n",
    "\n",
    "- formalize some **concepts** encountered in previous classes\n",
    "- outline procedural **protocols**\n",
    "- highlight **good practises** and common **mistakes**\n",
    "\n",
    "$\\rightarrow$ The target is to understand <u>how to operate</u> \"the black box\" and make it transparent (_to us, not the referee :p_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Black Box problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_One step back $-$ what is **Machine Learning**?_\n",
    "<br>\n",
    "> A branch of _Artificial Intelligence_ (_AI_) which regards algorithms that can <u>improve their performance through experience and data</u>.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ML_AI.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 0.  The ML landscape.\n",
    "            <br>\n",
    "            (From <a href=\"https://www.researchgate.net/figure/Relationship-between-artificial-intelligence-AI-machine-learning-ML-and-deep_fig1_338083201\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML can be best understood when compared to the **traditional** approach:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/traditional_vs_ML.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.1.  The traditional approach (<i>top</i>) compared to the ML one (<i>bottom</i>).\n",
    "            <br>\n",
    "            (From <a href=\"https://github.com/ageron/handson-ml2\">Gerone, A. \"<i>Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow</i>\"</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Differently from the hard-coded approaches followed by traditional modelling, ML proposes a **train-application** scheme.\n",
    "\n",
    "The idea is that:\n",
    "- a machine (a computer) can learn the **parameters** of an arbitrarily complex model by **training** over some input **data**\n",
    "- the \"**Analize errors**\" task is performed by the machine itself\n",
    "- once the model is trained, the machine can predict the response for previously **unseen data**.\n",
    "\n",
    "- - -\n",
    "\n",
    "Hence, the **Black Box problem** arises:\n",
    "\n",
    "> _\"We will <u>always</u> get an output, but is that correct?\"_\n",
    "\n",
    "$\\rightarrow$ In order to evaluate our \"box\" we need to have:\n",
    "- complete control over **inputs** and **outputs**\n",
    "- a solid **assessement** protocol\n",
    "\n",
    "_This brings us to ..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ingredients for a successful ML model\n",
    "\n",
    "1. **Metrics of performance (_previous notebooks_)**\n",
    "    > _\"Wait $-$ is this a bug or a feature?\"_\n",
    "\n",
    "3. **Clean data (_this notebook_)**<br>\n",
    "    > _\"garbage in, garbage out\"_<br>\n",
    "    > _~literally any wannabe data analyst on youtube_\n",
    "\n",
    "3. **Protocols: training, validation, and test sets (_this & next notebook_)**\n",
    "    > _\"Divide [train de probatio] et impera\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "## Data visualization\n",
    "\n",
    "When possible, let's **see** the data, or at least their **statistical properties**.\n",
    "\n",
    "We will generate and observe some mock data with `sklearn.datasets.make_classification`:\n",
    "- very useful tool to **quickly test** your algos!\n",
    "- can set **properties** as the number of _samples_ and _features_, of the _informative_ and _redundant_ features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=3, n_informative=1,\n",
    "                           n_redundant=1, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1, weights=None, flip_y=0.01,\n",
    "                           class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick visualization: corner plot with seaborn\n",
    "\n",
    "`Seaborn` is a powerful tool for quick visualizations.\n",
    "- Deals **automatically** with plot limits etc.\n",
    "- **Wide range** of pre-set visualizations: _corner plots_, _violin plots_, etc.\n",
    "- Needs `pandas.DataFrames` as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corner plot:\n",
    "sns.pairplot(df_data, hue=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are:\n",
    "- redundant?\n",
    "- informative?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "We just need X$_{1}$ or X$_{2}$ for the classification of these data!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Correlation matrices\n",
    "\n",
    "We can see the _redundancy_ in a quantitative way, by calculating and displaying the **covariance** and the **correlation** between each pair of features.\n",
    "\n",
    "> **Covariance** $\\rightarrow Cov(X, Y)$ = ${{1}\\over{1-N}} \\sum_{i=1}^{N} (x_i - E(X))~(y_i - E(Y))$\n",
    ">\n",
    "> **Correlation** $\\rightarrow Cor(X, Y)$ = ${{Cov~(X, Y)}\\over{\\sigma_X ~ \\sigma_Y}}$ \n",
    "\n",
    "We can additionally plot the significance of the correlation, using the $p$-values from a Pearson correlation test (see Hypothesis Testing session!).  Using the `scipy.stats.personr` package ([link](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html)) words:\n",
    "\n",
    "\n",
    ">_The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "     \n",
    "# Covariance matrix:\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# Pearson's correlation and p-value matrices:\n",
    "#  See: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html\n",
    "cor   = np.zeros((np.shape(X)[1], np.shape(X)[1]))\n",
    "pvals = np.zeros((np.shape(X)[1], np.shape(X)[1]))\n",
    "\n",
    "for i in range(np.shape(X)[1]):\n",
    "    for j in range(np.shape(X)[1]):\n",
    "        cor[i, j], pvals[i, j] = pearsonr(X[:, i].ravel(), X[:, j].ravel())\n",
    "\n",
    "# Plotting -------------------------------------------------------------------\n",
    "\n",
    "# Configure a custom diverging colormap:\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Configure a custom diverging colormap:\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Converting matrices to dataframes for plotting with seaborn:\n",
    "def convert_M_to_pd(M, feature_names):\n",
    "    df = pd.DataFrame(data=M, columns=feature_names)\n",
    "    df = df.rename(index={i: feature_names[i] for i in range(len(feature_names))})\n",
    "    return df\n",
    "\n",
    "df_cov   = convert_M_to_pd(cov, feature_names)\n",
    "df_cor   = convert_M_to_pd(cor, feature_names)\n",
    "df_pvals = convert_M_to_pd(pvals, feature_names)\n",
    "\n",
    "# If you wish to mask the upper (redundant) traingle, uncomment this:\n",
    "# mask_cor = np.triu(np.ones_like(cor, dtype=bool))\n",
    "# mask_cov = np.triu(np.ones_like(cov, dtype=bool))\n",
    "mask_cor = None\n",
    "mask_cov = None\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 5]\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Covariance matrix\")\n",
    "sns.heatmap(df_cov, annot=True, mask=mask_cov, cmap=cmap)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Pearson's correlation matrix\")\n",
    "sns.heatmap(df_cor, annot=True, mask=mask_cor, cmap=cmap)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Correlation p-values matrix\")\n",
    "sns.heatmap(df_pvals, annot=True, mask=mask_cor, cmap=cmap)\n",
    "\n",
    "plt.show()\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of redundant features, we may apply a threshold on **covariance** or **correlation**, and remove one of the two redundant variables.\n",
    "\n",
    "However, watch out! $\\rightarrow$ These matrices only present the _linear_ relations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring datasets with many features\n",
    "\n",
    "Let's look at the same techniques seen above, when the number of features is **untractable**:\n",
    "\n",
    "<code>n_features</code> = 1000<br>\n",
    "<code>n_redundant</code> = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=1000, n_informative=700,\n",
    "                           n_redundant=300, n_repeated=0, n_classes=5,\n",
    "                           n_clusters_per_class=2, weights=None, flip_y=0.01,\n",
    "                           class_sep=3.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display _only the last 7 features_, since it is impractical to display them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This might take 10+ seconds to plot ...\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corner plot:\n",
    "g = sns.pairplot(df_data.iloc[:,-8:], hue=\"y\")\n",
    "g.fig.set_size_inches(10,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we cannot visualize the full correlation matrix $\\rightarrow$ we might just calculate it and **sort it** by correlation value.\n",
    "\n",
    "We might only print the significant correlations, e.g. |Cor| > 0.95.\n",
    "\n",
    "_Some other useful tricks can be found [here](https://likegeeks.com/python-correlation-matrix/)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cor = df_data[feature_names].corr()\n",
    "# NOTE1: This time we used a pandas tool to calculate the correlation\n",
    "#        matrix (while before we used the equivalent numpy tool) just\n",
    "#        to show an alternative method for the same thing.\n",
    "#        This returns a pandas dataframe.\n",
    "\n",
    "cor_pairs = df_cor.unstack()\n",
    "# converting correlation dataframe to 1D series\n",
    "\n",
    "cor_pairs_sort = cor_pairs.sort_values(kind=\"quicksort\")\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('Distribution of correlation coefficients')\n",
    "plt.hist(cor_pairs, bins=50)\n",
    "plt.show()\n",
    "\n",
    "df_sorted_pairs = pd.DataFrame(cor_pairs_sort).reset_index()\n",
    "df_sorted_pairs.columns=['feature_A', 'feature_B', 'Cor']\n",
    "\n",
    "# Let's sort by absolute value:\n",
    "df_sorted = df_sorted_pairs.sort_values(by='Cor', key=abs)\n",
    "\n",
    "# Let's keep the top <N> significant correlations:\n",
    "df_significant_5 = df_sorted.iloc[-5:, :].reset_index(drop=True)\n",
    "\n",
    "print('Top 5 Significant correlations (excluding same-variable):')\n",
    "display(df_significant_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hol'on! The top correlations are the variable with itself! $\\rightarrow$ We need a cleener view!\n",
    "\n",
    "### In-class Exercise: remove the same-variable correlations [2 mins]\n",
    "\n",
    "**Objective**: Display the correlation values distribution, but without the same-variable correlations.\n",
    "\n",
    "**Task**: You just have to remove the entries where **feature_A**\t== **feature_B** (i.e., the diagonal of the correlation matrix) from `df_sorted_pairs`.   It's just 1 or 2 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:1px'>\n",
    "\n",
    "_Our solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cor = df_data[feature_names].corr()\n",
    "# NOTE1: This time we used a pandas tool to calculate the correlation\n",
    "#        matrix (while before we used the equivalent numpy tool) just\n",
    "#        to show an alternative method for the same thing.\n",
    "#        This returns a pandas dataframe.\n",
    "\n",
    "cor_pairs = df_cor.unstack()\n",
    "# converting correlation dataframe to 1D series\n",
    "\n",
    "cor_pairs_sort = cor_pairs.sort_values(kind=\"quicksort\")\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('Distribution of correlation coefficients')\n",
    "plt.hist(cor_pairs, bins=50)\n",
    "plt.show()\n",
    "\n",
    "df_sorted_pairs = pd.DataFrame(cor_pairs_sort).reset_index()\n",
    "df_sorted_pairs.columns=['feature_A', 'feature_B', 'Cor']\n",
    "\n",
    "# Let's exclude same-variable correlations, which are 1 by definition:\n",
    "df_nonsame = df_sorted_pairs[ df_sorted_pairs['feature_A'] != df_sorted_pairs['feature_B'] ]\n",
    "\n",
    "# Let's sort by absolute value:\n",
    "df_sorted = df_nonsame.sort_values(by='Cor', key=abs)\n",
    "\n",
    "# Let's keep the top <N> significant correlations:\n",
    "df_significant_5 = df_sorted.iloc[-5:, :].reset_index(drop=True)\n",
    "\n",
    "print('Top 5 Significant correlations (excluding same-variable):')\n",
    "display(df_significant_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5>Ah!</font> Why there are no other correlations == 1?**\n",
    "\n",
    "Yet, we have _explicitly_ requested `sklearn.datasets.make_classification` to include redundant features! **Why this time we cannot spot the redundant features?!**\n",
    "\n",
    "$\\rightarrow$ Because a \"redundant\" feature might be a **linear combination** of the informative features!\n",
    "\n",
    "<u>SUMMARY:</u> Life is more complicated than simple 1-to-1 relationships, unfortunately, so we cannot easily spot redundant features ...\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/MITM.jpg\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "_PS: Additionally, remember that the correlation/covariance matrix is ill-defined (singular) for $n_{features}$ > $n_{samples}$.\n",
    "     A nice explanation can be found [here](https://stats.stackexchange.com/questions/60622/why-is-a-sample-covariance-matrix-singular-when-sample-size-is-less-than-number)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do, when we face a **large number** of features?\n",
    "> - Sit in the corner and cry\n",
    "> - Visualize data with some tricks (see [$\\S$Feature reduction](#Feature-reduction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling features\n",
    "\n",
    "NOTE: \"Scaling\" shall be interpreted as a generic form of **normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do you need to normalize?\n",
    "\n",
    "- Normalizing is <u>fundamental</u> when adopting algorithms that make use of **distance**.\n",
    "\n",
    "    _Example: **SVM**, **$k$NN**, or some **hierarchical clustering**_<br>\n",
    "    $k$NN Measures the distance between a _test_ point $\\hat{X}$ and all the _training_ points $X_i$: e.g. || $\\hat{X}$ -  ${X_i}$ ||$^2$\n",
    "  \n",
    "  \n",
    "- Normalizing is <u>fundamental</u> when adopting algorithms that **compare** the contribution of **features**.\n",
    "\n",
    "     _Example: **PCA**_<br>\n",
    "     PCA requires to scale and center features around 0 [$\\S$Feature reduction](#Feature-reduction)\n",
    "\n",
    "\n",
    "- Normalizing <u>is not necessary</u> with algorithms which look at each feature **independently**\n",
    "\n",
    "    _Example: **Random Forests**_<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "... However, normalizing features is usually a good habit because:\n",
    "\n",
    "- Scaling <u>helps</u> convergence when features have very different **dynamic ranges**.\n",
    " \n",
    "    _Example: algorithms using **gradient descent** (including **neural networks**)_<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Example**</u><font>\n",
    "\n",
    "Let's try to see what happens to the accuracy when we try a $k$NN classifier **with** and **without** normalizing.\n",
    "    \n",
    "> 3 classes, 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Let's create the usual mock data:\n",
    "X, y = make_classification(n_samples=1000, n_classes=3, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2, flip_y=0.5,\n",
    "                           shift=3, scale=None, random_state=42)\n",
    "# NOTE1: We set \"scale\" to \"None\" to generate randomly scale features!\n",
    "# NOTE2: flip_y=0.5 makes the classification more difficult!\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Let's fit directly:\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Accuracy on non-normalized data: %.2f' % clf.score(X_test, y_test))\n",
    "\n",
    "# Now, let's normalize with respect to the max of each feature, and re-fit:\n",
    "scaler = MinMaxScaler()\n",
    "X_train_n = scaler.fit_transform(X_train)\n",
    "X_test_n  = scaler.transform(X_test)\n",
    "\n",
    "clf_n = KNeighborsClassifier(n_neighbors=10)\n",
    "clf_n.fit(X_train_n, y_train)\n",
    "\n",
    "print('Accuracy on normalized data: %.2f' % clf_n.score(X_test_n, y_test))\n",
    "\n",
    "# Plotting decision boundaries:\n",
    "# See:\n",
    "#    https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf, X_train, cmap=cmap_light, ax=axes[0], response_method=\"auto\")\n",
    "\n",
    "axes[0].scatter(X_train[:,0], X_train[:,1], c=y_train, alpha=0.5)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf_n, X_train_n, cmap=cmap_light, ax=axes[1], response_method=\"auto\")\n",
    "\n",
    "axes[1].scatter(X_train_n[:,0], X_train_n[:,1], c=y_train, alpha=0.5)\n",
    "\n",
    "axes[0].set_title('Decision boundaries on native X_train')\n",
    "axes[1].set_title('Decision boundaries on normalized X_train')\n",
    "\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "axes[1].set_xlabel('X1$_n$')\n",
    "axes[1].set_ylabel('X2$_n$')\n",
    "\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of scalers\n",
    "\n",
    "Theoretically, infinite.\n",
    "\n",
    "The <code>sklearn</code> library implements several scalers:\n",
    "- check this [showcase of <code>sklearn</code>scalers](https://scikit-learn.org/stable/modules/preprocessing.html) \n",
    "- check the [effects of different scalers on outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) for their effects on oultiers\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1600>\n",
    "        <img src=\"images/Scalers_Showcase.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.1. A showcase of the effect of different <code>sklearn</code> scalers applied to different distributions.<br>\n",
    "            (From <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "<u>Basic transformations</u>\n",
    "\n",
    "- `MinMax`\n",
    "\n",
    "$$ X^{\\prime} = {X - X_{min} \\over X_{max} - X_{min}} $$\n",
    "\n",
    "- `StandardScaler`\n",
    "\n",
    "$$ X^{\\prime} = {X - <X> \\over \\sigma_X} $$\n",
    "\n",
    "\n",
    "- `QuantileTransformer`\n",
    "\n",
    "        Transforms the values to become a uniform or a normal distribution.\n",
    "\n",
    "Imports:\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    \n",
    "<u>Remember the golden rules:</u>\n",
    "\n",
    "    Train on train, apply on test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: do I need to normalize $y$ ?\n",
    "\n",
    "In regression problems, $y$ is a **continuous** variable (i.e., not a _label_, as for classification).\n",
    "\n",
    "Shall we normalize it?\n",
    "\n",
    "> **Depends on the specific problem** $\\rightarrow$ try it out.<br>\n",
    "> _e.g., you might want to fit better some ranges or equally at all ranges_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X, y, s=5)\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_reg_yyhat(y, yhat, title='Regression error',annot=None):\n",
    "\n",
    "    df_yyhat = pd.DataFrame(np.array([y.T, yhat.T, (yhat-y)/(y+1e-2)]).T,\n",
    "                            columns=[\"y\", \"yhat\", \"err_perc\"])\n",
    "    \n",
    "    lims = [min(-0.05, np.min(list(y) + list(yhat))),\n",
    "            max(1.05,  np.max(list(y) + list(yhat)))]\n",
    "\n",
    "    grid = sns.jointplot(data=df_yyhat, x=\"y\", y=\"err_perc\",\n",
    "                         xlim=lims, ylim=(-3, 3), height=3)\n",
    "    grid.plot_joint(sns.kdeplot, color=\"steelblue\", zorder=0, levels=6)\n",
    "        \n",
    "    grid.fig.suptitle(title, y=1.03, fontsize=16)\n",
    "    grid.ax_joint.set_xlabel('$y$', fontsize=14)\n",
    "    grid.ax_joint.set_ylabel('error [%]', fontsize=14)\n",
    "    \n",
    "    grid.fig.set_size_inches(5,5)\n",
    "    \n",
    "    grid.ax_joint.set_xlim(lims)\n",
    "    \n",
    "    # Annotation:\n",
    "    if annot:\n",
    "        grid.ax_joint.text(0.05, 0.95, annot, size=14, rotation=0, ha=\"left\",\n",
    "               va=\"center\", bbox=dict(boxstyle=\"round\", ec='grey', fc='white'),\n",
    "               transform=grid.ax_joint.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first fit without nomalizing y:\n",
    "\n",
    "reg = RandomForestRegressor().fit(X.reshape(-1,1), y)\n",
    "yhat = reg.predict(X.reshape(-1,1))\n",
    "\n",
    "plot_reg_yyhat(y, yhat, annot='Native $y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now re-fit, but first nomalizing y with a Quantile Transformer:\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=10, random_state=42)\n",
    "y_n = qt.fit_transform(y.reshape(1, -1).T).flatten()\n",
    "\n",
    "reg = RandomForestRegressor().fit(X.reshape(-1,1), y_n)\n",
    "yhat_n = reg.predict(X.reshape(-1,1))\n",
    "\n",
    "plot_reg_yyhat(y_n, yhat_n, annot='Quantile-transformed $y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature construction (dipoles, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Kaggle's [Stellar Classification Dataset - SDSS17](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_star = pd.read_csv(\"data/star_classification.csv\")\n",
    "df_star = df_star.sample(n=100, random_state=12)\n",
    "# keeping only a few objects to make the problem more difficult\n",
    "\n",
    "#display(df_star.head(5))\n",
    "\n",
    "print('Class demographics:')\n",
    "display(df_star.groupby(['class']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let's only use a few features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = df_star[['u', 'g', 'r', 'i', 'z', 'redshift']]\n",
    "y = np.array(df_star[['class']]).ravel()\n",
    "\n",
    "display(X.head(5))\n",
    "\n",
    "print('Number of features in X:', np.shape(X)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try a simple **Random Forests** classifier on the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Agnostic feature construcions**</u><font>\n",
    "\n",
    "There are many _agnostic_ methods, but one basic feature construction is to create **all polynomial combinations** of the features.\n",
    "\n",
    "E.g., the polynomials of degree 2 are:\n",
    "    \n",
    "> $X_1,~X_1^2,~X_1\\cdot{}X_2,~X_2^2,~X_2,$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ext = pd.DataFrame(poly.fit_transform(X))\n",
    "\n",
    "display(X_ext.head(5))\n",
    "\n",
    "print('New number of features in X:', np.shape(X_ext)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ext, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ _That's some improvement (we got 0.83, before)!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Prior knowledge feature construction**</u><font>\n",
    "\n",
    "What we did before by creating polynomial was to **relate** _magnitudes_ ('u', 'g', 'r', 'i', 'z') with each other and with a _distance_ ('redshift').\n",
    "    \n",
    "\n",
    "But, since we are cool and smart _Astronomers_, we could as well used our Astrophysics intuition to create **colors**, which we know are distance-_invariant_ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "df_mags = df_star[['u', 'g', 'r', 'i', 'z']]\n",
    "\n",
    "cc = list(combinations(df_mags, 2))\n",
    "df_colors = pd.concat([df_mags[c[0]].sub(df_mags[c[1]]) for c in cc], axis=1, keys=cc)\n",
    "df_colors.columns = df_colors.columns.map('-'.join)\n",
    "\n",
    "X_ext = df_colors\n",
    "\n",
    "display(X_ext.head(5))\n",
    "\n",
    "print('New number of features in X:', np.shape(X_ext)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ext, y, test_size=0.3, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5>Argh!</font>**\n",
    " We got a worst result than using the magnitudes themselves (0.83)!<br>\n",
    "_Well, Astronomy intuition is overrated, anyways!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different algorithms based on how many assumptions we want to make about the data.\n",
    "\n",
    "<font size=3><u>**Based solely on data properties**</u><font>\n",
    "    \n",
    "E.g., the <code>sklearn</code> selector <code>SlectKBest</code> allows to keep the best $k$ features based on some **statistical tests**.\n",
    "\n",
    "For example, we can use the $\\chi^2$ test to evaluate the dependency of $y$ from each feature, and select the top 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "features = ['u', 'g', 'r', 'i', 'z', 'run_ID', 'rerun_ID', 'cam_col']\n",
    "df_star_ = df_star[features+['class']]\n",
    "display(df_star_.head(10))\n",
    "\n",
    "X = df_star_[features]\n",
    "y = df_star_['class']\n",
    "\n",
    "X = np.abs(X)\n",
    "# NOTE: This is because the Chi2 test is assumed to work on counts or frequencies,\n",
    "#       hence it expects only positive values\n",
    "\n",
    "n_select = 2\n",
    "feature_selector = SelectKBest(chi2, k=n_select)\n",
    "\n",
    "X_reduced = feature_selector.fit_transform(X, y)\n",
    "\n",
    "# NOTE: Conveniently, the sklearn feature selectors have the same exact methods\n",
    "#       as the classifiers, regressors, or any other sklearn estimator:\n",
    "#         .fit()           -> learns the transformation\n",
    "#         .transform()     -> applies the transformation\n",
    "#         .fit_transform() -> learns and applies the transformation\n",
    "\n",
    "print('Top %s selected features:\\n\\t%s' %\\\n",
    "      (n_select, feature_selector.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Based on some performance**</u><font>\n",
    "    \n",
    "We want to keep the features which yield the best **performance**.<br>\n",
    "Yes, but which performance? $\\rightarrow$ the one of **an estimator of our choice**!\n",
    "(_e.g., for a classification problem, we need to pick a classifier._)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see the case of a **Sequential Selector**, and more specifically, of a **_forward_** feature selector, applied to a classification problem. \n",
    "\n",
    "**Sequential Forward Selection (SFS) algorithm:**\n",
    "\n",
    "The algorithm has to pick $p$ features among a set of input features $X = \\{x_1, x_2, ... x_d\\}$.<br>\n",
    "Let's call $J(F)$ the score of the classifier when applied to the feature subset $F \\subset X$.\n",
    "\n",
    "> 1. Start from an empty set: $Y_k\\{\\emptyset\\}$, $k=0$\n",
    ">\n",
    ">\n",
    "> 2. Select the best left-out feature:\n",
    ">\n",
    ">    $x^+$ = $arg max J(Y_k + x)$ where $x \\in X - Y_k$<br>\n",
    ">    _This is the feature that, when added, maximises the score J of the classifier_\n",
    "> \n",
    ">    $Y_{k+1} = Y_k + x^+$<br>\n",
    ">    _Add the selected feature to the pool_\n",
    ">\n",
    ">    $k = k + 1$<br>\n",
    ">    _Increase the counter_\n",
    ">\n",
    ">\n",
    "> 3. Stop if $k = p$, else go to 2\n",
    "\n",
    "The **_backward_** feature selector works similarly ...\n",
    "\n",
    "The Python library <code>mlxtend</code> contains several **Sequential Selectors**,  ([docs](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/); but check also their other algorithms [link](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/)).\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try <code>mlxtend</code> SFS using a <code>sklearn</code> classifier to evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "n_select = 2\n",
    "sfs = SFS(clf, k_features=n_select, forward=True, floating=False, verbose=2,\n",
    "          scoring='accuracy', cv=0)\n",
    "\n",
    "sfs = sfs.fit(X, y)\n",
    "# notice how mlxtend seamlessly integrates with sklearn ...\n",
    "\n",
    "print('Top %s selected features:\\n\\t%s' %\\\n",
    "      (n_select, [features[idx] for idx in list(sfs.k_feature_idx_)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Final remarks on feature selection**</u><font>\n",
    "    \n",
    "- Remember the **Golden Rule** $\\rightarrow$ Use only the **training set** to select (_not_ the _test_, for God's sake!)!\n",
    "\n",
    "- Shall I use a different classifier (estimator) to **select features** and to **classify**? $\\rightarrow$ Depends who you ask, both valid.\n",
    "\n",
    "- Sequential Selection is **computationally expensive** (looks at many feature combinations) $\\rightarrow$ May bu unfeasible for large datasets.\n",
    "\n",
    "<font size=3><u>**A much more advanced agorithm**</u><font>\n",
    "    \n",
    "[-] [Bourboudakis & Tsamardinos, 2017](https://arxiv.org/abs/1705.10770): \"_Forward-Backward Selection with Early Dropping_\"<br>\n",
    "\n",
    "> _Made in University of Crete!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature reduction $-$ PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, _visualizing multidimensonal data_ / _feature selection_ becomes quickly **untractable** as the number of features grows.\n",
    "\n",
    "How can we get an hint of, e.g. **outliers** and clusters?\n",
    "\n",
    "$\\rightarrow$ We can use tools that allow to \"summarize\" the features into a lower-dimensional space (**embedding**).<br>\n",
    "_For example, we can reduce to 2 or 3 dimensions, so that we can directly visualize the data._<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a \"easy\" dataset with lots of features ...\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=1000, n_informative=900,\n",
    "                           n_redundant=100, n_repeated=0, n_classes=5,\n",
    "                           n_clusters_per_class=1, weights=None, flip_y=0.01,\n",
    "                           class_sep=3.5, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "# NOTE: <class_sep> sets how far apart are the clusters: larger values produce\n",
    "#       easier sets, in the sense that they are more easily separable.\n",
    "\n",
    "classes = np.unique(y)\n",
    "print('There are %s classes' % len(classes))\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**What is PCA?**</u><font>\n",
    "    \n",
    "**_Principal Component Analysis_** (**PCA**) is an unsupervised method which finds the directions (**Principal Components; PCs**) which maximize the variance of the data.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1024>\n",
    "        <img src=\"images/PCA.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.2.  Detection of Principal Components ($b$) and reduction of dimensionality from 3D to 2D  ($c$).\n",
    "            <br>\n",
    "            (From <a href=\"https://www.davidzeleny.net/anadat-r/doku.php/en:pca\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<font size=3><u>**Algorithm**</u><font>\n",
    "\n",
    "The PCA theory reported here is mostly taken from this great paper: \n",
    "[Shlens, 2014, \"_A Tutorial on Principal Component Analysis_\"](https://arxiv.org/abs/1404.1100)\n",
    "\n",
    "- - -\n",
    "\n",
    "The algorithm essentially **diagonalizes** at the covariance matrix\n",
    "\n",
    "Covariance matrix:\n",
    "\n",
    "$$ C_X = {1 \\over n} XX^T$$\n",
    "\n",
    "We simply need to perform a _change of basis_:\n",
    "\n",
    "$$ X \\rightarrow Y,$$\n",
    "\n",
    "namely define a trasformation $P$: $Y = PX$, such that the covariance matrix of $Y$:\n",
    "\n",
    "$$ C_Y = {1 \\over n} YY^T$$\n",
    "\n",
    "is **diagonal**.\n",
    "\n",
    "We can prove that $P$ is matrix of the eigenvectors of $C_X$.\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "First, substitute  $Y = PX$ in $C_Y$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    C_Y =& {1 \\over n} YY^T\\\\\n",
    "        =& {1 \\over n} (PX)(PX)^T\\\\\n",
    "        =& {1 \\over n} P X X^T P^T\\\\\n",
    "        =& {1 \\over n} P (XX^T) P^T\\\\\n",
    "        =& P({1 \\over n}XX^T) P^T\\\\\n",
    "        =& P C_X P^T\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, the demonstration is completed by recognizing that any symmetric matrix is diagonalized by an orthogonal matrix of its **eigenvectors**.<br>\n",
    "\n",
    "$\\triangleright$ _Consult [Shlens, 2014](https://arxiv.org/abs/1404.1100) for the last step of the derivation._\n",
    "\n",
    "- - -\n",
    "\n",
    "<u>IMPORTANT:</u> We <u>must</u> first normalize the features or else the ones with the largest _dynamic range_ will dominate!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_n = StandardScaler().fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we can apply the PCA decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "embedding = pca.fit(X_n).transform(X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def plot_pca(pca, embedding, y=None):\n",
    "    '''\n",
    "    Plots the first 2 Principal Components (PCs) projections, along with PC\n",
    "    importance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pca : sklearn.decomposition.PCA\n",
    "        A trained PCA object.\n",
    "    embedding  : np.array\n",
    "        The data embeddings.\n",
    "    y : np.array (optional)\n",
    "        The embedding labels.\n",
    "    '''\n",
    "    fontsize_first  = 16\n",
    "    fontsize_second = 12\n",
    "\n",
    "    if y is None: y = np.zeros(len(embedding))\n",
    "\n",
    "    # Encoding labels in case they are categorical:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_e = le.fit_transform(y.ravel()).flatten()\n",
    "    \n",
    "    classes = le.classes_\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "    axes[0].set_title('Data embedded via PCA', fontsize=fontsize_first)\n",
    "    axes[0].set_xlabel('PC1', fontsize=fontsize_second)\n",
    "    axes[0].set_ylabel('PC2', fontsize=fontsize_second)\n",
    "    img = axes[0].scatter(embedding[:, 0], embedding[:, 1], s=20, c=y_e,\n",
    "                          cmap='tab20b', alpha=1.0)\n",
    "    plt.setp(axes[0], xticks=[], yticks=[])\n",
    "    cbar = plt.colorbar(img, boundaries=np.arange(len(classes)+1)-0.5, ax=axes[0])\n",
    "    cbar.set_ticks(np.arange(len(classes)))\n",
    "    cbar.set_ticklabels(classes)\n",
    "\n",
    "    ax2 = axes[1].twinx()\n",
    "    axes[1].set_title('PC importance', fontsize=fontsize_first)\n",
    "    axes[1].set_xlabel('PC index', fontsize=fontsize_second)\n",
    "    axes[1].set_ylabel('Explained variance [%]', fontsize=fontsize_second)\n",
    "    ax2.set_ylabel('Explained variance (cumulative) [%]', fontsize=fontsize_second, color='crimson')\n",
    "    axes[1].bar(np.arange(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_*100, width=0.8)\n",
    "    ax2.plot(np.cumsum(pca.explained_variance_ratio_)*100, color='crimson')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how our 100-dimensional data look like when embedded in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(pca, embedding, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty is $-$ the PCs are **sorted** by importance!\n",
    "\n",
    "We can look at the Explained variance and decide ow many to keep!<br>\n",
    "_E.g. the first **k** components that together explain 50% of the variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Physical intuition**</u><font>\n",
    "    \n",
    "Imagine you have a system which has an _intrinsical_ direction of movement:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=500>\n",
    "        <img src=\"images/PCA_intuition.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.3. A poorly-chosen set of features which could benefit from PCA to find the underlying direction of motion.\n",
    "            <br>\n",
    "            (From <a href=\"https://arxiv.org/pdf/1404.1100.pdf\">here</a>)<br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The X, Y, and Z variables **are correlated** by the motion along the spring axis.\n",
    "\n",
    "$\\rightarrow$ PCA might help us bypass the issue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Exercise: Run your own PCA [15 mins]\n",
    "\n",
    "**Objective**: Identify clusters in Kaggle's [Stellar Classification Dataset - SDSS17](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17).\n",
    "\n",
    "**Task**: You will have to apply PCA to the dataset (restricted to a few numerical values), project into the first two PCs, and identify the clusters (by eye). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_star = pd.read_csv(\"data/star_classification.csv\")\n",
    "df_star = df_star.sample(n=1000, random_state=12)\n",
    "# keeping only a few objects to speed up calculations\n",
    "\n",
    "print('Full dataset:')\n",
    "display(df_star.head(5))\n",
    "\n",
    "features = ['u', 'g', 'r', 'i', 'z', 'redshift']\n",
    "df_star_ = df_star[features]\n",
    "print('Dataset to run PCA on:')\n",
    "display(df_star_.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: When normalizing, remember that `sklearn` accepts dataframes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:1px'>\n",
    "\n",
    "_Our solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_star_n = StandardScaler().fit_transform(df_star_) \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "embedding = pca.fit(df_star_n).transform(df_star_n)\n",
    "\n",
    "plot_pca(pca, embedding, y=df_star['class'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mh, not a perfect class separation. If only it existed a **_non_-linear** PCA ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "PCA can use the **kernel trick** (just like in `SVM`) $\\rightarrow$ **Kernelization** applies non-linear transformations to the data, then performs PCA in the transformed space. \n",
    "\n",
    "How? $-$ <u>Intuition</u>\n",
    "\n",
    "The covariance matrix contain the multiplication of the data vectors among themselves:\n",
    "\n",
    "$$ C_X = {1 \\over n} = XX^T $$\n",
    "\n",
    "Now, you can imagine that if use a trasformation $\\Phi(.)$ to trasform the data to a new \"**feature space**\":\n",
    "\n",
    "$$ X \\rightarrow \\Phi(X)$$\n",
    "\n",
    "... in such feature space the covariance matrix will be composed of dot products of the type:\n",
    "\n",
    "$$~\\langle \\Phi(X_i), \\Phi(X_j)~\\rangle$$\n",
    "\n",
    "where $i$, $j$ are different samples.  \n",
    "The kernel trick allows us to express this as:\n",
    "\n",
    "$$~\\langle \\Phi(X_i), \\Phi(X_j)~\\rangle = K({X_i, X_j}) $$\n",
    "\n",
    "- - -\n",
    "\n",
    "<u>NOTE:</u> \n",
    "\n",
    "We <u>don't</u> define $\\Phi(.)$ and _then_ find the associated $K(.,.)$.\n",
    "\n",
    "On the contrary, we adopt a $K(.,.)$, and _assume_ there exists an associated $\\Phi(.)$ (_which we never use anyways_).\n",
    "\n",
    "$\\rightarrow$ in principle, $\\Phi(X)$ may belong to $\\mathbb{R}^\\infty$\n",
    "\n",
    "- - -\n",
    "\n",
    "E.g., **R**adial **B**asis **F**unction (**rbf**) kernel:\n",
    "> $$K({X_i, X_j}) = e^{|| X_i - X_j ||^2 \\over \\gamma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "#kernel_pca = KernelPCA(n_components=None, kernel=\"rbf\", gamma=10)\n",
    "#kernel_pca = KernelPCA(n_components=None, kernel=\"poly\" degree=2)\n",
    "kernel_pca = KernelPCA(n_components=None, kernel=\"cosine\")\n",
    "\n",
    "embedding = kernel_pca.fit(df_star_n).transform(df_star_n)\n",
    "\n",
    "plot_pca(pca, embedding, y=df_star['class'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>MPORTANT:</u> The result is _strongly dependent_ on the **hyperparameters**!\n",
    "(_play with rbf kernel and gamma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  In Class Exercise: Eigen-galaxies!\n",
    "\n",
    "**Objective**: Let's try and visualize the Principal Components!\n",
    "\n",
    "**Task**: Display the eigenvectors.\n",
    "\n",
    "**Dataset**: We will use a dataset of galaxy images, the [Galaxy10 DECals Dataset](https://astronn.readthedocs.io/en/latest/galaxy10.html), nicely packaged into a dataframe.\n",
    "\n",
    "Data type: images (21785 images)\n",
    "\n",
    "Classes<br>\n",
    " Class 0 (3461 images): Disk, Face-on, No Spiral<br>\n",
    " Class 1 (6997 images): Smooth, Completely round<br>\n",
    " Class 2 (6292 images): Smooth, in-between round<br>\n",
    " Class 3 (394 images): Smooth, Cigar shaped<br>\n",
    " Class 4 (1534 images): Disk, Edge-on, Rounded Bulge<br>\n",
    " Class 5 (17 images): Disk, Edge-on, Boxy Bulge<br>\n",
    " Class 6 (589 images): Disk, Edge-on, No Bulge<br>\n",
    " Class 7 (1121 images): Disk, Face-on, Tight Spiral<br>\n",
    " Class 8 (906 images): Disk, Face-on, Medium Spiral<br>\n",
    " Class 9 (519 images): Disk, Face-on, Loose Spiral<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "script_path = str(Path().absolute())\n",
    "print('Current path: %s\\n\\n' % script_path)\n",
    "\n",
    "path_data = script_path + '/data'\n",
    "\n",
    "# Downloading if dataset not found:\n",
    "if not os.path.isfile(path_data+\"/Galaxy10.h5\"):\n",
    "    url = \"http://astro.utoronto.ca/~bovy/Galaxy10/Galaxy10.h5\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(path_data+\"/Galaxy10.h5\", \"wb\").write(r.content)\n",
    "\n",
    "# Loading catalogue:\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "with h5py.File(path_data+\"/Galaxy10.h5\", \"r\") as F:\n",
    "    images = np.array(F[\"images\"])\n",
    "    labels = np.array(F[\"ans\"])\n",
    "    \n",
    "# Dropping class 5, which only has 17 galaxies:\n",
    "idxs_valid = np.where(labels != 5)[0]\n",
    "images = images[idxs_valid]\n",
    "labels = labels[idxs_valid]\n",
    "\n",
    "classes = sorted(np.unique(labels))\n",
    "print('There are %s classes' % len(classes))\n",
    "\n",
    "print('Image array shape: %s\\n' % str(np.shape(images)))\n",
    "\n",
    "# Displaying one image for each class:\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, len(classes), figsize=(14, 5))\n",
    "plt.suptitle('Galaxy class examples', y=0.7, fontsize=14)\n",
    "\n",
    "for i, class_ in enumerate(classes):\n",
    "    idx = np.where(labels == class_)[0][0]\n",
    "    # index of first galaxy in the list, for class <classs>\n",
    "    ax = axes[i]\n",
    "    ax.set_title('Class: %s' % class_)\n",
    "    ax.imshow(images[idx])\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.show()\n",
    "\n",
    "# Grabbing image properties:\n",
    "n_y, n_x, n_colors = np.shape(images)[1], np.shape(images)[2], np.shape(images)[3]\n",
    "# NOTE: Image dimensions are always transposed, when represented as matrices!\n",
    "print('Original shape of the images: (n_x = %s, n_y = %s, n_colors = %s)\\n\\n' %\\\n",
    "      (n_x, n_y, n_colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's \"**flatten**\" the dataset, so we can use it in `sklearn`:\n",
    "    \n",
    "    1 row == 1 image\n",
    "    1 pixel == 1 feature\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=500>\n",
    "        <img src=\"images/Flattened_Image.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.4. Re-arrangement of pixels into features to \"<i>flatten</i>\" an image.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Flattening images to 1D arrays (and casting to float):\n",
    "X_ = images.reshape([len(images), np.shape(images)[1]*np.shape(images)[2]*np.shape(images)[3]])\n",
    "X_ = X_.astype(np.float32)\n",
    "# TRICK: These are RGB images (integer values 0--256): we can convert to a\n",
    "#        low-encoding float (32 instead of e.g. 64 bits) without losing information.\n",
    "\n",
    "feature_names = [\"pixel\"+str(i) for i in range(np.shape(X_)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data_orig = pd.DataFrame(data=np.append(X_, labels[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Balancing classes using pandas:\n",
    "#\n",
    "# For each class, keeping as many objects as for the class with min number\n",
    "# of objects ...\n",
    "df_data_g = df_data_orig.groupby('y')\n",
    "df_data = pd.DataFrame(df_data_g.apply(lambda x: x.sample(df_data_g.size().min()).reset_index(drop=True)))\n",
    "# Keeping only class of galaxies to better understand the PCs:\n",
    "# df_data = df_data.loc[df_data['y'] ==7]\n",
    "# Re-exctracting the matrices after rebalancing ...\n",
    "X = df_data[feature_names].values\n",
    "y = df_data['y'].values\n",
    "\n",
    "print('Each class contains ~%s galaxies' % int(round(len(y)/len(classes))))\n",
    "\n",
    "# Keeping only a [random] subset of the data to speed up calculations:\n",
    "n_samples_keep = 1000 # max: len(X)\n",
    "idxs_keep = np.random.randint(len(X), size=n_samples_keep)\n",
    "X = X[idxs_keep]\n",
    "y = y[idxs_keep]\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(5))\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's perform the _linear_ PCA decomposition as usual ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_n = scaler.fit_transform(X) \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "embedding = pca.fit_transform(X_n)\n",
    "\n",
    "plot_pca(pca, embedding, y=y.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 mins]:**\n",
    "\n",
    "The `sklearn` PCA stores the eigenvectors (== _unit array giving the direction of the PC_) in:\n",
    "\n",
    "> `pca.components_`\n",
    "    \n",
    "and the eigenvalues (== _lenght of the PC vector_) in:\n",
    "\n",
    "> `pca.explained_variance_`\n",
    "    \n",
    "Each entry is 1 eigenvector eigenvalue.\n",
    "Try to print one, to see the content:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:1px'>\n",
    "\n",
    "_Our solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first eigenvector is:')\n",
    "print(pca.components_[0])\n",
    "print('... it has %s dimensions ...' % len(pca.components_[0]))\n",
    "print('... and its eigenvalue is: %s' % pca.explained_variance_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [10 mins]**:\n",
    "\n",
    "\n",
    "Let's now plot the first 10 eigenvectors.\n",
    "\n",
    "They are images, sooo ... we shall reshape them to (`n_x`, `n_y`, `n_colors`) plot them right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eigenvectors_to_plot = 10\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(n_eigenvectors_to_plot):\n",
    "    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
    "    \n",
    "    eigenvector_i = ...\n",
    "    # eigenvector == unit_vector * size\n",
    "\n",
    "    # Scaling to [0, 1] for visualization purposes:\n",
    "    eigenvector_i = MinMaxScaler().fit_transform(eigenvector_i.reshape(-1,1))\n",
    "    \n",
    "    ax.imshow(pca.components_[i].reshape(..., ..., ...),\n",
    "              cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:2px'>\n",
    "\n",
    "_Our solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "n_eigenvectors_to_plot = 10\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(n_eigenvectors_to_plot):\n",
    "\n",
    "    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
    "    \n",
    "    eigenvector_i = pca.components_[i] * pca.explained_variance_[i]\n",
    "    # eigenvector == unit_vector * size\n",
    "    \n",
    "    # Scaling to [0, 1] for visualization purposes:\n",
    "    eigenvector_i = MinMaxScaler().fit_transform(eigenvector_i.reshape(-1,1))\n",
    "\n",
    "    ax.set_title('Eigen-galaxy %i' % i)\n",
    "    ax.imshow(eigenvector_i.reshape(n_x, n_y, n_colors),\n",
    "              cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ We can think of PCs as the _basic_ \"**ingredients**\" composing a galaxy.<br>\n",
    "_&emsp; - PC1 maps a the intensity scale_<br>\n",
    "_&emsp; - subsequent PCs map finer and finer morphological details_\n",
    "\n",
    "The \"**recipe**\" to obtain galaxy _XYZ_ is to mix the ingredients using the \"**dosage**\" given by the _embedding values_.\n",
    "\n",
    "- - -\n",
    "\n",
    "More generally, PCA **decomposition** allows to break data along its _significant_ components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Data Preparation\n",
    "\n",
    "Aside from **pre-processing** practices seen here, more exist $-$ including:\n",
    "\n",
    "- **Non-linear feature reduction**<br>\n",
    "  &emsp; &emsp; _e.g., t-SNE, UMAP_ $\\rightarrow$ [2022 Summer School for Astrostatistics in Crete - ML Practices\n",
    "](https://github.com/astrostatistics-in-crete/2022_summer_school/blob/main/ML_Practices/ML_Practices_S.ipynb)\n",
    "- **Feature encoding**<br>\n",
    "  &emsp; &emsp; _e.g., One Hot Encoding_ $\\rightarrow$ [2022 Summer School for Astrostatistics in Crete - ML Practices\n",
    "](https://github.com/astrostatistics-in-crete/2022_summer_school/blob/main/ML_Practices/ML_Practices_S.ipynb)\n",
    " \n",
    "- **Inputing missing values**<br>\n",
    "  &emsp; &emsp; _e.g., missingno, MICE and its variants_  $\\rightarrow$\n",
    "  [`missingno`](https://github.com/ResidentMario/missingno),\n",
    "  [`sklearn.impute.IterativeImputer`](https://scikit-learn.org/stable/modules/impute.html),\n",
    "  [`miceforest`](https://pypi.org/project/miceforest/)\n",
    "\n",
    "- **Dealing with imbalanced classes**<br>\n",
    "  &emsp; &emsp; _e.g., SMOTE_  $\\rightarrow$[`imblearn.over_sampling.SMOTE`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)\n",
    "  \n",
    "- **Removing outliers** (?)\n",
    "\n",
    "- **Handling numerical exceptions** (zeros, NaNs!)\n",
    "\n",
    "... and many more!\n",
    "\n",
    "Have a look at this [post about Data Cleaning](https://towardsdatascience.com/automated-data-cleaning-with-python-94d44d854423) as a starting point! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocols\n",
    "\n",
    "> **Protocols** = Correct methodologies to assess out ML model\n",
    "\n",
    "Let's see a few, in order of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 0 $-$ Comparing against baseline\n",
    "\n",
    "> _Which is the minimum achievable score?_\n",
    "\n",
    "<table><tr>\n",
    "    <td width=200>\n",
    "        <img src=\"images/Goku_Young.jpg\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Let's consider a simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "x1 = np.random.randint(10, size=10)\n",
    "x2 = np.random.randint(10, 20, size=10)\n",
    "y = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "df_data = pd.DataFrame(np.array([x1, x2, y]).T, columns=['x1', 'x2', 'class'])\n",
    "\n",
    "display(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df_data[['x1','x2']].values\n",
    "y = df_data[['class']].values.ravel()\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('LogisticRegression | accuracy: %.2f' % clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this result?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>    \n",
    "    \n",
    "A **Majority Class** classifier, i.e. a _trivial_ classifier which always returns the most frequent class seen in training, would have scored 90% accuracy!\n",
    "\n",
    "<table><tr>\n",
    "    <td width=200>\n",
    "        <img src=\"images/Yamcha.jpg\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Always compare against dummy estimators!**</u><font>\n",
    "\n",
    "... or at least keep in mind the minimum achievable score $=$ **baseline**.  Having a comparison reference is <u>very important</u> when publishing, to convince about the goodness of your model.\n",
    "    \n",
    "In some cases, the baseline can also be found **theoretically**, it does not matter. \n",
    "\n",
    "Some examples of dummy estimators:\n",
    "    \n",
    " - <u>Classification</u>:\n",
    "   - **Random (Trivial) Classifier**: a classifier which returns a random class\n",
    "   - **Stratified Classifier**: a classifier which returns a class with a probability following the distribution of classes seen in training\n",
    "    \n",
    "    [<code>sklearn</code> Dummy Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n",
    "    \n",
    "        \n",
    " - <u>Regression</u>:\n",
    "   - **Constant Regressor**: a regressor which alwyas returns a value, e.g. the mean ($y_{train}$) or median ($y_{train}$)\n",
    "    \n",
    "    [<code>sklearn</code> Dummy Regressors](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n",
    "\n",
    "... or create your own based on the problem you are addressing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 $-$ Cross Validation (a.k.a. $k$-fold Cross Validation)\n",
    "\n",
    "> _Am I overfitting the train set?_\n",
    "\n",
    "<table><tr>\n",
    "    <td width=200>\n",
    "        <img src=\"images/Goku.png\">\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the basic **train/[validation]/test** split:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Train_Test.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.1.  Indicative recipe for splitting a dataset into the train and test sets.\n",
    "            <br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "_Would the results change if we shuffled the data differently?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Cross Validation**</u><font>\n",
    "\n",
    "It involves _re-fitting_ the model over different partitionings of the dataset:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/CV_k4.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.2.  Data splitting for a Cross Validation with $k = 4$ folds.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "A **$k$-fold CV** splits the data in $k$ parts (**folds**), and, at each iteration:\n",
    "   - uses **$k$-1** folds for training\n",
    "   - uses the **remaining** fold for testing (_although we indistinctively call it **validation** set, in this context_)\n",
    "\n",
    "The model is tested $k$ times, always on unseen data.\n",
    "\n",
    "In this way, CV actually tackles 2 issues:\n",
    "\n",
    "- Assessment of **bias**<br>\n",
    "  _(i.e. how much the model depends on a specific training set)_\n",
    "\n",
    "- Exploitation of the **whole dataset**<br>\n",
    "  _(i.e. all data are eventually used for both training and testing)_\n",
    "\n",
    "Each model fit produces a performance score, therefore we can obtain both an **average** estimate, and a **uncertainty** on it.\n",
    "\n",
    "<u>IMPORTANT:</u> While the CV gives performance estimates, the final model shall be **_re_-trained on all** the data.<br>\n",
    "$~~~~~~~~~~~~~~~~~~~$ We can reasonably assume that the re-trained model will be at least as performing as the CV average.<br>\n",
    "$~~~~~~~~~~~~~~~~~~~$ (Actually, you shall _re_-fit on all data also when you use the simpler train/validation/test protocol).\n",
    "\n",
    "- - -\n",
    "\n",
    "**Q:** What is the right $k$?<br> \n",
    "**R:** Common values range between 3 and 10, but depends on:\n",
    "- how many data we have<br>\n",
    "- how much time we have<br>\n",
    "\n",
    "because for larger $k$ we got more estimates, but in exchange we get:\n",
    "\n",
    "- a smaller test set at each iteration\n",
    "- more computations\n",
    "\n",
    "\n",
    "<font size=3><u>**Variants**</u><font>\n",
    "\n",
    "- **Repeated CV**: CV, but repeated $n$ times with different shuffling\n",
    "\n",
    "- **Stratified CV**: CV, but the demographics of each fold is representative of the data (e.g. same class demoographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Exercise: Make your own CV [20 mins]\n",
    "\n",
    "**Objective**: Display the distribution of scores of 2 classifiers.\n",
    "\n",
    "**Task**: You will have to:\n",
    "\n",
    "1. Pick 2 classifiers of your liking (e.g., see alist [here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html))\n",
    "2. Loop over the CV iterations\n",
    "3. Inside ach iteration:\n",
    "    - Split the data in train and test\n",
    "    - Fit both classifiers\n",
    "    - Assess the accuracy of both classifiers\n",
    "    - Store the result for the curent iteration\n",
    "4. Create a plot showing the distributions of results \n",
    "\n",
    "<u>Hint</u>: Use a CV with at least 10 folds (_increment, if you like, once the code works_), and $-$ in the final plot $-$ use 5 bins (_or else it would be too coarse)_.\n",
    "\n",
    "**Dataset**: We will use  Kaggle's [Stellar Classification Dataset - SDSS17](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17), restricted to just a few magnitudes.<br>\n",
    "The target of the classification is the object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for our exercise:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23441</th>\n",
       "      <td>22.30567</td>\n",
       "      <td>21.43649</td>\n",
       "      <td>19.75798</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90125</th>\n",
       "      <td>17.91316</td>\n",
       "      <td>16.73492</td>\n",
       "      <td>16.57163</td>\n",
       "      <td>STAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57892</th>\n",
       "      <td>23.00886</td>\n",
       "      <td>22.49020</td>\n",
       "      <td>20.71650</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>19.80938</td>\n",
       "      <td>17.89119</td>\n",
       "      <td>16.83623</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99545</th>\n",
       "      <td>24.32647</td>\n",
       "      <td>23.16088</td>\n",
       "      <td>21.35538</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              u         g         r   class\n",
       "23441  22.30567  21.43649  19.75798  GALAXY\n",
       "90125  17.91316  16.73492  16.57163    STAR\n",
       "57892  23.00886  22.49020  20.71650  GALAXY\n",
       "7228   19.80938  17.89119  16.83623  GALAXY\n",
       "99545  24.32647  23.16088  21.35538  GALAXY"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_star = pd.read_csv(\"data/star_classification.csv\")\n",
    "df_star = df_star.sample(n=1000, random_state=12)\n",
    "# keeping only a few objects to speed up calculations\n",
    "\n",
    "#print('Full dataset:')\n",
    "#display(df_star.head(5))\n",
    "\n",
    "features = ['u', 'g', 'r']#, 'i', 'z', 'redshift']\n",
    "df_star_ = df_star[features+['class']]\n",
    "print('Dataset for our exercise:')\n",
    "display(df_star_.head(5))\n",
    "\n",
    "X = df_star_[features].values\n",
    "y = df_star_['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = 10\n",
    "# number of CV folds\n",
    "\n",
    "# Splitting the folds:\n",
    "#   The `KFold` method allows us to directly get the train and test indexes\n",
    "#   across the iterations, sparing us the tedious index tracking\n",
    "kf = KFold(n_splits=cv)\n",
    "kf.get_n_splits(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############################################\n",
    "# Replace \"...\" with the appropriate code! #\n",
    "############################################\n",
    "\n",
    "from sklearn ... # import classifier 1\n",
    "from sklearn ... # import classifier 2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining the classifiers of choice:\n",
    "clf_1 = ...\n",
    "clf_2 = ...\n",
    "\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "# list of scores across the iterations\n",
    "\n",
    "# CV iterations:\n",
    "for k, (idxs_train, idxs_test) in enumerate(kf.split(X)):\n",
    "    print('Iteration %s' % (k+1))\n",
    "\n",
    "    X_train = ...\n",
    "    X_test  = ...\n",
    "    y_train = ...\n",
    "    y_test  = ...\n",
    "    \n",
    "    print('\\t- Fitting')\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    print('\\t- Predicting labels')\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    print('\\t- Assessing performance and storing')\n",
    "    score_1 = ...\n",
    "    score_2 = ...\n",
    "    \n",
    "    scores_1.append(score_1)\n",
    "    scores_2.append(score_2)\n",
    "\n",
    "print('Re-fitting on all the available data')    \n",
    "# NOTE: We don't need it later, it is just here as a reminder of good practice.\n",
    "...\n",
    "...\n",
    "\n",
    "print('Plotting score distributions and their average values')    \n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:1px'>\n",
    "\n",
    "_Our solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 2\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 3\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 4\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 5\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 6\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 7\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 8\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 9\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Iteration 10\n",
      "\t- Fitting on train fold(s)\n",
      "\t- Predicting on test fold\n",
      "\t- Assessing performance and storing\n",
      "Re-fitting on all the available data\n",
      "CPU times: user 3.62 s, sys: 16.2 ms, total: 3.63 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining the classifiers of choice:\n",
    "clf_1 = svm.SVC(kernel='linear', C=5)\n",
    "clf_2 = RandomForestClassifier()\n",
    "\n",
    "scores_1 = []\n",
    "scores_2 = []\n",
    "# list of scores across the iterations\n",
    "\n",
    "# CV iterations:\n",
    "for k, (idxs_train, idxs_test) in enumerate(kf.split(X)):\n",
    "    print('Iteration %s' % (k+1))\n",
    "\n",
    "    X_train = X[idxs_train]\n",
    "    X_test  = X[idxs_test]\n",
    "    y_train = y[idxs_train]\n",
    "    y_test  = y[idxs_test]\n",
    "    \n",
    "    print('\\t- Fitting on train fold(s)')\n",
    "    clf_1.fit(X_train, y_train)\n",
    "    clf_2.fit(X_train, y_train)\n",
    "\n",
    "    print('\\t- Predicting on test fold')\n",
    "    yhat_test_1 = clf_1.predict(X_test)\n",
    "    yhat_test_2 = clf_2.predict(X_test)\n",
    "    \n",
    "    print('\\t- Assessing performance and storing')\n",
    "    score_1 = accuracy_score(y_test, yhat_test_1)\n",
    "    score_2 = accuracy_score(y_test, yhat_test_2)\n",
    "    # NOTE: We could just as well use the `clf.score()` method, but better\n",
    "    #       to always explicitly specify the score we pick.\n",
    "    \n",
    "    scores_1.append(score_1)\n",
    "    scores_2.append(score_2)\n",
    "\n",
    "print('Re-fitting on all the available data')    \n",
    "# NOTE: We don't need it later, it is just here as a reminder of good practice.\n",
    "clf_1_final = clf_1.fit(X, y)\n",
    "clf_2_final = clf_2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting score distributions and their average values\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Plotting score distributions and their average values')    \n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(scores_1, bins=5, color='C0', alpha=0.5)\n",
    "plt.hist(scores_2, bins=5, color='C1', alpha=0.5)\n",
    "plt.axvline(x=np.mean(scores_1), ls='--', lw=5, color='steelblue',\n",
    "            label='clf$_{1}$ (mean)')\n",
    "plt.axvline(x=np.mean(scores_2), ls='--', lw=5, color='tomato',\n",
    "            label='clf$_{2}$ (mean)')\n",
    "plt.xlabel('Accuracy score', fontsize=14)\n",
    "plt.ylabel('Number density', fontsize=14)\n",
    "plt.legend(fontsize=14, bbox_to_anchor=(1., 1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5>Ah!</font> As you can see, the result _does_ depend on the split!**\n",
    "\n",
    "Without CV, i.e. if we only picked 1 specific train/test split $-$ and with some bad luck $-$ we might have got to [<u>wrong</u>] the conclusion that SVM performed better than RF.\n",
    "\n",
    "<u>IMPORTANT:</u> The **best model** is the one that **generalizes best**.\n",
    "\n",
    "- - -\n",
    "\n",
    "### Final remarks on CV\n",
    "\n",
    "Clearly, we do not need to reinvent the wheel $\\rightarrow$ `sklearn.model_selection.cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "print('Scores for classifier %s over the CV:' % clf)\n",
    "print('\\t', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>Summary</u></font>\n",
    "\n",
    "- CV can be used to assess the **generalized behaviour** of an estimator\n",
    "\n",
    "- Can I use CV to **compare** 2 estimators and pick the best? $\\rightarrow$ **YES.**\n",
    "\n",
    "- Can I use CV to **compare** 2 estimators, pick the best, <u>and</u> claim its average score? $\\rightarrow$ **NO!** (_see next notebook_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
