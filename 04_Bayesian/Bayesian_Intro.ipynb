{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf760e4",
   "metadata": {},
   "source": [
    "<font size=6>**Introduction to Bayesian Statistics**</font>\n",
    "\n",
    "\n",
    "# 1. The problem: Counting photons...\n",
    "\n",
    "Typically, an X-ray imaging observation gives a collection of **events**: coordinates and energies of **individual photons** detected by the telescope. Essentially, we count photons, and this is why we usually refer to the photons as **counts**, while the **count rate** informs us about the number of photons per second. In general, the count rate scales with the flux of the field (or source) in the energy band of the telescope during the observation.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/NGC1482_opt.jpg\" alt=\"Drawing\" style=\"height:400px;\"/> </td>\n",
    "<td> <img src=\"images/NGC1482.png\" alt=\"Drawing\" style=\"height: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Using **source detection algorithms** we group photons together and generate a list of sources and the photons we got from each - namely, the *source region*. For a given source, the integrated energy of the photons from its source region gives us an estimate on the energy collected from the telescope. Of course, the response of the detector and absorption effects should be accounted for. Having many counts allows us to fit for the spectrum of the source. Modeling the detector properties, the intergalactic absorption and the spectra allows us to measure the *bolometric* flux of the source - the flux as if we could observe the source with a perfect detector. Provided we know the distance of the source we can convert from flux to luminosity... an intrinsic property of the source (which is what we usually care for)!\n",
    "\n",
    "## 1.1. Estimate the flux of a source\n",
    "\n",
    "Using the Chandra X-ray Observatory, we found a source that emitted 5 counts in an observation of 1ks of the galaxy NGC 1482. What is the luminosity of the source?\n",
    "\n",
    "* Using a typical model for the spectrum of an X-ray source (power law with index=1.7), and Galactic absorption $N_{\\mathrm{H}}=4.07\\times10^{20}\\,\\mathrm{cm}^{-2}$ (corresponding to the direction of NGC 1482 galaxy), as well as the characteristics of the telescope, we found that: 1 count/s corresponds to a flux of $2.851\\times 10^{-11}$ erg/s/cm$^2$.\n",
    "* The distance of NGC 1482 is estimated to be 19.6 Mpc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db421a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_time_in_seconds = 1.0e3   # from the duration of the observation\n",
    "distance_in_Mpc = 19.6             # from literature\n",
    "counts_to_flux_factor = 2.851e-11  # from X-ray observers' tools (e.g., Chandra PIMMS tool)\n",
    "counts = 5                         # from source detection\n",
    "\n",
    "\n",
    "counts_per_second = counts / exposure_time_in_seconds\n",
    "print(f\"Count rate : {counts_per_second:10.3g} count/s\")\n",
    "\n",
    "source_flux = counts_to_flux_factor * counts_per_second\n",
    "print(f\"Source flux: {source_flux:10.3g} erg/s/cm^2\")\n",
    "\n",
    "flux_to_luminosity = 1.19e50 * distance_in_Mpc ** 2.0\n",
    "source_luminosity = source_flux * flux_to_luminosity\n",
    "print(f\"Luminosity : {source_luminosity:10.3g} erg/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8863d",
   "metadata": {},
   "source": [
    "## 1.2. What is the uncertainty on this luminosity?\n",
    "\n",
    "When counting things that are not correlated together, we expect a **Poisson distribution**. If an X-ray source emits on average $\\lambda$ photons that can be detected by our telescope during the observation (accounting for absorption, instrumentation, etc.) then the probability to detect any number of photons is given by the **probability mass function** of the Poisson-distributed random variable $X$ with parameter $\\lambda$:\n",
    "\n",
    "$$\\large \\mathrm{Pois}(k; \\lambda) = \\dfrac{ \\lambda^k e^{-\\lambda} }{ k! } $$\n",
    "\n",
    "Conveniently, the parameter $\\lambda$ is also the **expected value** or the mean of the Poisson distribution:\n",
    "\n",
    "$$\\large \\mu = \\mathrm{E}[X] = \\sum_{k=0}^{\\infty} k \\times \\mathrm{Pois}(k; \\lambda) = \\cdots = \\lambda $$\n",
    "\n",
    "The standard deviation of the distribution, which is a measure of the *spread of the random samples from the expected value* is the square root of the variance which is equal to $\\lambda$:\n",
    "\n",
    "$$\\large \\sigma = \\sqrt{\\mathrm{Var}[X]} = \\sqrt{ \\mathrm{E}\\left[\\left(X-\\mathrm{E}[X]\\right)^2\\right] } = \\cdots = \\sqrt{\\lambda} $$\n",
    "\n",
    "### What most of us have in mind when thinking of uncertainties, sigmas, etc.\n",
    "To put things in perspective, in the commonly-appeared/used normal (or Gaussian) distribution, **within one standard deviation** from the expected value lies the $\\sim68\\%$ of the samples:\n",
    "\n",
    "<img src=\"images/normal_dist.png\" alt=\"Drawing\" style=\"height:400px;\"/>\n",
    "\n",
    "### Let's see the Poisson distribution with $\\lambda = 5$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "lam = 5.0\n",
    "k = np.arange(0, 20)\n",
    "Pk = st.poisson(lam).pmf(k)\n",
    "\n",
    "in_CI = (k >= lam-lam**0.5) & (k <= lam+lam**0.5)\n",
    "fraction_in_CI = f\"{100.0 * sum(Pk[in_CI]):.1f}%\"\n",
    "\n",
    "real_interval = st.poisson(lam).interval(0.68)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.bar(k[in_CI], Pk[in_CI], color=\"r\", label=fraction_in_CI+r\" in $\\left[5-\\sqrt{5}, 5+\\sqrt{5}\\right]$\")\n",
    "plt.bar(k[~in_CI], Pk[~in_CI], color=\"b\")\n",
    "\n",
    "plt.axvline(real_interval[0], color=\"k\", ls=\":\", lw=2, label=\"68% CI\")\n",
    "plt.axvline(real_interval[1], color=\"k\", ls=\":\", lw=2)\n",
    "\n",
    "plt.ylabel(\"Probability mass (unitless)\")\n",
    "plt.xlabel(\"Number of events, counts, sources, etc.\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2d502",
   "metadata": {},
   "source": [
    "### What is the uncertainty on the source luminosity according the Poisson-nature of the counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b95131",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_uncertainty = counts ** 0.5\n",
    "luminosity_uncertainty = flux_to_luminosity * counts_to_flux_factor * counts_uncertainty / exposure_time_in_seconds\n",
    "print(f\"Uncertainty on luminosity: {luminosity_uncertainty:.3g} erg/s\")\n",
    "print(f\"We may report: ({source_luminosity/1.e39:.3g} +/- {luminosity_uncertainty/1.e39:.3g}) x 10^39 erg/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4e71e",
   "metadata": {},
   "source": [
    "## 1.3. Are we using the Poisson nature of the counts the correct way?\n",
    "\n",
    "The uncertainy is calculated as the square root of the mean value of the Poisson distribution. **But we don't know the expected value!** We have only observed one sample from the Poisson distribution.\n",
    "\n",
    "> We can get 5 counts if the mean value is 5, but also in the case of any different mean value!\n",
    "\n",
    "## Exercise 1: Pick three different natural numbers for the  mean of the Poisson (including 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis_maximum = 12\n",
    "poisson_means = [..., 5, ...]\n",
    "\n",
    "plt.figure()\n",
    "for mean_value in poisson_means:\n",
    "    k = np.arange(int(xaxis_maximum)+1)\n",
    "    Pk = st.poisson(mean_value).pmf(k)\n",
    "    plt.plot(k, Pk, \"o:\", label=f\"$\\lambda$ = {mean_value}\")\n",
    "plt.axvline(5, color=\"0.5\", label=\"Observed\")\n",
    "plt.ylabel(\"Probability mass\")\n",
    "plt.xlabel(\"Number of counts\")\n",
    "plt.ylim(ymax=0.25)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Probability of 5 counts with different $\\lambda$ parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847633d",
   "metadata": {},
   "source": [
    "## But isn't the observation a good estimate on the mean value?\n",
    "\n",
    "Someone could argue that the peak of the distribution is the most probable value. Therefore, measuring one sample provides some information on the mean value, and consequently on the standard deviation (since the two are related in the Poisson distribution)\n",
    "\n",
    "### However...\n",
    "\n",
    "> Many different parameters in a distribution may result in the same peak value!\n",
    "\n",
    "The peak of Pois(5) is both at 4 and 5! So, if we detected 4 counts, should we have inferred that $\\lambda=5$ instead of $\\lambda=4$?\n",
    "\n",
    "Similarily, the peak of Pois(6) is both at 5 and 6!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0de256",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis_maximum = 12\n",
    "poisson_means = [4, 5, 6]\n",
    "\n",
    "plt.figure()\n",
    "for mean_value in poisson_means:\n",
    "    k = np.arange(int(xaxis_maximum)+1)\n",
    "    Pk = st.poisson(mean_value).pmf(k)\n",
    "    plt.plot(k, Pk, \"o:\", label=f\"$\\lambda$ = {mean_value}\")\n",
    "plt.axvline(5, color=\"0.5\", label=\"Observed\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Number of counts\")\n",
    "plt.ylim(ymax=0.25)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Probability of 5 counts with 'nearby' $\\lambda$ parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c5391",
   "metadata": {},
   "source": [
    "## Exercise 2: $\\lambda$ is a real number!\n",
    "\n",
    "Pick a few values close to 5 to see the shape of the Poisson distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a04f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis_maximum = 12\n",
    "poisson_means = [..., 5.0, ...]\n",
    "\n",
    "plt.figure()\n",
    "for mean_value in poisson_means:\n",
    "    k = np.arange(int(xaxis_maximum)+1)\n",
    "    Pk = st.poisson(mean_value).pmf(k)\n",
    "    plt.plot(k, Pk, \"o:\", label=f\"$\\lambda$ = {mean_value}\")\n",
    "plt.axvline(5, color=\"0.5\", label=\"Observed\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Number of counts\")\n",
    "plt.ylim(ymax=0.25)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Probability of 5 counts with $\\lambda$ parameters close to 5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927a57e",
   "metadata": {},
   "source": [
    "# 2. The Bayesian probability\n",
    "\n",
    "It's all about our interpretation of probability, and how we represent knowledge. \n",
    "\n",
    "## The classic (frequentist) and the *Bayesian way*?\n",
    "\n",
    "> A frequentist assigns probabilities to data - parameters are fixed. The probability is a *frequency* of data outcomes.\n",
    "\n",
    "We did the above analysis assuming the source counts are equal to the detected ones, and assigned probabilities to different data outcomes. Our inference on the source luminosity and its uncertainty is telling us that if we got a good estimate from the observation, *repeating the experiment many times* we will find the above variation in the luminosity.\n",
    "\n",
    "**However, we have no indication that the source will continue shining! The \"repetition\" part of the frequentist interpretation is often unrealistic!**\n",
    "\n",
    "> A Bayesian assigns probabilities to hypotheses. The probability is a *degree of belief* in a value of a parameter.\n",
    "\n",
    "- The data is not a random variable. We made a measurement and we know it with absolute certainty. Instead, we want to understand the physical processes producing the data we got.\n",
    "- Knowing the result from repeating the experiment is very specific: maybe we care about what was the luminosity at that given period of time. This is not a repeatable situation!\n",
    "\n",
    "![image from Sivia](images/Sivia_Logic.png)\n",
    "\n",
    "The intrinsic source luminosity (and therefore the expected counts) is unknown and we want to infer it from the data. Therefore we ask: what is the probability that our source has an average count $\\lambda$ given the data (detected number of counts) for all possible values of $\\lambda$ (or hypotheses)?\n",
    "\n",
    "$$\\large P(\\lambda | k) = \\mathrm{?}$$\n",
    "\n",
    "## Probability calculus (Freq. and Bays. alike)\n",
    "\n",
    "How do we begin to calculate the above probability?\n",
    "\n",
    "When we are dealing with probabilties, there are useful notations and rules:\n",
    "\n",
    "$ P(A) $ is a number between 0 and 1 denoting our degree of belief that a given event/hypothesis $A$ is true.\n",
    "\n",
    "$$\\large P(\\text{tails}) = 0.5 $$\n",
    "\n",
    "$ P(\\bar{A})$ is the probability that $A$ is not true, and it is equal to $1-P(A)$.\n",
    "\n",
    "$$\\large P(\\text{heads}) = P(\\text{not tails}) = 1 - P(\\text{tails}) = 0.5 $$\n",
    "\n",
    "$ P(A, B) $ is the **joint** probability that $A$ and $B$ are both true. Of course $P(A, B) = P(B, A)$\n",
    "\n",
    "$$\\large P(\\text{we see tails}) = P(\\text{the coin was tossed}, \\text{tails}) $$\n",
    "\n",
    "$ P(A | B)$ is the conditional probability: $A$ to be true if $B$ is true, or **A given B**.\n",
    "\n",
    "$$\\large P(\\text{tails} | \\text{the coin was tossed}) = 0.5 $$\n",
    "$$\\large P(\\text{tails} | \\text{the coin was not tossed}) = 0 $$\n",
    "\n",
    "$P(A, B) = P(A | B) P(B) $ is the chain rule which allows us to factor joint probabilities or calculate conditional probabilities.\n",
    "\n",
    "We throw two coins...\n",
    "$$\\large P(\\text{heads from 1st}, \\text{heads from 2nd}) = P(\\text{heads from 1st} | \\text{heads from 2nd}) \\times P(\\text{heads from 2nd}) = 0.5 \\times 0.5 = 0.25 $$\n",
    "(where we assumed that the tossings are independent).\n",
    "\n",
    "We have two coins. We throw the first, and if the result is tails, we are allowed to throw the next one.\n",
    "\n",
    "$$\\large P(\\text{heads from 1st}, \\text{heads from 2nd}) = P(\\text{heads from 2nd}, \\text{heads from 1st}) = P(\\text{heads from 2nd} | \\text{heads from 1st}) \\times P(\\text{heads from 1st}) = 0 \\times 0.5 = 0 $$\n",
    "\n",
    "## Bayes' theorem... law... rule...\n",
    "\n",
    "Applying the chain rule we have two ways to write the probability of two events:\n",
    "\n",
    "$$\\large P(A, B) = P(A | B) P(B) $$\n",
    "$$\\large P(B, A) = P(B | A) P(A) $$\n",
    "\n",
    "By equalizing the right-hand sides we get:\n",
    "\n",
    "$$\\large P(A | B) = \\dfrac{ P(B|A) P(A) }{ P(B) } $$\n",
    "\n",
    "So in data analysis we can characterize the belief in a hypothesis using data in the following way:\n",
    "\n",
    "$$\\large P(\\text{hypothesis} | \\text{data}, I ) = \\dfrac{ P(\\text{data | hypothesis}, I) P(\\text{hypothesis} | I)}{ P(\\text{data} | I) } $$\n",
    "\n",
    "The symbol $I$ denotes the **background information**: all relevant knowledge we have about the problem we are solving but are not part of the data.\n",
    "\n",
    "For example, if we are testing whether a die is fair, the data are a sequence of outcomes (e.g., 1, 6, 3, 5, 3, 1), the hypothesis is **it's a fair die**, and $I$ = $\\big\\{$ all dice have 6 sides, a fair die has equiprobable sides, the Moon is not made of cheese, $\\cdots \\big\\}$.\n",
    "\n",
    "For brevity, **we usually omit writing the $I$** in the equations, but **it's always there**... somewhere in the background!\n",
    "\n",
    "All the terms of this equation, the famous Bayes' rule, have names:\n",
    "\n",
    "* **Posterior**: $P(\\text{hypothesis} | \\text{data}, I)$ is the degree of belief we have on the hypothesis after (a posteriori) looking at the data\n",
    "* **Likelihood**: $P(\\text{data | hypothesis}, I)$ is the likelihood of collecting the data at hand, given that the hypothesis is true (what a frequentist would focus on...)\n",
    "* **Prior**: $P(\\text{hypothesis} | I)$ is the degree of belief in the hypothesis before looking at the data (a priori). E.g., from previous studies, mathematical or physical constraints, ...\n",
    "* **Evidence**: $P(\\text{data} | I)$ is the probability of getting the data independently of whether the hypothesis is true or false. We almost never know the evidence and in most problems its value is not affecting the analysis!\n",
    "\n",
    "> In Bayesian Analysis we assign degrees of belief to hypotheses, which we \"update\" using experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e9b4c",
   "metadata": {},
   "source": [
    "## 3. The Bayesian approach to our photon-counting problem\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k) = \n",
    "\\dfrac{ \\large P(k | \\lambda) P(\\lambda) }{ P(k) }\n",
    "$$\n",
    "\n",
    "*(for brevitiy we omitted the background information $I$)*\n",
    "\n",
    "## 3.1. Constructing the likelihood\n",
    "\n",
    "The likelihood is expressed in terms of the PDF of the Poisson with expected value the unknown parameter $\\lambda$:\n",
    "\n",
    "$$\\large k \\sim \\text{Pois}(\\lambda)$$\n",
    "\n",
    "$$\\large\n",
    "P(k | \\lambda) = \\text{Pois}(k; \\lambda) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!}\n",
    "$$\n",
    "\n",
    "*(usually words like Pois, Norm, Gamma, etc. describe distributions. Using the Dist(x; p) notation we are referring to the probability mass function (discrete distirbutions) or the probability density function (continuous distributions) with parameters $p$ evaluated at $x$)*\n",
    "\n",
    "## Exercise 3: Pick the correct function to plot the likelihood\n",
    "\n",
    "You can consult the [scipy.stats.poisson documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html) to find the probability mass function of the Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377457bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.0, 20, 200)\n",
    "k = counts\n",
    "\n",
    "likelihoods = st.poisson. ...(k, lambdas)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, likelihoods, \"k-\")\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"Likelihood $P(5; \\lambda)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850f979",
   "metadata": {},
   "source": [
    "This is not a Poisson distribution, since the \"random variable\" is now $\\lambda$ which is continuous (Poisson is a *discrete* distribution)! In reality this is the **Gamma distribution** with shape parameter $\\kappa=k+1$ and scale parameter $\\theta=1$, or simply $\\text{Gamma}(k+1, 1)$.\n",
    "\n",
    "$$\\large\n",
    "P(k | \\lambda) = \\text{Pois}(k; \\lambda) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!} = \\dfrac{\\lambda^{(k+1)-1} e^{-\\lambda}}{\\Gamma(k+1)} = \\text{Gamma}(\\lambda; k+1, 1)\n",
    "$$\n",
    "\n",
    "## 3.2. Constructing the prior\n",
    "\n",
    "What about the prior? If we have no previous information about the source intensity, then we could be completely agnostic and consider all possible values as equal:\n",
    "\n",
    "$$\\large  P(\\lambda) = \\text{a positive constant}, \\quad \\lambda \\in \\mathbb{R} $$ \n",
    "\n",
    "Constant priors of an infinite or finite range are called **uniform priors**. However this is not an actual probability density function, since it's integration over the whole range $\\lambda \\in \\left(-\\infty, \\infty\\right)$ would lead to infinity. Instead, we could take a safe, huge, but finite range. At the end of the day, it doesn't make any difference, so we often use these **improper priors**.\n",
    "\n",
    "In our case we can do better by requiring the $\\lambda$ is positive since our source luminosity cannot be negative!\n",
    "\n",
    "$$\\large  P(\\lambda) = A \\begin{cases} 1, \\quad \\lambda \\geq 0 \\\\ 0, \\quad \\lambda < 0 \\end{cases} $$\n",
    "\n",
    "Note that this is still an **imporper prior**. We can even omit the normalization factor $A$ since we cannot rescale an improper prior. In practice, we work with proportionalities when the scale is unknown (or irrelevant): $P(\\lambda) \\propto \\cdots$.\n",
    "\n",
    "But to do things \"properly\"... we can assume a uniform distribution. Maybe previous studies showed that point sources in galaxies have an upper limit in the luminosity, which in our observation corresponds to 10 counts.\n",
    "\n",
    "$$\\large  P(\\lambda) = A \\begin{cases} 1, \\quad \\lambda \\in [0, 10] \\\\ 0, \\quad \\text{elsewhere} \\end{cases} $$\n",
    "\n",
    "Normalizing the prior results in:\n",
    "\n",
    "$$\\large \\int_{-\\infty}^{\\infty} P(\\lambda) d\\lambda = 1 \\Rightarrow \\int_{0}^{10} A d\\lambda = 1 \\Rightarrow A = \\frac{1}{10} $$\n",
    "\n",
    "## 3.3. The evidence\n",
    "\n",
    "The term $ P(k)$ is called *evidence*, or marginal likelihood. It describes the probability of getting the data given the background information, but regardless of the parameters of the models (or the hypotheses). In the theory example, this is:\n",
    "\n",
    "$$\\large P(\\text{data}) = P(\\text{data | hypothesis}) P(\\text{hypothesis}) + P(\\text{data} | \\overline{\\text{hypothesis}}) P(\\overline{\\text{hypothesis}}) $$\n",
    "\n",
    "Therefore, it is the summation of the likelihood of the data considering all possible outcomes of the experiment. In the case that we are calculating the posterior for a model parameter, say, the number of lines in a spectrum we would have a posterior looking like this:\n",
    "\n",
    "$$\\large P(\\text{num. of lines | spectrum}) = \\dfrac{ P(\\text{spectrum | num. of lines}) P(\\text{num. of lines}) }\n",
    "   { P(\\text{spectrum}) } $$\n",
    "   \n",
    "with the evidence being:\n",
    "\n",
    "$$\\large P(\\text{spectrum}) = P(\\text{spectrum | no lines}) P(\\text{no lines}) + P(\\text{spectrum | one line}) P(\\text{one line}) + \\cdots = \\sum_{n=0}^{\\infty} P(\\text{spectrum | num. of lines = n}) P(\\text{num. of lines = n}) $$\n",
    "\n",
    "Notice, that we are practically integrating, or **marginalizing** the numerator of the Bayes formula, hence, normalizing the probabilities. Notice, however, that since the evidence does not depend on the hypothesis, when calculating the **relative** probabilities of different number of lines, the evidence is **cancelled out**. For this reason, **in most problems we can ignore this term, and only if necessary, we calculate it to normalize the posterior.** When selecting between hypothesis, or different models, the evidence may be imporant.\n",
    "\n",
    "In our source photon problem the evidence is\n",
    "$$\\large P(k) = \\int_{-\\infty}^{\\infty} P(k | \\lambda) P(\\lambda) d\\lambda $$\n",
    "which just normalizes the numerator, and we can ignore it.\n",
    "\n",
    "\n",
    "## 3.4. How the posterior looks now...\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k) = \n",
    "    \\begin{cases}\n",
    "        \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\times \\frac{1}{10} \\times \\dfrac{1}{\\text{evidence}} &,\\quad \\lambda \\in [0, 10]\n",
    "\\\\\n",
    "        0 & ,\\quad \\text{elsewhere}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Since we ignore the normalization from the evidence, it doesn't make sence to keep the normalization of the prior. Any uniform prior can be neglected, but we should make sure we are respecting the bounds of the priors (if any)! Therefore,\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k) \\propto \n",
    "    \\begin{cases}\n",
    "        \\dfrac{\\lambda^k e^{-\\lambda}}{k!} &,\\quad \\lambda \\in [0, 10]\n",
    "\\\\\n",
    "        0 &,\\quad \\text{elsewhere}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### A way of working...\n",
    "\n",
    "The extended bibliography in classical statistics, the standardization of its methods, and the numerous implementations in software may be powerful and useful but often obscure the assumptions and caveats we are trading in. During our Bayesian analysis, writing down the equations and deriving the final formula, made us consider the nature of the data, our previous knowledge, assumptions, background information, etc.\n",
    "\n",
    "### A way of thinking...\n",
    "\n",
    "Notice that this is very similar to the likelihood a frequentist would use. The **practical difference** is that we are now estimating $\\lambda$, taking into account prior information. The **conceptual difference** is that we assigned a probability to the unknown parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15309b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(lam):\n",
    "    if lam < 0 or lam > 10:\n",
    "        return 0.0\n",
    "    return 1.0\n",
    "\n",
    "def likelihood(k, lam):\n",
    "    return st.poisson(lam).pmf(k)\n",
    "\n",
    "def posterior(lam, k):\n",
    "    return likelihood(k, lam) * prior(lam)\n",
    "\n",
    "lambdas = np.linspace(0.0, 20, 1001)\n",
    "likelihoods = st.poisson.pmf(counts, lambdas)\n",
    "posteriors = np.array([posterior(lam, counts) for lam in lambdas])\n",
    "\n",
    "normalized_posteriors = posteriors / np.trapz(posteriors, lambdas)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, likelihoods, color=\"b\", ls=\"-\", lw=4, alpha=0.5, label=\"Likelihood\")\n",
    "plt.plot(lambdas, posteriors, \"r-\", label=\"Unnormalized posterior\")\n",
    "plt.plot(lambdas, normalized_posteriors, \"k:\", label=\"Normalized posterior\")\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c323908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode_mean_and_std(x, Px):\n",
    "    mode = x[np.argmax(Px)]\n",
    "    mean = sum(x * Px) / sum(Px)\n",
    "    variance = sum((x - mean) ** 2.0 * Px) / sum(Px)\n",
    "    return mode, mean, variance ** 0.5\n",
    "\n",
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, likelihoods)\n",
    "print(f\"(using only the likelihood)   mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")\n",
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, posteriors)\n",
    "print(f\"(unnormalized posterior)      mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")\n",
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, normalized_posteriors)\n",
    "print(f\"(normalized   posterior)      mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a71704",
   "metadata": {},
   "source": [
    "Notes:\n",
    "> The prior alters the shape of the posterior and makes it different from the likelihood. A \"strict\" or informative prior will dominate the final result, while a \"loose\" or uniformative prior will have negligible effect.\n",
    "> Using a uniform prior possibly affects the bounds and the normalization, but the maximum a posteriori values might be the same as the maximum likelihood.\n",
    "\n",
    "### Questions for discussion\n",
    "\n",
    "#### Q1. Is the mean value of $\\lambda$ the same as if we only used the likelihood?\n",
    "\n",
    "#### Q2. Is the uncertainty affected by the prior?\n",
    "\n",
    "#### Q3. Does the normalization of the posterior affect the parameter estimation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78e444",
   "metadata": {},
   "source": [
    "# 4. Measuring a source luminosity in the presence of background\n",
    "\n",
    "In our X-ray image we see that there a detected photons all over the place. These may come from the detector (instrumental background) or foreground (stars in Mikly Way) or background (distant quasars) sources. If we are interested in the luminosity of a source in NGC 1482, we want to correct for the \"background\". Previous studies have measured the flux or photon counts per unit area, in the bandwidth of our observation. The source detection algorithm that gave as the 5 counts, also found the \"source region\" and based on its area, we find that we expect on average $\\beta=3$ counts from the background!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc8173",
   "metadata": {},
   "source": [
    "## 4.1. How many are the source counts $n$ if we measured $k$ counts and expect $\\beta$ counts?\n",
    "\n",
    "### Method 1: Simply substract the background and assume Poisson distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed990",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = counts - beta\n",
    "n_err = n ** 0.5\n",
    "print(f\"Source counts: {n:.2f} +/- {n_err:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9fe80",
   "metadata": {},
   "source": [
    "#### Q4. What if the background level was higher than the detected counts? Are the source counts definitely 0? If not, is there an uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1af3f",
   "metadata": {},
   "source": [
    "### Method 2: The mean background, is a... mean. Take into account the Poisson nature of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b59d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_err = 0.0   # the counts we got are fixed\n",
    "beta_err = beta ** 0.5\n",
    "\n",
    "n = counts - beta\n",
    "n_err = (counts_err ** 2.0 + beta_err ** 2.0) ** 0.5\n",
    "print(f\"Source counts: {n:.2f} +/- {n_err:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312aac3",
   "metadata": {},
   "source": [
    "Now the probability distribution of the source counts is not Poisson. The uncertainty comes from applying the one-term approximation of the error-propagation formula (which furthermore assumes Gaussian errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be527ef",
   "metadata": {},
   "source": [
    "### Method 3: A \"combinatorial\" approach\n",
    "\n",
    "The probability of having no counts from the source, is the probability that they all detected counts come from the background. The probability of one source count = 4 counts from the background, and so on.\n",
    "\n",
    "$$\\large P(n | k , \\beta) = \\text{Pois}(k - n; \\beta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e059fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 3.0\n",
    "n = np.arange(0, 11)\n",
    "Pn = st.poisson(beta).pmf(counts - n)\n",
    "# now normalize it...\n",
    "Pn /= sum(Pn)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(n, Pn, color=\"r\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Number of source counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41179c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode, mean, std = calculate_mode_mean_and_std(n, Pn)\n",
    "print(f\"mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd1a51",
   "metadata": {},
   "source": [
    "#### Q6. What is the problem with this approach? \n",
    "By accounting for the background (and its Poissonian nature) we are able to quantify the likelihood of a given number of counts coming from a source. However, we never took into account the Poissonian nature of the source counts! We completely exclude the case we happened to not get any count from a powerful source (which would typically give back, say, 6 counts).\n",
    "\n",
    "## 4.2. The Bayesian approach\n",
    "\n",
    "\n",
    "### Writing what we know about the data, nature and their connection...\n",
    "\n",
    "We detect $k$ counts, out of which $n$ are coming from a source with $\\lambda$ counts on average, and $b$ counts from the background with mean level $\\beta$. Both the souce and the background counts are Poisson distributed:\n",
    "\n",
    "$$ \\large\n",
    "\\begin{align}\n",
    "k &= n + b \\\\\n",
    "n &\\sim \\text{Pois}(\\lambda) \\\\\n",
    "b &\\sim \\text{Pois}(\\beta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Writing things we want to know (left side) as a function of what we do know (right side)...\n",
    "\n",
    "Given that we have a measurement of $k$ and $\\beta$ (from a previous study), we want to know the $\\lambda$. We use the Bayes' rule (neglecting evidence) to quantify the posterior on $\\lambda$:\n",
    "\n",
    "$$\\large P(\\lambda | k, \\beta) \\propto P(k | \\lambda, \\beta) P(\\lambda | \\beta) $$\n",
    "\n",
    "The **likelihood** requires the knowledge of what is the number of total counts given the mean source and backgroud counts. In this case we are lucky: the sum of Poisson random variables is a new Poisson variable with mean value equal to the sum of the mean values of the original Poisson variables:\n",
    "\n",
    "$$ \\large k \\sim \\text{Pois}(\\lambda + \\beta) $$\n",
    "\n",
    "This makes complete sense: the Universe doesn't care about what we define as a source or background! For all we know we can consider a source as two objects: the first and second halves!\n",
    "Therefore,\n",
    "\n",
    "$$ \\large P(k | \\lambda, \\beta) = \\text{Pois}(k; \\lambda + \\beta) = \\text{Pois}(k; \\lambda + 3)$$\n",
    "\n",
    "(the last part comes from the fact that we assume infinite accuracy on $\\beta$.)\n",
    "\n",
    "For the **prior** on $\\lambda$ we select the improper prior that allows only non-negative values, and therefore\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k, \\beta) \\propto \n",
    "    \\begin{cases} \n",
    "        \\text{Pois}(k; \\lambda + 3), &\\lambda \\geq 0 \\\\\n",
    "        0, &\\lambda < 0 \n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "## Exercise 4: Construct the likelihood, prior and posterior functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b07138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(total_counts, source_mean, bkg_mean):\n",
    "    return st.poisson(...).pmf(total_counts)\n",
    "\n",
    "def prior(source_mean):\n",
    "    return ... if source_mean < 0 else ...\n",
    "\n",
    "def posterior(source_mean, total_counts, bkg_mean):\n",
    "    return likelihood(total_counts=total_counts, source_mean=source_mean, bkg_mean=bkg_mean) * prior(source_mean)\n",
    "\n",
    "lambdas = np.linspace(0.0, 20.0, 101)\n",
    "posteriors_fixedbkg = np.array([posterior(source_mean=lam, total_counts=counts, bkg_mean=beta) for lam in lambdas])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, posteriors_fixedbkg, \"k-\")\n",
    "plt.xlabel(\"Source mean counts\")\n",
    "plt.ylabel(\"Posterior density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, posteriors_fixedbkg)\n",
    "print(f\"COUNTS    :       mode = {mode:8.2f} | mean +/- std = {mean:8.2f} +/- {std:8.2f}\")\n",
    "\n",
    "cdf = np.cumsum(posteriors_fixedbkg) / np.sum(posteriors_fixedbkg)\n",
    "\n",
    "# mode, mean, std = calculate_mode_mean_and_std(lambdas, posteriors_fixedbkg)\n",
    "index_p16 = np.argmin(np.abs(cdf-0.16))\n",
    "index_p84 = np.argmin(np.abs(cdf-0.84))\n",
    "index_p025 = np.argmin(np.abs(cdf-0.025))\n",
    "index_p975 = np.argmin(np.abs(cdf-0.975))\n",
    "index_p001 = np.argmin(np.abs(cdf-0.0005))\n",
    "index_p999 = np.argmin(np.abs(cdf-0.9995))\n",
    "\n",
    "factor = flux_to_luminosity * counts_to_flux_factor / exposure_time_in_seconds\n",
    "\n",
    "print(f\"LUMINOSITY:       mode = {factor*mode:8.2e} | mean +/- std = {factor*mean:8.2e} +/- {factor*std:8.2e}\")\n",
    "print(f\"               68%   CI: ({factor*lambdas[index_p16]:.2e}, {factor*lambdas[index_p84]:.2e})\")\n",
    "print(f\"               95%   CI: ({factor*lambdas[index_p025]:.2e}, {factor*lambdas[index_p975]:.2e})\")\n",
    "print(f\"               99.9% CI: ({factor*lambdas[index_p001]:.2e}, {factor*lambdas[index_p999]:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cb8ce",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* Using the Bayesian interpretation of probability we quantified the likelihood of having a source of a given intensity.\n",
    "* We took into account all information: Poissonian nature of both source and background counts\n",
    "* We were allowed to apply constraints from previous studies (in the prior)\n",
    "* The result looks as expected: it is defined only for non-negative values, peaks close to 0 as we would expect from a weak detection, but does not exclude a powerful source (no clear upper limit)!\n",
    "* We can use the well-defined posterior to report using point estimates (e.g., mean, mode, std), confidence intervals (or regions), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf3126",
   "metadata": {},
   "source": [
    "# 4.3. Using uncertainties in the background\n",
    "\n",
    "The mean background level of $3$ was just an example of what the literature might report. In reality, researchers put uncertainties in that value as well! Let's assume it's $3 \\pm 1.0$.\n",
    "\n",
    "What is the posterior now? The background level $\\beta$ cannot be treated as background information. It is not a fixed value, but a parameter for which we do not have absolute knowledge. Consequently, we have to consider all posibilities for the mean source and background counts.\n",
    "\n",
    "$$\\large P(\\lambda, \\beta | k) \\propto P(k | \\lambda, \\beta) P(\\lambda, \\beta)$$\n",
    "\n",
    "The likelihood is still as in the previous example:\n",
    "\n",
    "$$ \\large P(k | \\lambda, \\beta) = \\text{Pois}(k; \\lambda + \\beta)$$\n",
    "\n",
    "The *joint prior* can be broken into simpler terms:\n",
    "\n",
    "$$\\large P(\\lambda, \\beta) = P(\\lambda | \\beta) P(\\beta)$$\n",
    "\n",
    "\n",
    "(*Note that we could write it as $P(\\lambda, \\beta) = P(\\beta | \\lambda) P(\\lambda) $, but the following analysis doesn't change!*)\n",
    "\n",
    "The first term is quantifying the prior on $\\lambda$ but given the value of $\\beta$, taking into account any possible correlation between our parameters. In our case, we have no reason to believe they are correlated: what a source in our galaxy does is unrelated to the emission from sources in the Milky Way (foreground) or the distant Universe (background). Consequently, $P(\\lambda | \\beta) = P(\\lambda)$, and:\n",
    "\n",
    "$$\\large P(\\lambda, \\beta) = P(\\lambda) P(\\beta) $$\n",
    "\n",
    "We can use the same prior on $\\lambda$ as before (uniform for $\\lambda \\geq 0$). For the $\\beta$ we use the literature values to take a truncated version of the normal distribution (background is non-negative):\n",
    "\n",
    "$$\\large P(\\beta) \\propto \\text{Norm}(\\beta; \\mu=3, \\sigma=1) \\quad \\text{for} \\quad b \\geq 0 $$\n",
    "\n",
    "Consequently, the two-dimensional posterior on both $\\lambda$ and $\\beta$ is\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda, \\beta | k) \\propto \n",
    "    \\begin{cases} \n",
    "        \\text{Pois}(k; \\lambda + \\beta) \\times \\text{Norm}(\\beta; \\mu=3, \\sigma=1), &\\lambda, \\beta \\geq 0 \\\\\n",
    "        0, &\\lambda < 0 \\text{ or } b < 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The background level $\\beta$ might be unknown but we are not specifically interested in its value. We only care about the source level $\\lambda$. A quantity that we are not interested in but has to enter the Bayesian analysis is a **nuisance parameter**. To infer the quantities of interest, we use marginalization by integrating over all possible values for the nuisance parameter (or summation when it is discrete):\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k) = \\int_{-\\infty}^{\\infty} P(\\lambda, \\beta | k) d\\beta\n",
    "$$\n",
    "\n",
    "Taking into acount the limits ($b$ is non-negative)...\n",
    "\n",
    "$$\\large \n",
    "P(\\lambda | k) = \\int_{0}^{\\infty} P(\\lambda, \\beta | k) d\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "\n",
    "beta = 3.0\n",
    "beta_err = 1.0\n",
    "\n",
    "def likelihood(total_counts, source_mean, bkg_mean):\n",
    "    return st.poisson.pmf(total_counts, source_mean + bkg_mean)\n",
    "\n",
    "def prior_source(source_mean):\n",
    "    return 0.0 if source_mean < 0 else 1.0\n",
    "\n",
    "def prior_background(bkg_mean):\n",
    "    return st.norm.pdf(bkg_mean, beta, beta_err)\n",
    "\n",
    "def joint_posterior(source_mean, total_counts, bkg_mean):\n",
    "    return (likelihood(total_counts=total_counts, source_mean=source_mean, bkg_mean=bkg_mean) \n",
    "            * prior_source(source_mean) * prior_background(bkg_mean))\n",
    "\n",
    "def posterior(source_mean, total_counts):\n",
    "    lower_limit = max(0, beta - beta_err*5)\n",
    "    upper_limit = beta + beta_err*5\n",
    "    \n",
    "    def integrand(background):\n",
    "        return joint_posterior(source_mean=source_mean, \n",
    "                               total_counts=total_counts, \n",
    "                               bkg_mean=background)\n",
    "\n",
    "    result = scipy.integrate.quad(integrand, a=lower_limit, b=upper_limit)\n",
    "    return result[0]\n",
    "\n",
    "posteriors_varbkg = np.array([posterior(source_mean=lam, total_counts=counts) for lam in lambdas])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, posteriors_fixedbkg / np.trapz(posteriors_fixedbkg, x=lambdas), \"k-\", label=\"Fixed background\")\n",
    "plt.plot(lambdas, posteriors_varbkg / np.trapz(posteriors_varbkg, x=lambdas), \"r--\", label=\"Background has a distribution\")\n",
    "plt.xlabel(\"Source mean counts\")\n",
    "plt.ylabel(\"Posterior density\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, posteriors_fixedbkg)\n",
    "print(f\"FIXED BACKGROUND:       mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")\n",
    "mode, mean, std = calculate_mode_mean_and_std(lambdas, posteriors_varbkg)\n",
    "print(f\"VARIABLE BACKGROUND:    mode = {mode:.2f} | mean +/- std = {mean:.2f} +/- {std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
